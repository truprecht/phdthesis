\documentclass[../../document.tex]{subfiles}


% Throughout this section, we will assume the following variables as inputs and hyperparameters:
% \begin{itemize}
%     \item the input sentence \(w\) has the length \(\ell\),
%     \item the set of supertag blueprints is denoted by \(B\),
%     \item the previous step predicted \(k \in \DN_+\) supertag blueprints per position,
%     \item the family \((b^{(i)}_{\kappa} \in B \mid i \in [\ell], \kappa \in [k])\) of predicted supertags with confidence \((\varphi^{(i)}(w, b^{(i)}_{\kappa}) \mid i \in [\ell], \kappa \in [k])\) fom the previous step \ref{parsing:item:1},
%     \item a growing sequence of confidence interval sizes \((\beta_i \in \DR \mid i \in I)\) for some non-empty integer index set \(I\), and
%     \item we aim to find a collection of up to \(n \in \DN_+\) best derivations in this step.
% \end{itemize}

\begin{document}
    \section{Statistical Parsing with Predicted Supertags}\label{sec:parsing}
    This section describes the algorithms involved in combining the predicted supertag blueprints into derivations.
    We destinguish two cases:
    \begin{inparaenum}[]
        \item In the case of \abrv{lcfrs} or hybrid grammars supertags, we rely on algorithms that are well established in statistical parsing with these two grammars formalisms to obtain derivations. \Cref{sec:parsing:lcfrs} elaborates on this subject and gives references to the literature.
        \item Since \abrv{dcp} have not been used in such a setting before, we dedicate \cref{sec:parsing:dcp} to an adaption of well-established algorithms from \abrv{lcfrs} to the case of \abrv{dcp} supertags.
    \end{inparaenum}
    These algorithms are embedded in a shared framework that is discussed in the following.

    \begin{algorithm}[t]
        \caption{\label{alg:parsing:incremental}
            Incremental parsing algorithm that uses increasing sets of predicted supertags with a sequence confidence intervals.
            The least interval that contains an admissible derivation is used to determine an ordered sequence of \(n\) best admissible derivations.
        }
        \subfile{algorithms/incremental-parsing.tex}
    \end{algorithm}

    \Cref{alg:parsing:incremental} illustrates the incremental parsing algorithm as described by \citet[Section~5.1]{Clark04} and \citet[Section~2.2.2]{Auli12} and adjusted to our setting.
    It works as follows:
    \begin{itemize}
        \item The algorihm iterates over the growing confidence interval sizes \(\beta_1, \beta_2, \ldots\) (line 2) and computes the least confidence within the interval \(\delta^{(i)}\) for each position in the input sentence (line 3).
        \item Then, it collects the supertag blueprints predicted within this confidence interval and instantiates them for each sentence position (line 4).
        \item The weight assignment \(\weight\) for the resulting set of rules is determined from the family \(\varphi\) of prediction confidences (line 5).
        \item If there is any admissible derivation in the set of supertags \(S\), then it returns the an ordered sequence of (at most \(n\)) admissible derivations and ends (lines 6 and 7). Thus, only the supertags within the least possible prediction confidence interval are used to determine the derivations.
        \item If there were no admissible derivations and the prediction confidences \(\delta^{(i)}\) are non-exhausting (not all predicted supertags were used), then it continues with the next confidence interval. Otherwise it aborts, since no admissible derivation can be found (lines 8 and 9).
        \item If no derivations are found, it reports a failure (line 10).
    \end{itemize}
    In practice, we implement the confidence intervals as the sequence of all multiples of a base value \(\hat{\beta}\in \DR\) of the form \((\beta_i = i \cdot \hat{\beta} \mid i \in \DN_+)\).
    In the following, we will call this base value \emph{parsing step range} and denote it as the symbol \(\beta\).
    \Citet{Clark04} and \citet{Auli12} used a fixed sequences of values in their experiments.
    This approach differs from the one we used in previous versions of the parser \citep{RupMoe21, Rup22}, where we used a fixed amount of predicted blueprints per sentence position.
    The following two subsection are concerned with lines 6 and 7 of the algorithm:
        They explain the algorithms involved in determining the admissible derivations for the sequence of sentence positions and enumerating them according to their weight.

    \subsection{Parsing with \abrv{Lcfrs} and \abrv{Hg} Supertags}\label{sec:parsing:lcfrs}
    In the case of \abrv{lcfrs} and \abrv{hg} supertags, the sets of admissible derivations for an input sequence are defined using the nonterminals and the \abrv{lcfrs} compositions in the grammar rules.
    The \abrv{dcp} compositions in \abrv{hg} supertags, as well as the transportation marker in \abrv{lcfrs} supertags, do not influence the sets of derivations at all.
    They are solely used to define the constituent structure for an already determined derivation.
    With both formalisms, we exploit statistical parsing procedures for \abrv{lcfrs} as \citet{Geb20} did in the case of parsing with hybrid grammars.
    Parsing procedures for \abrv{lcfrs} are well documented, e.g.\@ as described by \citet[Section~7.1]{Kal10}.
    For the case of \(k\) best parsing, the approach can be extended using the techniques described by \citet{HuaChia05} to enumerate found derivations with increasing weight.
    In previous publications, we used the \abrv{lcfrs} parser implemented by \citet{CraSchBod16} and extended it for our purposes.\footnote{
        For using \emph{disco-dop} with our predicted supertags, we implemented some transformations to accommodate that it is only able to parse with binary and simple \abrv{lcfrs}.
        Each supertag is transformed into a collection of appropriate rules by splitting the terminal symbol into a separate rule where the supertag is its \abrv{lhs} nonterminal.
        The transformation constructs a ternary rule for each supertag that contains a lexical binary rule; it is then split into two using the usual binarization techniques and introducing a unique intermediate nonterminal.
        These rules are connected using unique nonterminals, so they must occur together in derivations.
        This transformation is easily reverted in a derivation by identifying the supertag at the bottom rule of such a cluster of rules and replacing the whole construct with the supertag.
    }
    However, we omitted this in favor of a unified implementation that implements parsing with \abrv{dcp} supertags as well.

    \subsection{Fanout-Restricted Parsing with \abrv{Dcp} Supertags}\label{sec:parsing:dcp}
    To our knowledge, using grammars that solely consist of \abrv{dcp} rules was not considered before in the field of discontinuous constituent parsing.
    By itself, the \abrv{dcp} compositions do not impose any restrictions to the lexical symbols in the defined sets of derivations, compared to \abrv{lcfrs}, which precisely specify which parts of the spans may occur next to each other.
    Our early implementations of parsers for \abrv{dcp} supertags struggled with the sheer amount of derivations that can be found in contrast to \abrv{lcfrs}.
    Therefore, we impose restrictions for the case of parsing with \abrv{dcp} supertags; we call a \abrv{dcp} supertag derivation \emph{admissible} if the following two properties are satisfied for each of its subtrees $d$:
    \begin{enumerate}
        \item We extend the rules by a fanout restriction to limit the sets of derivations; each \abrv{dcp} supertag is a tuple \((r, f)\) where the positive integer \(f \in \DN_+\) is called the \emph{fanout restriction}. Consider the \abrv{dcp} supertag derivation $d$ rooted in \((r,f)\). The fanout of the set of all lexical symbols within \(d\) must be exactly \(f\).
        \item\label{prop:dcp:admissible:order}  Consider the supertag derivation $d$ is of the form \((r,f)\;\big( d_1, \ldots, d_k \big)\). Then the child derivations \(d_1, \ldots, d_k\) must be ordered with respect to the least occurring lexical symbol. 
    \end{enumerate}\todo{move to beginning of extraction chapter}

    We adapt a statistical parsing algorithm for \abrv{lcfrs} (cf.\@ \citealp{Burden05}, Section~3; and \citealp{Kal10}, Section 7.1.2) to the case of \abrv{dcp} supertags.
    In the following, the parsing algorithm is considered in two consecutive steps that we first outline and then discuss in detail:
    \begin{enumerate}
        \item \label{step:parsing:chart}
            The first step computes a compact representation of the set of admissible derivations for an input sentence; this representation is called \emph{parse chart}.
            The parse chart groups derivations with the same \abrv{lhs} nonterminal at the root and the same set of lexical symbols within the whole derivation.
            It stores each derivation as a \emph{backtrace}; that is, a tuple consisting of the derivation's root and a sequence of indices identifying all child derivations.
            Moreover, it stores the best weight of any derivation for each combination of \abrv{lhs} nonterminal and set of lexical symbols.
        \item \label{step:parsing:derivation}
            In the second step of the parsing algorithm, the parse chart is used to read the derivation with the best weight.
            This step recursively descends into the parse chart and, in each recursion, determines the rule at the root of each (sub-)derivation with the best weight.
            It is extended using the techniques described by \citet{HuaChia05} to enumerate the derivations ordered by their weight.
            With this extension, it implements a \(k\) best parsing algorithm.
    \end{enumerate}

    \begin{algorithm}
        \caption{\label{alg:parsing:chart}
            This listing illustrates an algorithms to compute a parse chart, i.e.\@ a compact representation of admissible derivations and their weights, for \abrv{dcp} supertags.
            This coarsely resembles the algorithm for parsing \abrv{lcfrs} as shown by \citet[the ``na\"ive algorithm'' in Section~3]{Burden05}.
        }
        \subfile{algorithms/parsing.tex}
    \end{algorithm}

    \subsubsection*{Step \ref{step:parsing:chart}: Computing the Parse Chart}
    \Cref{alg:parsing:chart} illustrates a process that implements this step.
    It is derived from an \abrv{lcfrs} parsing algorithm as presented by \citet[the na\"ive algorithm in Section~3]{Burden05}.
    In the following, we will refer to this original presentation of the parsing algorithm by \abrv{bl}.
    Unlike \abrv{bl}, we opted to describe the process in pseudocode instead of a deduction system.
    This process demands the following inputs:
    \begin{inparaitem}[]
        \item the set of \abrv{dcp} supertags \(S\) with the set of nonterminals \(N\),
        \item the weight for each supertag \(\weight\colon S \to \DR\), and
        \item the input sentence length \(\ell \in \DN_+\).
    \end{inparaitem}
    Additionally, we utilize a distinct symbol \(X_i \notin N\) for each position \(i \in [\ell]\) that does not occur as a nonterminal; these symbols will be used as markers representing the lexical symbol within each supertag.
    In the ramainder, we describe the algorithm first by explaining the structures and meaning of the occurring objects, then we discuss the procedure shown in the listing, and lastly, we discuss the properties of the output.

    \paragraph{Parse items, backtraces and parse states.} During the procedures, we construct tuples of the form \((a, b, w)\) (in lines 7--9, 18--19 and 26--29); each of this tuple represents an investigation in a derivtation rooted in a \abrv{dcp} supertag \(s \in S\) as follows:
    \begin{itemize}
        \item The first component \(a\) is a tuple of the form \((A, f, B_1 \ldots B_j, L)\) where
            \begin{compactitem}
                \item \(A\) is the \abrv{lhs} nonterminal in \(s\), and the sequence \(B_1 \ldots B_j\) consists of the \abrv{rhs} nonterminals in \(s\) as well as the symbol \(X_i\) where $i\in [\ell]$ is the lexical symbol in \(s\),
                \item \(f \in \DN_+\) is the fanout restriction in \(s\), and
                \item \(L \subseteq [\ell]\) is a set of sentence positions.
            \end{compactitem}
            If \(j>0\), the tuple \(a\) corresponds to an \emph{active item} in \abrv{bl}.
            Such an active item denotes a partially investigated derivation, which is rooted in the supertag \(s\) and carries the lexical symbols in \(L\).
            The symbols \(B_1, \ldots, B_j\) denote nonterminals for the successor derivations still left to investigate; if \(X_i\) occurs among them, then it represents the lexical symbol in the root of the derivation.
            These following investigations are conducted in reverse order, from right (index $j$) to left (index $1$).
            If \(j=0\), the investigation is complete and found a derivation. This corresponds to a \emph{passive item} in \abrv{bl}.
            This object \(a\) is called a \emph{parse item}.
        \item The second component \(b\) is a tuple of the form \((s, L_{j+1} \ldots L_k)\) where
            \begin{compactitem}
                \item \(s\) is the supertag at the root of the investigated derivations, and
                \item \(L_{j+1}, \ldots, L_k \subseteq [\ell]\) are the sets of sentence positions in successor derivations that were already considered during the investigation such that the sets are disjoint and \(L_{j+1} \cup \ldots \cup L_k \subseteq L\).
            \end{compactitem}
            The tuple \(b\) is called a \emph{backtrace} and used to reproduce the represented derivations.
            This corresponds to the ``list of daughters'' stored with the parse items in \abrv{bl}.
        \item
            The third component \(w \in \DR\) is the weight for the parse item.
            As the description in \abrv{bl} was unweighted, this part does not occur there.
    \end{itemize}
    In the following, we call such an element \((a,b,w)\) a \emph{parse state}.

    \paragraph{Data collections.} The procedure \textsc{fillChart} uses four collections to store (intermediate) results:
    \begin{itemize}
        \item The set \(Q\) contains parse states; it is used as a priority queue that stores parse states until they are further investigated.
            The weight \(w\) in each parse state decides the order in which parse states are drawn from \(Q\).
        \item A \emph{parse chart} \(C\) is an \(N \times \mathcal{P}([\ell])\)-indexed family of subsets of \(S \times \mathcal{P}([\ell])^*\); it stores a backtrace for each derivation that was found in the process.
            For each \(L \subseteq [\ell]\) and \(A \in N\), the set \(C_{(A, L)}\) contains backtraces of the form \((s, L_1 \ldots L_k)\) where \(s = (A \to c\,(B_1, \ldots, B_k), f)\) is the root of an admissible derivation containing the lexical symbols in \(L\).
            The combinations \((B_1, L_1), \ldots, (B_k, L_k)\) recursively reference the successor derivations stored in \(C\).
            Whenever we draw a passive item for an admissible derivation from the queue, its backtrace is stored in \(C\) (lines 12--13).
        \item The \(N\times \mathcal{P}([\ell])\)-indexed family \(W\) of elements in \(\DR\) stores the weight of the investigated derivations.
            For each \(L \subseteq [\ell]\) and \(A \in N\), the value \(W_{(A, L)}\) is the best weight of any derivation rooted in a supertag with the \abrv{lhs} nonterminal \(A\) and the set of lexical symbols \(L\).
            Such as \(C\), this collection is updated each time we investigate a passive item (line 14).
        \item An \(N\)-indexed family of sets of parse states \(I\) such that \(I_{\hat{B}}\) contains parse states with an active parse item of the form \((A, f, B_1\ldots B_{j-1}\hat{B},L)\); that is, \(\hat{B}\) is the last element in the sequence of successor nonterminals and the next to investigate.
    \end{itemize}

    \paragraph{Procedure discussion.} We distinguish two parts in the procedure \textsc{fillChart} shown in \cref{alg:parsing:chart}:
    \begin{itemize}
        \item First, there is a setup in lines 2--9 where the four collections \(C, W, I\) and \(Q\) are initialized (lines 2--4), and the queue \(Q\) is equipped with parse states containing an initial active item for each supertag (lines 6--9).
            The active items are constructed from the supertags by adopting the \abrv{lhs} nonterminal and fanout restriction from the supertag.
            The sequence \(B_1' \ldots B_{k+1}'\) is obtained from the \abrv{rhs} nonterminals and one symbol \(X_i\) for the lexical symbol \(i \in \ell\) occurring in the supertag.
            These symbols are ordered according to their corresponding variables and the lexical symbol in \(c\).
            The last component of each active item starts with an empty set of sentence positions.
            This initialization of active items corresponds to the ``Predict'' inference rule in \abrv{bl}.
        \item
            The remainder in lines 10--23 contains a loop investigating each parse state added to the queue \(Q\).
            It distinguishes three cases for each parse state popped from \(Q\):
            \begin{compactitem}
                \item In the first case (lines 12--16), the parse states contains a passive parse item for an admissible derivation.
                    It is added to the collections \(C\) and \(W\).
                    After that, the loop in lines 15--16 consults the collection \(I\) to re-iterate all parse states with active items that need to be investigated using a passive item with matching \abrv{lhs} nonterminal \(A\).
                    The function \textsc{addSucessor} constructs a follow-up parse item and stores it in the queue \(Q\).
                \item The second case (lines 17--19) adds the lexical symbol to the set of leaves and constructs a follow-up parse item.
                \item The final case (lines 20--23) considers an active item that represents an incomplete derivation and shall be supplemented with a subderivation for the right-most nonterminal \(B_k\).
                    The tuple is stored in \(I\) indexed by the nonterminal \(B_k\).
                    Finally, in lines 22--23, the function \textsc{addSuccessor} is used again to construct follow-up items with the information about the investigated passive items stored in \(C\) and the weights in \(W\).
            \end{compactitem}
            The first and third case correspond to the ``combine'' inference rule in \abrv{bl}.
    \end{itemize}
    In contrast to \abrv{bl}, we investigate the parse states in reverse order of the \abrv{rhs} nonterminals.
    This is due to the property \cref{prop:dcp:admissible:order} of admissible \abrv{dcp} supertag derivations; that is, for each derivation of the form \(d\,(d_1, \ldots, d_k)\), the successor derivations \(d_1, \ldots, d_k\) must be orderd according to their least lexical symbols.
    For each step during the investigation and in reverse order, this boils down to the check in line 25 (\(\min L' < \min L\)).
    If we investigated in order from left to right in the sequence of \abrv{rhs} nonterminals, then we needed to additionally store the least lexical symbol from the previous investigation step.

    \paragraph{Properties of the Output.} After executing the procedure \textsc{fillChart}, the objects \(W\) and \(C\) are as follows:
    \begin{itemize}
        \item For each admissible derivation \(d \in \widehat{\derivs}^S\) rooted in \(s = (A \to c\,(B_1, \ldots, B_k), f)\) that contains the lexical symbols \(L\), there is a backtrace \((s, L_1 \cdots L_k) \in C_{(A, L)}\) such that \(L_1, \ldots, L_k \subseteq L\) are the sets of lexical symbols in the successors of \(d\).
        For each \(A\in N\) and \(L \subseteq [\ell]\), there are no other elements in \(C_{(A,L)}\). Specifically, if there is no such derivation starting with a supertag with the \abrv{lhs} nonterminal \(A\) and that contains the lexical symbols \(L\), then \(C_{(A,L)} = \emptyset\).
        \item For each \(A \in N\) and \(L \subseteq [\ell]\), the value \(W_{(A, L)} = \max_{d \in \widehat{\derivs}^S_A(L)} \weight(d)\) is the best weight of any derivation rooted in a supertag with the \abrv{lhs} nonterminal \(A\) and the lexical symbols \(L\). If there is no such derivation, then \(W_{(A, L)} = -\infty\).
    \end{itemize}

    \begin{algorithm}
        \caption{\label{alg:parsing:deriv}
            Illustration for the enumeration of $n$ best derivations from a parse chart obtained as illustrated in \cref{alg:parsing:chart}.
            This is a direct adaption of the algorithm presented by \citet{HuaChia05}.
        }
        \subfile{algorithms/read-derivation.tex}
    \end{algorithm}

    \subsubsection*{Step \ref{step:parsing:derivation}: Reading Derivations from the Parse Chart}
    The listing of \cref{alg:parsing:deriv} shows how the parse chart \(C\) and the weight assignment \(W\) obtained in the previous step are used to enumerate derivation trees with respect to their weight.
    The algorithm is derived from the \(n\)-best parsing algorithm introduced by \citet[Algorithms~1, 2 and 3]{HuaChia05} with some adjustments to accommodate our grammar format and notation (e.g.\@ non-binary grammars and the specific form of \abrv{dcp} supertags).\footnote{
        \Citet{HuaChia05} described their algorithms in terms of weighted hypergraphs that represent all derivations within a grammar instance for an input sentence; that is, such a weighted hypergraph denotes a parse chart.
        Here, we replace their notation by the parse chart denoted by the families \(C\) and \(W\) from the previous step.
    }
    In the following, we will refer to this publication as \abrv{hc} when we compare the two algorithms.
    Aside the two structures \(C\) and \(W\) from the previous step, the algorithm expects the following inputs:
    \begin{inparaitem}[]
        \item the length \(\ell \in \DN_+\) of the input sentence,
        \item the set of supertags \(S\)  with the set of nonterminals \(N\) and the initial nonterminal \(I \in N\),
        \item the weight assignment \(\weight\colon S \to \DR\) for each supertag, and
        \item the number of best derivations \(n \in \DN_+\) to enumerate.
    \end{inparaitem}
    As for the previous step, we describe the algorithm first by explaining the structures and meaning of the occurring objects, then we discuss the procedure shown in the listing, and lastly, we discuss the properties of the output.
    
    \paragraph{Indexed parse items and indexed backtraces.}
    Within the listing, the following two types of objects are commonly used:
    \begin{itemize}
        \item
            Firstly, there are tuples of the form \((A, L, m)\) where \(A \in N\) is a nonterminal, \(L \subseteq [\ell]\) is a set of sentence positions, and \(m \in [n]\) is a positive integer less or equal \(n\).
            This form of tuples occurs in the subscript of the families \(D\) and \(W'\) and as parameters of the functions \textsc{LazyKthBest} and \textsc{readDerivation}.
            They extend the passive parse items from the previous step by the index \(m\).
            Each occurrence of such a tuple is used to reference the \(m\)th derivation rooted in a supertag with the nonterminal \(A\) containing the lexical symbols \(L\).
            Such tuples are called \emph{indexed parse items}.
        \item
            Secondly, there are tuples of the form \((s, \vec{L}, \vec{m})\) where \(s\in S\) is a supertag, \(\vec{L} \in \mathcal{P}([\ell])^*\) is a sequence of sets of sentence positions and \(\vec{m} \in [n]^*\) is a sequence of positive integers less or equal \(n\) such that the length of \(\vec{L}\) and \(\vec{k}\) is the arity of the \abrv{dcp} rule in \(s\).
            These tuples occur as parameters of the functions \textsc{LazyNext} and \textsc{weight} and as elements in the collections \(H\), $D$ and $E$.
            This notation extends the backtraces from the previous step by an index less or equal to \(n\) for each successor; we call tuples of this form \emph{indexed backtraces}; they correspond to the derivations with backpointers in \abrv{hc} (cf.\@ \citealp{HuaChia05}, Definition 7).
            As such, it is used to reproduce a specific derivation rooted in the supertag \(s\) and the children are specified by the indexed parse items \((B_1, L_1, m_1), \ldots, (B_k, L_k, m_k)\) where \(B_1, \ldots, B_k\) are the \abrv{rhs} nonterminals in \(s\),, \(L_1, \ldots, L_k\) are the sets in \(\vec{L}\) and \(m_1, \ldots, m_k\) are the indices in \(\vec{m}\).
    \end{itemize}

    \paragraph{Data collections.}
    Four collections are used throughout the algorithm:
    \begin{itemize}
        \item \(D\) is an \(N \times \power([\ell]) times [n]\)-indexed family of indexed backtraces (i.e.\@ elements in \(S \times \power([\ell])^* \times [n]^*\)).
            For each indexed parse item that is explored during the algorithm, it stores an indexed backtrace to reconstruct the referenced derivation.
            This collection is denoted using the identifier \(\hat{\mathbf{D}}\) by \abrv{hc}.
        \item \(H\) is an \(N \times \power([\ell])\)-indexed family of sets of indexed backtraces (i.e.\@ subsets of \(S \times \power([\ell])^* \times [n]^*\)).
            Each of these sets is used as a priority queue that stores an agenda for the next best derivations.
            The priority of each element is the sum of weights in the derivation determined by the indexed backtrace; it is computed in the function \textsc{weight} (lines 5--6 and 29).
            These priority queues are called \(\mathit{cand}\) in \abrv{hc}'s algorithms.
        \item \(W'\) is an \(N \times \power([\ell]) \times [n]\)-indexed family of weights in \(\DR\) that extends the weight assignment \(W\) obtained in the previous step from passive parse items to indexed parse items.
            For each \(A\in N\), \(L \subseteq [\ell]\) and \(m \in [n]\), it stores the weight of the \(m\)th best derivation rooted in a supertag with \abrv{lhs} nonterminal \(A\) and with the lexical symbols \(L\).
        % \item
        %     \(E\) is an \(N \times \power([\ell])\)-indexed family of sets of indexed backtraces (i.e.\@ subsets of \(S \times \power([\ell])^* \times [n]^*\)).
        %     For each \(A\in N\) and \(L \subseteq [\ell]\), the set \(E_{(A, L)}\) stores all elements that were added to the priority queue \(H_{(A, L)}\); it is used to avoid adding t
    \end{itemize}

    \paragraph{Procedure and function discussion.}
    There are three functions and one procedure in the listing:
    \begin{itemize}
        \item The function \textsc{weight} computes the weight of a derivation identified by an indixed backtrace.
            It uses pre-computed values for the successors stored in \(W'\); these values are determined during the initialization (line 3) or during the execution of \textsc{LazyKthBest} (line 24).
        \item The procedure \textsc{LazyNext} determines a set of follow-up indexed backtraces; each of these follow-ups is constructed from a given indexed backtrace by incrementing exactly one index \(i \in [k]\) among the \(k\) successors (line 9).
            Each follow-up indexed backtrace is inserted into the priority queue \(Q\) (line 11) if there is the necessary \((m_i+1)\)th derivation for the \(i\)th successor.
            This existence is determined with the function \textsc{LazyKthBest} (line 10).
            This procedure works in the same manner as the procedure of the same name introduced by \abrv{hc}.
        \item The function \textsc{LazyKthBest} determines if a derivation for an indexed parse item \((A, L, m)\) exists and computes the necessary intermediate results to store the indexed backtrace that reproduces this derivation in \(D_{(A, L, m)}\) as well as its weight in \(W_{((A, L, m))}\).
            The function computes the number of derivations \(m'\) that are already stored as indexed backtraces within the family \(D\) for the root nonterminal \(A\) and the sentence positions \(L\) (line 13).
            It initializes a queue \(H_{(A, L)}\) of indexed backtraces (with all indices set to 1) for each backtrace in the parse chart \(C\) (lines 14--15) if it has not been initialized before.
            The loop starting at line 16 determines one derivation in each iteration and stores the corresponding indexed backtrace until the \(m\)th is found.
            During the loop, the priority queue \(H_{(A, L)}\) is repopulated in lines 17--18:
                If there is at least one indexed backtrace for \(A\) and \(L\) in \(D\), then it adds the each indexed backtraces where exactly one index is incremented using the procedure \textsc{LazyNext}.
            If the heap stays empty after this step of re-population, then there are no further candidates, and we abort in lines 19--20.
            Otherwise, there we determine the next best derivation identified by the best weighted indexed backtrace in the priority queue \(H_{(A, L)}\) (line 21).
            It is stored accordingly in \(D\), and its weight in \(W'\) (lines 23--24).
            This function takes the role of the procedure \textsc{LazyKthBest} by \abrv{hc}.
        \item \textsc{readDerivation} reconstructs a derivation for an indexed parse item \((A, L, m)\) from the indexed backtraces stored in \(D\).
            It acquires the indexed backtrace at \(D_{(A, L, m)}\) and keeps its supertag as the root of the derivation (lines 27 and 30).
            The successor derivations are reconstructed in recursive calls of the function for each child of the indexed parse item (lines 28--29).
    \end{itemize}

    \paragraph{Properties of the functions and output.}
    The two outputs of \cref{alg:parsing:deriv} consider the set of \(n\)-best sequences of admissible \abrv{dcp} supertag derivations \(\mathcal{D} = \argmax^{(n)}_{\hat{d} \in \widehat{\derivs}^S([\ell])} \weight(\hat{d})\).
    The integer \(n' \in \DN\) is the number derivations \(|\vec{d}'| = \min(n, |\widehat{\derivs}^S([\ell])|)\) in each sequence \(\vec{d}' \in \mathcal{D}\) that is either limited by the input \(n \in \DN_+\) for the algorithm or by the actual size of the set of admissible derivations.
    The sequence \(\vec{d}\) is of the length \(n'\) and such that the following properties are satisfied:
    \begin{compactitem}
        \item Each element in the sequence is an admissible derivation in \(\widehat{\derivs}^S\), beacause the function \textsc{readDerivation} exclusively constructs admissible derivations.
        \item For each \(A\in N\) and \(L \subseteq [\ell]\), function calls of the form \(\Call{readDerivation}{A, L, i}\) construct derivations with descreasing weight assigned by \(\weight\) in the order of rising values for \(i\); thus, the elements in $\vec{d}$ are ordered with decreasing weight assigned by \(\weight\). That is, or the sequence \(\vec{d} = d_1 d_2 \ldots d_{n'}\), the elements are ordered by \(d_1 \geq d_2 \geq \ldots \geq d_{n'}\). Moreover, each other derivation \(\hat{d} \in  \widehat{\derivs}^S([\ell]) \setminus \{d_1, \ldots, d_{n'}\}\) has a smaller or equal weight \(d_{n'} \geq \hat{d}\).
    \end{compactitem}
    Thus, the sequence \(\vec{d}\) is an element of \(\mathcal{D}\).

    \subsubsection*{Some Notes on the Implementation of the Algorithms}
    In our implementation, the algorithms above are augmented with techniques well-established either generally in programming or specifically in parsing.
    The most notable are as follows:
    \begin{itemize}
        \item Throughout the \cref{alg:parsing:chart,alg:parsing:deriv}, sets of sentence positions are used as components of parse items and indices of the collections.
            Each non-empty set of sentence positions \(L \subseteq [\ell]\) is expressed as a sequence of \emph{spans} of the form \(((l_1, r_1), \ldots, (l_s, r_s))\) such that \(s\in \DN_+\) is the fanout of the set \(L\), and, for each \(i \in [s]\), the span \((l_i, r_i)\) is the tuple of the first and last element in the sequence \(w_i\) in the linearization \((w_i \mid i \in [s])\) of \(L\).
            Thus, such a sequence must contain non-overlapping and non-bordering spans in increasing order from left to right; each pair of neighboring indices is strictly increasing: \(r_i < l_{i+1}\) for \(i \in [s-1]\).
            Sequences of spans are a common technique in discontinuous parsing \citep[e.g.][cf.\@ range vector in Definition~6.6]{Kal10}.
            These span sequences are more efficient in storing the sets of positions and computing the union when \(s\) is much smaller than the set of positions in the spans (which is usually the case).
            Moreover, the minimum \(l_1\) and the fanout \(s\) are easily accessible.
        \item The queues \(Q\) in \cref{alg:parsing:chart} and \(H_{(A,L)}\) in \cref{alg:parsing:deriv} are implemented using the \emph{heap} data structure.
            It allows us to access the minimal element efficiently.
        \item Objects that occur repeatedly without any structural investigation are mapped to (and replaced by) integers for efficient storage and comparisons; this process is called \emph{integerizing}.
            A trivial example is nonterminal symbols, which are only used in comparisons to find matching rules during the parsing process.
            Also tuples of the form \((A, L)\) for each nonterminal \(A\) and set of sentence positions \(L\) are integerized during the implementation of both \cref{alg:parsing:chart,alg:parsing:deriv} becuase they are frequently used as indices.
        \item The collections expressed as families in both \cref{alg:parsing:chart,alg:parsing:deriv} are implemented either as \emph{hash maps} or as \emph{lists} when the index is integerized.
            Even though they are denoted with an initial value for each index in both \cref{alg:parsing:chart,alg:parsing:deriv}, they are empty at the start and assume these values implicitly if there is no entry for the index.
            Otherwise, collections with indices containing a subset of sentence positions like \(C\) and \(W\) would be highly infeasible.
    \end{itemize}
\end{document}