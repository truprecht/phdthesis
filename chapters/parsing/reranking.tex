\documentclass[../../document.tex]{subfiles}

\begin{document}
    \section{Reranking}\label{sec:reranking}
    This section focuses on a refinement procedure to pick one final constituent tree among a sequence of \(n \in \DN_+\) parses.
    A \deflab{reranking} process is an empirical prediction that assesses a previous empirical prediction by assigning a new confidence value to each label in conjunction with its prior confidence value.
    In the case of reranking in the field of parsing, it takes the predicted \(n\)-best sequence of parses, each complemented with the score of the parse, and computes a new score for each parse tree; we use the best among these scores to pick the final parse tree.
    The concept of reranking has its tradition in information retrieval and recommender systems. Both are disciplines in machine learning that focus on finding matching information resources for queries or suggestions for individuals, respectively.
    Reranking is used in this context to refine matches obtained in a query by a prior prediction model \citep{carbonell1998use,adomavicius2009toward}.

    Formally, we define a \emph{reranking process} as follows:
        Let \(Y\) be a set of labels and \(\mathcal{C}\) a set of confidence values.
        A reranker is a function \(\psi \colon Y \times \mathcal{C} \to \mathcal{C}\) that assigns a confidence value to each pair of a label and \emph{prior confidence value}.
    Often, only a tiny fraction \(\hat{Y} \subseteq Y\) of the set of labels, called \emph{search space}, is considered for the reranking process.
    In the case of reranking in parsing, the search space is the collection of constituent trees within an \(n\)-best sequence and the reranking process is used to find the highest scoring among those.
    Consider a sequence of \(n\) labels \((t_1, \ldots, t_n) \in Y^n\) and an assignment of prior confidence values \(\phi \colon Y \to C\).
    We call a label \(t\) that maximizes the reranker \(\psi\)'s confidence value a \deflab{pick among a sequence}[\(\psi\)-pick among \(t_1 \ldots t_n\)], that is, \(
        t \in \argmax_{\hat{t} \in \hat{Y}} \psi(\hat{t}, \phi(\hat{t}))
    \) where \(\hat{Y} = \{t_i \mid i \in [n]\}\) is the search space.
    \Cref{sec:references:reranking} gives a summary of the literature for reranking procedures applied in the field of parsing constituent trees.

    In this work, we rerank constituent trees using the \deflab{discontinuous data-oriented parsing} \defabrv{discontinuous data-oriented parsing}{disco-dop} formalism.
    Disco-dop has been characterized as an instance of \abrv{lcfrs} with latent annotations \citep[Section~4]{Cra11} as well as hybrid grammars \citep[Section~8.5.1]{Geb20}.
    The formalism was introduced by \citet{Cra11} as a generalization of data-oriented parsing \citep{Bod92} to the case of discontinuous constituent structures.
    Such grammars consist of constituent tree fragments with substitution sites at leaves that carry nonterminal symbols.
    To accommodate discontinuities, each tree fragment is complemented by an \abrv{lcfrs} composition that restricts the substitution in consideration of the terminal symbols occurring in the tree fragments.
    \Cref{fig:ex:dop} shows an example of a disco-dop grammar.
    Following \citet{San11}, \citet{Cra11}, we use the \emph{double dop} extraction approach for discontinuous constituent trees.
    It extracts rules for each constituent tree fragment in the training set that occurs at least twice.
    We restrict the fragments such that they do not include tokens from the constituent trees but end at the \abrv{pos} symbols.
    A weight function assigns a probability to each fragment conditioned on the root constituent symbol; these probabilities are computed via relative frequency estimation.
    We slightly diverge from the weight structure that we use for parsing with the supertags; we do not use the Viterbi-equivalent sum and maximum operations but the probabilistic product and sum operations.
    
    \begin{figure}
        \null\hfill
        \subfile{../figures/example-constituents.tex}
        \hspace{1cm}
        \subfile{figures/example-dop.tex}
        \hfill\null
        \caption{\label{fig:ex:dop}
            The running example constituent tree and five rules of a disco-dop grammar in the notation of discontinuous tree substitution grammars as used by \citet{CraSchBod16}.}
    \end{figure}

    To compute the weight that such a disco-dop grammar assigns to a constituent tree, we follow the interpretation as weighted \abrv{lcfrs} with latent annotation.
    Consider that we extract a disco-dop grammar from a treebank with the set of constituent symbols \(\varGamma\), \abrv{pos} symbols \(\varPi\) and the set of tokens \(\varSigma\).
    Then such an interpretation is an \abrv{lcfrs} of the form \(G = (\varGamma \times L, \varPi, R, N_0)\) and weight function \(\weight\colon R \to \DR\)  where
    \begin{itemize}
        \item  \(L\) is a non-empty and finite set of latent symbols (e.g., we use a non-empty integer index set \(L \subset \DN_+\), its size depends on the number of extracted tree fragments),
        \item \(R\) is the set of rules for the extracted tree fragments, and
        \item \(N_0 \subset \varGamma \times \{l_0\}\) is a set of constituent symbols appearing at the root supplied with a distinct latent symbol \(l_0\) (in our case \(l_0 = 1\)).
    \end{itemize}
    The rules in \(R\) are of the same form as the usual treebank \abrv{lcfrs} (cf.\@ \cref{sec:extraction:lcfrs}, Step \ref{enum:lcfrs:step2}) except that \abrv{pos} symbols occur as lexical symbols in the rules, and the nonterminal symbols are extended using the latent symbols to accommodate the tree fragments that are split into multiple rules.
    The latent symbol \(1 \in L\) occurs at each nonterminal for the root and leaves of such a tree fragment; we denote the nonterminal \((A, \lambda)\) with \(A\in \varGamma\) and \(\lambda \in L\) in the form \(A^{(\lambda)}\).
    \Cref{ex:dop} shows examples of such tree fragments in a disco-dop grammar and an interpretation as \abrv{lcfrs} rules with latent annotations.
    The process to determine the weight of a constituent tree \(\xi \in \Xi_{\varGamma, \varPi, \varSigma}\) assigned by the disco-dop grammar \(G\) is as follows:
    \begin{enumerate}
        \item First, the constituent structure of \(\xi\) is transformed into a \abrv{lcfrs} derivation \(d\) (as in Step \ref{enum:lcfrs:step2} of the extraction in \cref{sec:extraction:lcfrs}), and the lexical symbols are replaced by the corresponding \abrv{pos} symbols of the constituent tree \(\xi\). The rules in \(d\) have nonterminals in \(\varGamma\) and terminals in \(\varPi\).
        \item For each position \(\rho\) in \(d\) (in the direction from bottom to the top) and each latent symbol \(\lambda \in L\), we compute the weight \(w_\rho^{(\lambda)} \in \DR\). Let the subtree \(d|_\rho\) be of the form \(A \to c\,(B_1, \ldots, B_k)\;\big(d_1, \ldots, d_k)\), then the weight is as follows:
            \[
                w_\rho^{(\lambda)} = \sum_{A^{(\lambda)} \to c\,(B_1^{(\lambda_1)}, \ldots, B_k^{(\lambda_k)}) \in R} \weight(A^{(\lambda)} \to c\,(B_1^{(\lambda_1)}, \ldots, B_k^{(\lambda_k)})) \cdot \prod_{i \in [k]} w_{\rho\cdot i}^{(\lambda_i)} \text{.}
            \]
        \item The weight of the constituent tree \(\xi\) is defined as \(\weight(\xi) = w_\varepsilon^{(1)}\).
    \end{enumerate}
    This process is equivalent to the weight computation in a probabilistic bottom-up tree automaton with treebank \abrv{lcfrs} rules as terminal symbols and latent symbols in \(L\) as states.
    
    We implement the disco-dop grammar into our parsing process as follows:
    \begin{itemize}
        \item The weighted disco-dop grammar \((G,\weight)\) is extracted from the training portion of the treebank (the same trees that are used to extract the supertag blueprints).
        \item We define the reranker \(\varPsi\colon \Xi_{\varGamma, \varPi, \varSigma} \times \DR \to \DR\) as \(\varPsi(\xi, v) = \weight(\xi)\) where \(\DR\) is the domain of our confidence values. It does not use any prior scores and solely relies on the probability assigned by the disco-dop grammar.
        \item Let \(\xi_1 \ldots \xi_n \in {(\Xi_{\varGamma, \varPi, \varSigma})}^n\) be a sequence of constituent trees of length \(n\in \DN_+\) and the input to the reranking process.
        The \(\varPsi\)-pick among \(\xi_1 \ldots \xi_n\) is the constituent tree with the greatest probability assigned by the weighted disco-dop grammar.
        It is the result of the reranking process and thus the result of our parsing process.
    \end{itemize}


    \begin{example}\label{ex:dop}\NoEndMark
        \Cref{fig:ex:dop} shows five rules of a disco-dop grammar that consist of fragments in the constituent tree shown on the left of the illustration.
        The rules are denoted in the form of discontinuous tree substitution grammars as characterized by \citet{CraSchBod16}.
        They denote them similarly to \abrv{lcfrs}:
            Each leaf that is a substitution site is complemented with a sequence of variables, and each leaf that is a terminal has an index, such that each of these objects uniquely occurs within each rule.
            A composition below the root node indicates how terminals may be concatenated by the rules using the variables and indices from the leaves.
            The compositions are tuples, allowing discontinuities in the same sense as in \abrv{lcfrs}.
        
        We interpret the five tree fragments as rules in an \abrv{lcfrs} with latent annotations as follows.
        The set of latent symbols is the integer index set of size 5 (one plus the number of fragments) \(L = [5]\), and the set of constituent symbols is \(\{\cn{vp}, \cn{np} \cn{s}, \cn{sbar}, \cn{wh}, \cn{prt}\}\); thus, the set of nonterminals symbols is \(
            \{ A^{(\lambda)} \mid A \in \{\cn{vp}, \cn{np} \cn{s}, \cn{sbar}, \cn{wh}, \cn{prt}\} , \lambda \in [5]\}  \text{.}
        \)
        The following are the \abrv{lcfrs} rules for the five fragments:
        \begin{align*}
            &\cn{sbar}^{(1)} \to (\x^1_1)\;(\cn{s}^{(2)}),\quad
                \cn{s}^{(2)} \to (\x^1_1\,\x_2^1\,\x_1^2)\;(\cn{vp}^{(1)}, \cn{np}^{(1)}), \tag{$\zeta_1$}\\[.2cm]
            &\cn{np}^{(1)} \to (\cn{pt}\,\cn{nn})\;() \tag{$\zeta_2$},\\[.2cm]
            &\cn{vp}^{(1)} \to (\x_1^1\,\cn{vbn}\,\x_2^1)\;(\cn{wh}^{(4)}, \cn{prt}^{(4)}),\quad
                 \cn{vbn}^{(4)} \to (\cn{wrb})\;(),\quad
                 \cn{prt}^{(4)} \to (\cn{rp})\;(), \tag{$\zeta_3$}\\[.2cm]
            &\cn{sbar}^{(1)} \to (\x^1_1)\;(\cn{s}^{(5)}),\quad
                \cn{s}^{(5)} \to (\x^1_1\,\x_2^1\,\x_1^2)\;(\cn{vp}^{(1)}, \cn{np}^{(5)}),\quad
                \cn{np}^{(5)} \to (\cn{pt}\,\cn{nn})\;(), \tag{$\zeta_4$}\\[.2cm]
            &\cn{vp}^{(1)} \to (\x^1_1\,\cn{vbd}\,\x_1^2)\;(\cn{vp}^{(1)})\text{.} \tag{$\zeta_5$}
        \end{align*}

        Consider that the five fragments in \cref{fig:ex:dop} are supplied with some probabilistic weights \((\weight(\zeta_i) \in \DR \mid i \in [5])\).
        These weights are assigned to the \abrv{lcfrs} rules for the root of each fragment, such that:
        \begin{align*}
            \weight(\cn{sbar}^{(1)} \to (\x^1_1)\;(\cn{s}^{(2)})) &= \weight(\zeta_1), \\[.2cm]
            \weight(\cn{np}^{(1)} \to (\cn{pt}\,\cn{nn})\;()) &= \weight(\zeta_2), \\[.2cm]
            \weight(\cn{vp}^{(1)} \to (\x_1^1\,\cn{vbn}\,\x_2^1)\;(\cn{wh}^{(4)}, \cn{prt}^{(4)})) &= \weight(\zeta_3), \\[.2cm]
            \weight(\cn{sbar}^{(1)} \to (\x^1_1)\;(\cn{s}^{(5)})) &= \weight(\zeta_4),  \\[.2cm]
            \weight(\cn{vp}^{(1)} \to (\x^1_1\,\cn{vbd}\,\x_1^2)\;(\cn{vp}^{(1)})) &= \weight(\zeta_5) \text{.}
        \end{align*}
        All other rules are assigned to the probability \(1\).

        Let us consider the above process to assign a weight to the example constituent tree using these rules and probabilities.
        First, the constituent tree is transformed into the derivation illustrated below.
        The gray values next to each node show the weights computed at each subtree from bottom to the top and for each latent symbol; all values that are not shown are zero.
        For example, there are the two rules \(\cn{prt}^{(4)} \to (\cn{wrb})\;()\) and  \(\cn{prt}^{(4)} \to (\cn{rp})\;()\), each with the weight \(1\), for the bottom-most nodes at positions \(1\,1\,1\,1\) and \(1\,1\,1\,2\).
        At both of those positions, we consider the weight \(1\) for the latent symbol \(4\) and \(0\) for all other latent symbols.
        For their parent position \(1\,1\,1\), we identify the matching rule \(\cn{vp}^{(1)} \to (\x_1^1\,\cn{vbn}\,\x_2^1)\;(\cn{wh}^{(4)}, \cn{prt}^{(4)})\) and determine the weight \(w_{1\,1\,1}^{(1)} = \weight(\zeta_3) \cdot w_{1\,1\,1\,1}^{(4)} \cdot w_{1\,1\,1\,2}^{(4)} = \weight(\zeta_3)\).
        For the rightmost node at position \(1\,2\), there are two matching rules for the latent symbols \(1\) and \(5\) at the \abrv{lhs} nonterminal; hence, there are the two non-zero weights \(w_{1\,2}^{(1)} = \weight(\zeta_2)\) and \(w_{1\,2}^{(5)} = 1\).

        \vspace{.5cm}

        \null\hfill{}
        \subfile{figures/reranking-example.tex}
        \hfill\exampleqed

    \end{example}
\end{document}