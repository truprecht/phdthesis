\documentclass[../../document.tex]{subfiles}

\begin{document}
    \section{Reranking}
    This section focusses on a refinement procedure that we use to pick one final constituent tree among a sequence of \(k\) parses.
    A \deflab{reranking} process is itself an empirical prediction that assesses a previous empirical prediction by assigning confidence values to each label in conjunction with its prior confidence value.
    In the case of reranking in the field of parsing, it takes the predicted sequence of \(k\) best parses, each complemented with the score of the parse, and computes a new score for each parse tree; we use the best among these new scores to pick the final parse tree.
    The concept of reranking has its tradition in information retrieval and recommender systems, both are disciplines in the field of machine learning that focus on finding matching information resources for queries, or suggestions for individuals, respectively.
    Reranking is used in this context for further refinement of matches that were obtained in a query by a prior prediction model. \citep{carbonell1998use,adomavicius2009toward}
    
    Formally, we define a \emph{reranking} process as follows:
        Let \(Y\) be a set of labels, \(\mathcal{C}\) a set of confidence values.
        A reranker is a function \(\psi \colon Y \times \mathcal{C} \to \mathcal{C}\) that assigns a confidence value to each pair of a label and \emph{prior} confidence value.
    Often, only only a tiny fraction \(\hat{Y} \subseteq Y\) of the set of labels, called \emph{search space}, is considered for the reranking process.
    In the case of reranking in parsing, the search space is the collection of constituent trees in a \(k\) best sequence and the reranking process is used to find the highest scoring among those.
    Let \((t_1, \ldots, t_k) \in Y^k\) be such a sequence of \(k\) labels and \(\phi \colon Y \to C\) an assignment of prior confidence values.
    We call the \(\psi\)-\deflab{pick} among \(t_1 \ldots t_k\) the tree that maximizes the reranker \(\psi\)'s confidence value, i.e. \(
        \hat{t} = \argmax_{t \in \hat{Y}} \psi(t, \phi(t))
    \) where \(\hat{Y} = \{t_i \mid i \in [i]\}\) is the search space.
    
    Reranking has quite a history in parsing.
    One track of research driven by \citet{collins2001convolution,shen2003svm,collins05,huang2008forest} i.a.\@ defined discriminative classifiers that predict assessment scores for constituent trees in the form described above.
    Their reranking models consist of two parts:
        a \emph{feature extraction} procedure that maps each constituent tree into a vector, and a function that maps each vector to a score.
    The latter part was often realized as a support vector machine or perceptron, as they are computanionally efficient in both learning and prediction, and they are interpretable.
    These reranking models are used to score the constituent trees obtained from a parser based on context-free grammars, which are known to be highly ambiguous on their own when it comes to parsing natural languages.
    \citet{collins2001convolution, shen2003svm} define features from the training set of constituent trees by counting the occurring tree fragments.\footnote{
        A tree fragment is a coherent part that occurs a constituent tree.
        Each subtree is a tree fragment, and beyond that a fragment may also end at an inner node.
        E.g.\@ \(\cn{s} (\cn{vp} (\cn{vp} \tn{was}), \cn{np})\) is a fragment in our running example "where the survey was carried out".
        As there is a vast amount of such fragments in a whole treebank, \citet{collins2001convolution} define a kernel function that counts the tree fragments occurring commonly in two constituent trees.
        The use of tree fragments relates this strategy to the approach of \emph{data oriented parsing}. \citep{Bod92}
    }
    \citet{collins05,charniak2005coarse} define a hand-craftet set of feature patterns that are instantiated using the constituent trees in a training set.
    Both of these methods used their rerankers for a pick among an sequence of \(n\) best constituent trees.
    \citet{huang2008forest} introduced an approach that applies the reranking mechanism to a parse chart, practically eliminating the restriction to a limited number of constituent trees.
    However, this is only an approximative solution.
    The general training procedure for the reranker is common among the approaches:
        Parse charts or \(n\) best sequences of constituent trees are obtained from sentences in the training set with an off-the-shelf parser and context-free grammars that are obtained from the training set as well.
        To simulate disjoint data used for obtaining the grammars and parsing the sentences (but both coming from the training set), they define hold-out data by principle of cross validation.
        This obtained data is used to extract the features and train a discriminative classifier for deciding among the candidate parse trees.
        
    Another track that fits into the concept of reranking are coarse-to-fine approaches among grammar formalisms.
    These approaches define pipelines that concists of multiple parsing stages, and a coarser/finer relation on the used grammar formalism or instances, respectively.
    Comparing two stages, the earlier uses a coarser and the later a finer formalism/instance; the later stage uses the results of its predecessors to filter unviable items during its own parsing process; we consider each stage after the first a reranking process that refines the result of the previous one.
    \citet{Cha06} use probabilistic context-free grammars for parsing and define the carser/finer relation over the nonterminal symbols.
    They start with defining a usual (binarized and Markovized) treebank grammar at the finest level, and construct a coarser grammar by replacing nonterminal symbols with equivalence classes.
    This is repeated 3 times in total (so there are 4 stages), the coarsest stage uses merely one nonterminal (next to an artificial root nonterminal).
    \citet{CraSchBod16} uses three different probabilistic grammar formalisms for parsing: context-free grammars, \abrv{lcfrs} and data-oriented parsing; which are considered in that order from coarse to fine.
    Both approaches describe a filtering mechanism by relating nonterminals (and thus parse items) between stages and testing in the finer stage if the related parse item existed or is better than some threshold weight, respectively.
    \citet{Denkinger17} used similar techniques to mimic the coarse-to-fine parsing approach in automata formalisms instead of grammar formalisms.
    Togehter, we implemented a coarse-to-fine parser that utilizes an automata characterization for \abrv{lcfrs} in two stages, one with a context-free approximation and another that refines the results by rejecting inapproprate derivations. \citep{RupDen19} 
    
    In this work, we rerank constituent trees using the \defabrv{discontinuous data-oriented parsing}{disco-dop} formalism, which was characterized as an instance of \abrv{lcfrs} with latent annotations \citep[cf.\@ section~4]{Cra11} and hence hybrid grammars. \citep[cf.\@ section~8.5.1]{Geb20}
    The formalism was introduced by \citet{Cra11} as a generalization of data-oriented parsing \citep{Bod92} to the case of discontinuous constituent structures, as its name suggests.
    The grammars consist of constituent tree fragments with subsitution sites at leaves that carry nonterminal symbols.
    To accomodate the discontinuities, each tree fragments is complemented by an \abrv{lcfrs} composition that restricts the substitution in consideration of the terminal symbols occurring in the tree fragments.
    \Cref{ex:dop} shows an example of a disco-dop grammar.
    Following \citet{San11,Cra11}, we use the \emph{double dop} extraction approach for discontinuous constituent trees.
    It extracts rules for each constituent tree fragment in the training set that occurs at least twice.
    We restrict the fragments such that they do not include tokens from the constituent trees but end at the \abrv{pos} symbols.
    We estimate probabilistic weights for each rule by conditioned on the \abrv{lhs} nonterminal/root constituent symbol.
    However, in contrast to the coarse-to-fine approach described by \citet{Cra11}, we do not define a filtering mechanism based on the parse chart from the previous part of the parsing pipeline, but view the disco-dop reranking as a completely separate process for weighting each single constituent tree in a \(k\) best parsing sequence.
    Therefore, we augment the usual parsing procedure for \abrv{lcfrs} (which can be used for disco-dop) such that it can only produce derivations for the input constituent tree by matching its nodes.
    Moreover, we slightly diverge from the weight structure that we use for parsing with the supertags:
        In this case we do not use the Viterbi-equivalent sum and maximum operations with logarithms of proabilistic values, but the pababilistic product and sum operations.
    These augmentations are straight-forward and we refrain from a further elaboration of the parsing process.
    
    \begin{example}\label{ex:dop}
        \Cref{fig:ex:dop} shows four rules of a disco-dop grammar that consist of fragments in the tree to the left.
        The rules are in the notation used by \citet{CraSchBod16}.
        They denote them in a similar fashion to \abrv{lcfrs}:
            Each leaf that is a substitution site is complemented with a sequence of variables and each leaf that is a terminal with an index, such that each of these objects uniquely occurs within each rule.
            A composition below the root node indicates how terminals may be concatenated by the rules, using the variables and indices from the leaves.
            The compositions are tuples allowing discontinuities just in the same sense as in \abrv{lcfrs}.
        The illustrated rules are able to produce the constituent structure with two different derivations, one starting with the upper left rule and one with the bottom right one.
        The following hybrid grammar rules are equivalend to the above notation: \begin{align*}
            \cn{sbar} &\to (\x_1^1 \, \x_2^1 \, \x_1^2)\;(\cn{sbar} (\cn{s} (x_1, x_2)))\;(\cn{vp}_2, \cn{np}) \\
            \cn{np} &\to (\cn{pt} \, \cn{nn})\;(\cn{np} (\cn{pt}, \cn{nn})) \\
            \cn{vp}_2 &\to (\cn{wrb}, \cn{vbn} \, \cn{rp})\;(\cn{vp} (\cn{wh}(\cn{wrb}),\cn{vbn},\cn{np}(\cn{rp}))) \\
            \cn{sbar} &\to (\x_1^1 \, \cn{pt} \, \cn{nn} \, \x_1^2)\;(\cn{sbar} (\cn{s} (\cn{vp}_2(x_1),\cn{np}(\cn{pt}, \cn{nn}))))\;(\cn{vp}_2)
        \end{align*}
    \end{example}
    
    \begin{figure}
        \null\hfill
        \subfile{../figures/example-constituents.tex}
        \hspace{1cm}
        \subfile{figures/example-dop.tex}
        \hfill\null
        \caption{\label{fig:ex:dop}
            The running example constituent tree and four rules of a disco-dop grammar with derivations for the constituent structure and its sequence of \abrv{pos} tags.}
    \end{figure}
\end{document}