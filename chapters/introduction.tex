\documentclass[../document.tex]{subfiles}

\begin{document}
    \chapter{Introduction}
    The fascination for natural languages shared by many in computer science is probably because language is seen as the carrier of human knowledge and information.
    \Citet{Turing2009} drew a conclusion that can be seen as a primer on the assessment of artificial intelligence:
    \begin{quote}
        \emph{``A computer would deserve to be called intelligent if it could deceive a human into believing it was human.''} \hfill ---Alan Mathison Turing
    \end{quote}
    This quote can quickly be drawn in connection to the \emph{imitation game}, later renamed \emph{Turing Test} in honor of \citeauthor{Turing2009}'s pioneer work.
    The Turing Test is successful if a computer's answers to natural language questions cannot be distinguished from those of a human.
    It was the first attempt to measure the quality of a system that generates pieces of natural language.
    But the principle can also be applied to other tasks that are based on natural language data:
        If the results of the tasks for unseen data instances created by a computer cannot be distinguished from those of a human, then we can speak of artificial intelligence.
    One of those tasks is to determine the information carried by a natural language sentence, called a \deflab{semantic analysis}.
    Another one is the \deflab{syntax analysis}, which breaks a sentence down into its components.
    The latter constitutes the application of the methods presented and discussed in this thesis.

    Naturally, a syntactic analysis can hardly be seen as valuable information to gather from a piece of language, besides its beauty as a task in academia.
    It has been part of more sophisticated tasks in the field of natural language processing, such as the translation of sentence between natural languages \citep{Zhang19,Yang22}, relation extraction \citep{Ngu19} and summarization \citep{Bal21}.
    Moreover, it has been shown that syntactic analyses can be used to improve the performance of neural models in a wide range of tasks settled in the field of natural language understanding, such as semantic role labeling and natural language inference \citep{Liu19, Wang19, Fei20}.
    Of course, this requires high-quality syntactic analyses.

    \begin{figure}
        \null\hfill
        \subfile{figures/example-dependencies.tex}
        \hfill
        \subfile{figures/example-dependencies-discontinuous.tex}
        \hfill\null
        \caption{\label{fig:dependency}
            Two dependency trees for the English phrases ``the survey was carried out at the institute'' and ``where the survey was carried out''.
        }
    \end{figure}

    \begin{figure}
        \null\hfill
        \subfile{figures/example-continuous-constituents.tex}
        \hfill
        \subfile{figures/example-constituents.tex}
        \hfill\null
        \caption{\label{fig:constituent}
            Two constituent trees for English phrases: ``the survey was carried out at the institute'' and ``where the survey was carried out''.
        }
    \end{figure}

    \section*{Analyzing the Syntax of Natural Language Sentences}
    Two major forms express the syntax of natural language sentences: \deflab{dependency trees} (\citealp[Section~2.1]{Kue09}; and \citealp[Section~18.1]{Jur23}) and \deflab{constituent trees} \citep[Section~3]{Cho56}.
    Analyzing and illustrating a natural language phrase in such an expression is called \deflab{parsing}.
    Dependency trees describe relations between the words of a sentence, such as the \emph{lexical head-dependent relation} that identifies the central word of each (sub-)phrase as \emph{lexical head} and connects its subphases as \emph{dependents}.
    \Cref{fig:dependency} shows two examples of dependency trees.
    Each dependency tree consists of the analyzed sentence and an arrow for each word in the sentence.
    The arrow from above indicates the root of the dependency tree.
    Each arrow between two words depicts a lexical head-dependent relation; the arrow's base is at the lexical head, and the tip is at the dependent.
    E.g.\@ ``was'' is the root, and ``survey'' is a dependent of ``was'' in both trees.

    A constituent tree illustrates a hierarchy of (sub-)phrases in a sentence; each phrase is labeled with a common category that denotes its function within the sentence or in a superseded phrase.
    \Cref{fig:constituent} shows two examples of constituent trees.
    In each constituent tree, the analyzed sentence is illustrated at the bottom.
    At the next higher level, there are part-of-speech tags, which constitute the most simple syntactic structures in natural language and classify the role of each single word.
    Each symbol beyond that denotes a constituent with connections drawn to all words or subphases that are part of the constituent.
    E.g.\@ in the first tree, ``survey'' is a word in the sentence, ``\cn{nn}'' is its assigned part-of-speech tag \emph{noun}, and the symbol ``\cn{np}'' above it denotes a \emph{noun phrase} that consists of ``the survey''.
    These two expressions for the syntax are not mutually exclusive, and we will later consider the lexical head as an extension, but for now, we focus on constituent trees.
%    We will sometimes consider constituent trees where each constituent is extended by a head assignment, including a head-dependent relation.
%    In that case, the constituent trees are illustrated using double-struck edges that indicate the word in the sentence that acts as the head of the phrase.
%    For each word in the sentence and for each constituent where the word is the head, the head of each child constituent is then dependent if it is unequal.

    There are structural differences between the two constituent trees in \cref{fig:constituent}:
        The second one contains \emph{crossing branches} between the constituent symbols; thus, some constituents have non-contiguous sentence parts.
    We call such a constituent tree \deflab{discontinuous}.
    In the first one, all constituents are contiguous, and we call it \deflab{continuous}.\footnote{
        A similar phenomenon also exists in dependency trees, but its terminology is slightly different.
        The left tree in \cref{fig:dependency} does not contain any crossing edges; it is \emph{projective}; the right one is \emph{non-projective}.
    }
    The example shows a well-known phenomenon in the syntax of the English language, called \cn{wh}-movement that occurs in phrases with interrogative words.
    European languages such as Dutch, German, and Swiss-German are infamous for their discontinuous syntax \citep{Shieber85,Becker91}.
    There are \deflab{treebanks}, extensive collections of constituent trees created manually or semi-automatic, that give an impression of what frequency such phenomena occur in these languages \citep{Marcus94,EvaKal11,Skut98,Brants04,Noo13}.

    The theory of natural language syntax analysis is due to \citet{Cho56}.
    \Citeauthor{Cho56} formalized a rule-based framework for the syntax-based definition of languages, called \deflab{grammars}, and the famous \emph{Chomsky hierarchy} of grammar formalisms.
    \Citeauthor{Cho56} aimed to explain the syntax of natural languages using \deflab{context-free grammars}, the second out of four levels of grammars in the hierarchy.
    These grammars were defined with rules that directly model the parent-child relationship within constituent trees, such as \(\nt{S} \to \nt{NP}\,\nt{VP}\).
    However, context-free grammars cannot model the discontinuities that appear in natural language \citep{Shieber85}.
    The grammar formalism for the next higher level in the hierarchy, called \deflab{context-sensitive grammars}, was dismissed for the modeling and analysis of natural languages due to its complexity.
    As a consequence, there have been multiple proposals for formalisms that aim to add just enough power to context-free grammars to be able to handle discontinuous constituents and still allow efficient analyses:
        tree-adjoining grammars \citep{JosLevTak75}, multiple context-free grammars \citep{SekMatFujKas91}, linear context-free rewriting systems \citep{VijWeiJos87} and combinatory categorial grammars \citep{Ste11} to name a few.
    These formalisms were later brought under the umbrella term \deflab{mildly context-sensitive grammars}, a class that supersedes context-free and is strictly included in context-sensitive grammars.
    Later chapters of this thesis deal with such mildly context-sensitive grammar formalisms, particularly linear context-free rewriting systems and an extension of them, called hybrid grammars \citep{Ned14}.

    \section*{Autonomous Parsing of Natural Languages Sentences}
    Grammar formalisms have been used for parsing natural language sentences.
        For instance, context-free grammars are defined to model the direct parent-children relationships in constituent structures.
        They can be used to derive the constituent structure for a given sentence (\citealp{Cho56}, Section~3; \citealp{Jur23}, Chapter~17).
        The formalism is extended with probabilistic weights to estimate the adequacy of constituent structures (\citealp{Sup72}; \citealp{Jur23}, Appendix C).
    Since the publication of treebanks, it has been possible to extract grammars from the constituent trees automatically \citep{Cha96}.
    The independence assumptions in the earlier formulations of context-free grammars were identified as a limitation for the accuracy when parsing with grammars extracted from treebanks \citep[e.g.\@][Section~1.1]{collins2001convolution}.
    There has been research in extending the grammars to overcome this limitation, such as history-based grammars by \citet{Black94} and data-oriented parsing by \citet{Bod92}; most of these extensions were subsumed under the formalism of context-free grammars with latent annotation by \citet{Mat05} and \citet{Petrov06}.
    Others have approached the ambiguity of context-free grammars by cherry-picking a convincing constituent tree among many derived ones \citep{Col00}.
    However, these developments only consider continuous constituent structures.
    
    The research on parsing with grammars that can model discontinuities has been delayed by a few years and distributed across several concurrent formalisms.
    Starting with \citet{MaierSogaard08} and \citet{Kal10}, parsing procedures for natural languages with multiple context-free grammars have been studied.
    Similar refinements as for parsing with context-free grammars have been introduced to the formalism, such as latent annotations by \citet{Geb20} and discontinuous data-oriented parsing by \citet{Cra11}.
    Some of these extensions are under criticism for undermining the interpretability of the underlying grammar instances \citep[Chapter 9]{Geb20}.

    Other research involves parsers that do not rely on grammars; for instance, parsers modeled as transition systems by \citet{Verseley14} or \citet{CoaCoh19}, or conditional random fields by \citet{Petrov08}.
    Most notably, since the advent of deep learning, such parsing models based on discriminative classifiers became state-of-the-art in terms of accuracy and parse time; parses solely based on grammar formalisms could not keep up with them.
    Although, there is one significant advantage of grammar formalisms that still convinces some people to put effort into parsing with grammar formalisms:
        Grammar instances are easily interpretable, as they explicitly describe the constituent structures; deep neural networks do not allow any straightforward interpretation of their computation.
    Some approaches combine grammar-based parsers with discriminative classifiers to merge the advantages of both techniques, thus implementing interpretable, accurate, and fast parsing procedures.
    One of them is the topic of this thesis: \deflab{supertagging}.

    \begin{figure}
        \resizebox{\linewidth}{!}{\subfile{figures/parsing-overview-simplified.tex}}
        \todo[inline]{figure layout Ã¼berarbeiten}
        \caption{\label{fig:supertagging}
            A simplified overview of a parsing process with supertagging.
            A discriminative classifier predicts a small set of lexical grammar rules (supertags) per word in a sentence.
            The union of all predictions is assumed as a grammar instance for a parser of the underlying grammar formalism to obtain a rule derivation for the sentence, which is interpreted as a constituent tree.
        }
    \end{figure}

    \section*{Parsing via Supertagging}
    Supertagging was introduced by \citet{bangalore1999supertagging} for tree-adjoining grammars.
    It assumes an underlying \deflab{lexical} grammar formalism, i.e.\@, that contains precisely one lexical symbol in each rule.
    The grammar rules used for supertagging contain a singular lexical symbol that acts as a wildcard; we call these rules \deflab{supertag blueprints}.
    Supertagging refers to predicting a small sample of supertag blueprints for each lexical symbol in the input.
    After prediction, each blueprint's wildcard symbol is replaced with the sentence position it was predicted for; the obtained object is called a \deflab{supertag}.
    The union of all supertags for an input sentence is then used to parse the sentence.
    \Cref{fig:supertagging} illustrates a parsing process that utilizes supertagging.
    It is supposed to tackle the disadvantages of grammar-based parsers as follows:
    \begin{compactitem}
        \item The automatic extraction from treebanks is notorious for generating large sets of rules, and a significant factor for the time needed by grammar-based parsers was identified as the number of rules in the grammar \citep{dunlop2010reducing}.
            When only a tiny sample of grammar rules is used, the parsing process should be much faster.
        \item The small predicted grammars are more concise and less ambiguous than the ones extracted from treebanks.
            Predicting rules with accurate classifiers could beat complex grammar extensions such as latent annotations, and the results of the parsing process should be more accurate. In contrast, the underlying grammar instance is more simplistic.
    \end{compactitem}
    Over the years, the approach has been transferred to other grammar formalisms, most notably combinatory categorial grammars \citep{Clark04}, refined to improve the rule prediction quality \citep{vaswani2016supertagging,Kad18,tian20}, and evaluated in different languages and corpora \citep{Bla18}.
    The published results for parsing with supertagging are promising.
    Alas, supertagging-based parsers were primarily evaluated in the task of dependency parsing (or just the prediction of the grammar rules), or in some cases, the underlying grammar formalisms are restricted in allowing only discontinuity to a certain degree.
    Moreover, extracting the grammar instance for this approach in the needed format often requires language-specific knowledge \citep{Kas17,Bla18} or they used hand-crafted grammar instances \citep{Hoc07}.
    We aimed to tackle these limitations in the research that led to this thesis.
    We published an extraction procedure for lexical linear context-free rewriting systems that is agnostic of the used constituent tree corpus. That is, it does not require any knowledge about the language and allows unbounded degrees of discontinuity \citep{MoeRup20}.
    We used this extraction scheme to implement a parsing procedure for constituent trees with supertagging.
    We achieved state-of-the-art scores in the field of discontinuous parsing, not only compared to grammar-based parsers but also to all other approaches published at the time \citep{RupMoe21,Rup22}.

    \section*{Outline}
    This thesis includes the procedures for extracting and parsing with supertags based on linear context-free rewriting systems.
    These are also extended to the formalisms of hybrid grammars that subsume linear context-free rewriting systems and definite clause programs.
    We start in \cref{sec:preliminaries} with basic notation and definitions used throughout the document; \cref{sec:grammars} recapitulates the involved grammar formalisms.
    \Cref{sec:extraction} defines two formalisms for supertags and extraction procedures:
    \begin{compactitem}
        \item \Cref{sec:extraction:lcfrs} focuses on supertags in terms of linear context-free rewriting systems.
            The extraction procedure involves transporting lexical symbols within derivations read from constituent trees in a treebank with well-established strategies.
            This strategy was introduced in one of our earlier publications \citep{RupMoe21}.
        \item The second approach deals with a generalization of the previous formulation of supertags and their extraction procedure.
            We described this procedure in an earlier publication, but only informally and through examples \citep{Rup22}.
            \Cref{sec:extraction:guided} formalizes the approach using supertags based on the hybrid grammar formalism.
            It gives a modular extraction procedure that parametrizes the lexicalization and extraction of the grammar rules.
            In addition to the formalization, the section adds new options for the extraction parameters.
            One of these new options involves a restriction to a weaker underlying grammar formalism, called definite clause programs \citep{Der85}.
    \end{compactitem}
    The following \cref{chp:parsing} defines the parsing process with supertagging.
    It is based on the original formulation by \citet{bangalore1999supertagging} but adds a new optional refinement step.
    This refinement step is based on a well-established formalism in constituent parsing called discontinuous data-oriented parsing.
    The chapter introduces a statistical parsing procedure for the supertags based on definite clause programs in \cref{sec:parsing}. To our knowledge, this formalism was not used for parsing natural languages before.
    \Cref{sec:experiments} unrolls the experiments we conducted to assess the presented extraction and parsing procedures.
    A significant part of the chapter is dedicated to exploring the sets of parameter values and finding appropriate configurations for the extraction procedures.
    \Cref{sec:results} presents the final configurations and sets the obtained results into the context of the published state-of-the-art discontinuous constituent parsers.
    \Cref{sec:literature} summarizes related work in the area of constituent parsing as well as other parsing objectives, specifically syntactic dependencies and semantic representations.
    The chapter gives references to relevant publications within the parsing community.
    Lastly, we conclude the thesis in \cref{chp:conclusion} with a summary of its contributions and the results presented in the preceding chapters.
    The summary points to some open questions that could be resolved in future work.

    \ifSubfilesClassLoaded{%
        \printindex
        \bibliography{../references}%
    }{}
\end{document}