\documentclass[../document.tex]{subfiles}

\begin{document}
    \chapter{Preliminaries}\label{sec:preliminaries}
    This chapter deals with the mathematical foundations and notation used throughout all chapters.
    \Cref{sec:preliminaries:math} starts with the most basic notions.
    \Cref{sec:preliminaries:trees} deals with all concepts involved in string and tree structures.
    Finally, \cref{sec:preliminaries:ctrees} sets these terms into the context of syntactic parsing of natural languages.


    \section{Mathematical Notation}\label{sec:preliminaries:math}
    Sets are considered with the usual notation \(\subseteq\), \(\subset\), \(\cup\), \(\cap\) and \(\setminus\) for the inclusive and exclusive subset relation, as well as the operations for union, intersection, and difference, respectively.
    \(\emptyset\) is the empty set and \(\power(A)\) is the power set, i.e.\@ the set containing all subsets, of some set \(A\).
    The sets of integers, naturals (including zero), positives, and rationals are denoted by $\DZ$, $\DN$, $\DN_+$, and $\DR$, respectively.
    The \deflab{range} of integers between the pair \(n, m \in \DZ\) with \(n \leq m\) is denoted by \([n,m] = \{i \in \DZ \mid n \leq i \leq m\}\).
    For any positive integer \(n\), the notion \([n]\) abbreviates \([1,n]\), and \([0]\) is the empty set \(\emptyset\).
    A \emph{\(\DN_+\) prefix} is a set \(I \subseteq \DN_+\) such that
    \begin{inparaitem}[]
        \item \(1 \in I\), and
        \item for each \(i > 1\), if \(i \in I\) then \(i-1 \in I\) as well.
    \end{inparaitem}

    \paragraph{Relations.}
    Let \(A\) and \(B\) be some sets.
    The cross product \(A \times B\) is the set of all tuples \(\{(a,b) \mid a \in A, b\in B\}\).
    A \deflab{relation} from \(A\) to \(B\) is a set \(R \subseteq A \times B\).
    In case \(A\) and \(B\) coincide, we call \(R\) an \deflab<relation>{endorelation} on \(A\).
    For two elements \(a \in A\) and \(b \in B\), the term \(a \mathrel{R} b\) denotes the boolean statement \((a,b) \in R\) and \(a \not\mathrel{R} b\) denotes \((a,b) \notin R\).
    A \deflab{total order} on \(A\) is an endorelation \(\rel\) on \(A\) such that for each \(a, b \in A\)
    \begin{inparaenum}
        \item if \(a = b\) then \(a \rel b\),
        \item otherwise either \(a \rel b\) and \(b \not\rel a\), or \(b \rel a\) and \(a \not\rel b\).
    \end{inparaenum}

    \paragraph{Functions.}
    A (total) function from \(A\) to \(B\) is a relation from \(A\) to \(B\) such that for each \(a \in A\), there is exactly one \(b\) with \((a,b) \in f\).
    As usual, we denote the type of such a function by \(A \to B\), a quantification by \(f\colon A \to B\), and the argument-value relation by \(f(a) = b\).
    A partial function from \(A\) to \(B\) is a subset \(f \subseteq A \times B\) such that for each \(a \in A\), there is at most one \(b\) with \((a,b) \in f\).
    The type of a partial function is denoted with an upper half arrow, e.g., \(f\colon A \parto B\).
    The image of a (total or partial) function is the set of elements in \(B\) that are assigned to any value in \(A\), denoted by \(\im(f) = \{b\in B \mid (A \times \{b\}) \cap f \neq \emptyset\}\).
    The domain of definition of a partial function is the set of elements in \(A\) with an assigned value in \(B\), denoted by \(\dom(f) = \{a\in A \mid (\{a\} \times B) \cap f \neq \emptyset\}\).
    In some contexts, a partial function \(f\) is handled in the same manner as a total function from \(\dom(f)\) to \(B\) without further notice.

    \paragraph{Families.}
    An \(A\)-indexed \deflab{family} over \(B\) is a function \(f\colon A \to B\), often  denoted in a builder notation of the form \(\big(\mathrm{term}(a) \mid a \in A\big)\).
    The left-hand side \(\mathrm{term}(a)\) may assume any term and/or equation that depends on the index variable \(a\) to express an element of \(B\), and the right-hand side expresses the domain of the index variable \(a\).
    There may be multiple index variables. In that case, \(A\) is a cross-product of the index variables' domains.
    E.g.\@ if there are two indices \(x\) and \(y\) in the domains \(X\) and \(Y\), then we may also denote them individually in the builder notation as follows: \(\big( f(x,y) \mid x \in X, y \in Y \big)\).
    As an alternative to the builder notation, we also denote the argument-value relation in a form similar to the usual function notation, e.g.\@ an \(A\)-indexed family \(f\) is defined such that \(f_a = \mathrm{term}(a)\) for each \(a \in A\), where \(\mathrm{term}(a)\) is some term that may use the variable \(a\).
    Usually, we identify a family by some variable name and access its elements using this identifier and the index variables in its subscript or function notation.
    If the index set is defined in the builder notation or clear from the context, the term \(A\)-indexed is dropped, and we just call such an object a \emph{family}.

    Families are used implicitly in terms where an associative and commutative binary operation is lifted to a collection of elements.
    For example, the union of \(k\) sets \(\big(U_i \mid i \in [k]\big)\) may be written as \(\bigcup_{i \in [k]} U_i\).
    Besides the union (\(\bigcup\)), we will use such operators for the sum (\(\sum\)), product (\(\prod\)), minimum (\(\min\)) and maximum (\(\max\)).
    If the values and index set of a family coincide in such a notation, then the index is omitted completely.
    For example, we write \(\min [k]\) instead of \(\min_{i \in [k]} i\).
    The minima and maxima are determined according to the usual numerical order.
    In case we encounter different objects or we use any other total ordering \(\unlhd\), we denote it as a superscript, e.g., \(\min^{\unlhd}\).
    \(\argmin\) yields all indices with minimal value, for instance \(\argmin_{i \in [k]} v_i\) is the subset \(M \subseteq [k]\) such that, for each \(\hat{i} \in M\), there is no \(j \in [k]\) with \(v_j \unlhd v_{\hat{i}}\).
    Vice versa, \(\argmax\) yields the indices with maximal value.

    An \(A\)-indexed \deflab<family>{partition} of \(B\) is a \(A\)-indexed family \(p\) over subsets in \(\power(B)\) such hat:
    \begin{inparaenum}
        \item For each pair \(a, a' \in A\) of distinct elements \(a \neq a'\), the assigned subsets are disjoint: \(p(a) \cap p(a') = \emptyset\).
        \item The union of all subsets \(\bigcup_{a \in A} p(a)\) is the set \(B\).
    \end{inparaenum}

    \paragraph{Sequences.}
    An \emph{integer index set} \(I\) is a subset of \(\DN_+\) such that
    \begin{inparaenum}[]
        \item either \(1 \in I\) or \(I = \emptyset\), and
        \item for each \(i \in \DN_+ \setminus \{1\}\), if \(i \in I\) then \(i-1 \in I\).
    \end{inparaenum}
    Let \(I\) denote such an integer index set.
    A \deflab{sequence} over \(A\) is an \(I\)-indexed family over \(A\).
    If there is a \(k\in \DN_+\) such that \(I = [k]\), then we call it either a \emph{finite sequence over \(A\)} or, more specifically, a \emph{sequence over \(A\) of length \(k\)}, otherwise an \emph{infinite sequence}.
    \(A^k\) denotes all sequences over \(A\) of length \(k\).
    When a collection of \(k\) elements in \(A\) is quantified, they are usually enumerated with an ellipsis and separated by commas; this notation of the form \(a_1, \ldots, a_k \in A\) defines an implicit sequence \((a_i \in A \mid i \in [k])\).
    With a collection of fixed elements \(a_1, \ldots, a_k \in A\), the sequence \((a_i \in A \mid i \in [k])\) is abbreviated by an ellipsis without commas \(a_1 \ldots a_k\) (when there is a clear enumeration of items, like the index of the variables \(a_1, \ldots, a_k\)).
    An occurrence of the single element \(a_1\) may also denote the singleton sequence (of length 1).
    The \deflab<sequence>{empty sequence} (of length 0) is denoted by \(\varepsilon\).
    The set of all finite sequences over \(A\) is \(A^* = \bigcup_{k \in \DN} A^k\).
    \(A^+\) denotes the set of all non-empty sequences \(\bigcup_{k \in \DN_+} A^k\).
    The length of a sequence \(w \in A^*\) is denoted by \(|w|\).
    For any pair of sequences \(u,v \in A^*\), the \deflab<sequence>{concatenation} \(u \cdot v\) is the sequence \(w\in A^{|u|+|v|}\) such that \(w_i=u_i\) for each \(i \in [|u|]\) and \(w_{|u|+j} = v_j\) for each \(j \in [|v|]\).
    The concatenation is abbreviated by dropping the dot, and it takes advantage of its associativity to drop parenthesis in terms, so that e.g. \(w_1 w_2 w_3 w_4\) denotes \(((w_1 \cdot w_2) \cdot w_3) \cdot w_4\)
    The concatenation of a whole collection of sequences \((w_i \in A^* \mid i \in [k])\) in order of their indices is denoted with an ellipsis \(w_1 \ldots w_k\).

    A \deflab<sequence>{subsequence} \(v \in A^*\) in \(w\in A^*\) is a sequence of elements occurring consecutively within \(w\).
    The subsequence \(v\) may be located within \(w\) by a pair of indices for its start \(s \in [|w|+1]\) and its end \(e \in [s, |w|+1]\) such that \(v_i = w_{s-1+i}\) for each \(i \in [e-s])\).
    Whenever such a subsequence in \(w\) is identified using its location between the indices \(s\) and \(e\), it is denoted by \(w_{s:e}\).
    If one of the two indices is left empty, it denotes a prefix or postfix of the sequence \(w\).
    E.g. \(w_{:s}\) are the first \(s-1\) items and \(w_{s:}\) are the trailing \(|w|-s-1\) items in \(w\).

    Let \(\unlhd\) be a total order on \(A\), the \deflab<relation>[lexorder]{lexicographic order} on \(A^*\), denoted by \(\unlhd^*\), is defined such that
    \begin{compactenum}
        \item \(w \unlhd^* w \cdot v\) for each \(w, v \in A^*\), and
        \item \(w\cdot a\cdot u \rel^* w\cdot b\cdot v \) for each \(a,b \in A\) with \(a \rel b\) and each \(w,u,v \in A^*\).
    \end{compactenum}
    For example, \(\leq^*\) is the lexicographic order over sequences of numerical values where \(\varepsilon \leq^* 1\:1\:2\) and \(1\:1\:2\leq^*1\:2\).


    \section{Strings and Trees}\label{sec:preliminaries:trees}
    \paragraph{Alphabets and Strings.}
    An \deflab{alphabet} is a non-empty and finite set. Its elements are called \deflab<alphabet>{symbol}[symbols].
    Usually, uppercase Greek letters are used to denote alphabets, like \(\varSigma\), \(\varPi\), or \(\varGamma\).
    A \deflab<sequence>{string} is a finite sequence over a set of symbols \(\varSigma\).
    The set of all strings over \(\varSigma\) is \(\varSigma^*\).
    The notation for sequences is also used for strings.
    Subsequences in a string are called \emph{substrings}.

    \paragraph{Trees.}
    \deflab{tree}[Trees] are particular (finite) sequences that contain opening and closing parentheses to denote nesting hierarchies.
    For any alphabet \(\varSigma\) and set \(\varGamma\), the set of trees \(\T_\varSigma(\varGamma)\) is defined inductively: it is the smallest subset \(T\) of \((\varSigma \cup \varGamma \cup \{ (, )\})^*\) such that
    \begin{inparaenum}
        \item \(\varGamma \subseteq T\) and
        \item for each symbol \(\sigma \in \varSigma\), positive integer \(k \in \DN_+\) and trees \(t_1, \ldots, t_k \in T\), the sequence \(\sigma(t_1 \cdots t_k)\) is in \(T\).\footnote{
            We do not consider ranked trees.
            Each symbol \(\sigma \in \varSigma\) in a tree may occur with an arbitrary amount of successors denoted in parenthesis after it.
            Moreover, such a symbol may occur multiple times in a tree with varying amounts of successors.
        }
    \end{inparaenum}
    As such, trees are finite structures, but unlike strings, we do not consider empty trees.
    In case \(\varGamma \subseteq \varSigma\), the symbols at the leaves are not distinguished from inner nodes; then the set \(\T_\varSigma(\varGamma)\) is abbreviated by \(\T_\varSigma\).

    A \deflab<tree>{node} in the tree \(t \in \T_\varSigma(\varGamma)\) is an occurrence of a symbol in \(\varSigma \cup \varGamma\), i.e.\@ it is a combination of the symbol \(\sigma\) and a position it occurs.
    If a node is accompanied by a sequence of trees denoted in parentheses behind the symbol, these trees are called the \deflab<tree!node>{child}[children] of the node.
    Vice versa, from the viewpoint of a child node, the node before the wrapping parenthesis is called the \deflab<tree!node>{parent}.
    Each other node within the parenthesis is called a \deflab<tree!node>{sibling}.
    The leftmost node is called the \deflab<tree>{root} of the tree; each node without children is called a \deflab<tree>{leaf} of the tree; and each node with children is called an \deflab<tree>[inode]{inner node}.
    The \deflab<tree!node>{rank} of a node is its number of children, \(\rank(t)\) denotes the rank of the root in \(t\).
    Each leaf is of rank zero; nodes of rank one are called \deflab<tree!node>{unary}, and those of rank two are called \deflab<tree!node>{binary}.
    A tree is called \deflab<tree>[bintree]{binary} if each inner node has exactly two successors.

    Each illustration of a tree usually starts with the root node at the top. Below are its children connected by edges; this scheme recurs until the leaves are at the bottom.
    \cref{fig:pre:ctree} shows an example.

    \paragraph{Tree Positions.}
    A \deflab<tree>{position} in \(t\) is a sequence $w \in \DN^*$ that identifies a node in the tree.
    Starting with the root, each integer in \(w\) indicates the index of a child whose root is the reference for the remaining integers in \(w\).
    Alas, the set of positions, denoted by \(\pos(t)\), is defined recursively over the structure of trees:
    \begin{inparaenum}
        \item if \(t \in \varSigma\), then it is \(\pos(t) = \{\varepsilon\}\), and
        \item if \(t = \sigma(t_1 \cdots t_k)\) for some \(\sigma\in \varSigma\) and \(t_1, \ldots, t_k \in \T_\varSigma\), then it is \(\pos(t) = \{\varepsilon\} \cup \bigcup_{i\in[k]} \{i\}\cdot \pos(t_i)\).
    \end{inparaenum}
    The set of all inner node positions in \(t\) is denoted by \(\npos(t)\), and the set of leaf positions is \(\lpos(t)\).
    Let \(\rho \in \pos(t)\) be some position.
    Each subsequence in \(t\) that is also a tree in \(T_\varSigma\) is called a \deflab<tree>{subtree} in \(t\).
    The subtree at a position \(\rho\) is denoted by \(t|_\rho\).
    The \deflab<tree>{ancestor}[ancestors] of \(\rho\) are all real prefixes of the sequence \(\rho\), denoted by \(\ancestors(\rho)\).
    If \(\rho\) is of the form \(\rho = p_1 \ldots p_\ell\) for some \(\ell \in \DN_+\) and \(p_1, \ldots, p_\ell \in \DN_+\), then \(\ancestors(\rho) = \{ p_1 \ldots p_i \mid 0 \leq i \leq \ell-1 \}\).
    The \deflab<tree>{descendant}[descendants] of \(\rho\) in \(t\) are all positions in the subtree below \(\rho\), denoted by \(\descendants_t(\rho) = \{ \rho \cdot \rho' \in \pos(t) \mid \rho' \in \pos(t|_\rho) \}\).
    The symbol at a position \(\rho\) in \(t\) is denoted  \(t(\rho)\).
    We use the same terminology for position as for nodes, i.e.\@ child position, parent position, etc.; \(\children_t(\rho) = \{\rho\} \cdot \DN_+ \cap \pos(t)\) denotes the set of child positions below each position \(\rho\) in \(t\), and \(\parent(\rho) = \rho_{:|\rho|}\) the parent position for each \(\rho \in {\DN_+}^+\).
    Two positions \(\rho, \rho' \in \pos(t)\) are called \emph{independent}, if \(\rho\) is neither a descendant nor an ancestor of \(\rho'\).

    A \deflab<tree>{subforest} of \(t\) is a sequence of trees occurring consecutively (as siblings) within \(t\).
    A subforest may be located within \(t\) using a position \(\rho\) accompanied by a concluding child index that is greater or equal to the last child index in \(\rho\).
    Let \(i, e \in \DN_+\) and \(\rho' \in \pos(t)\) such that \(\rho = \rho' \cdot i\) and \(e \in [i,\rank(t|_{\rho'})+1]\).
    Analogously to the location of subsequences, the notation \(t|_{\rho:e}\) denotes the subforest in \(t\) at positions \(\rho' \cdot i, \rho' \cdot (i+1), \ldots, \rho \cdot (e-1)\).

    \begin{lemma}\label{lem:firstviasecond}
        Let \(\xi \in \T_{\varSigma}\) be binary tree over some alphabet \(\varSigma\).
        And let \((f_\rho \in \lpos(\xi) \mid \rho \in \npos(\xi))\) be such that \(f_\rho\) is the leftmost leaf position in the right subtree below \(\rho\).
        Then
        \begin{inparaenum}
            \item \(f_\rho\) is well defined for each \(\rho \in \npos(\xi)\) and
            \item for each pair \(\rho, \rho' \in \npos(\xi)\), \(\rho \neq \rho' \iff f_\rho \neq f_{\rho'}\).
        \end{inparaenum}
    \end{lemma}

%    \begin{proof}
%        The element \(f_\rho\) is obtained from \(\rho\) by appending a distinct postfix \(\omega \in {2}\cdot{1}^*\).
%        \begin{enumerate}
%            \item
%                As \(\rho\) is an inner node position in the binary (and finite) tree \(\xi\), there must be such a leaf position \(\rho \cdot w \in \lpos(\xi)\).
%            \item
%                The direction \(\rho \neq \rho' \Leftarrow f_\rho \neq f_{\rho'}\) follows from (i).
%                By the form of the postfixes \(\omega\) in \(f_\rho\) and \(\omega'\) in \(f_{\rho'}\) as above, they are unambiguously distinguishable from their prefix \(\rho\) and \(\rho'\).
%                Now, we have \(\rho \cdot \omega \neq \rho' \cdot \omega'\) and two cases:
%                \begin{compactitem}
%                    \item If \(\omega = \omega'\), then clearly follows \(\rho \cdot \omega \neq \rho' \cdot \omega' \Rightarrow \rho \neq \rho'\).
%                    \item If \(\omega \neq \omega'\), then, since \(f\) is well defined, also \(\rho\) and \(\rho'\) must be unequal. \qedhere
%                \end{compactitem}
%        \end{enumerate}
%    \end{proof}

    \paragraph{Indexed Trees.}
    An \deflab<tree>[itree]{indexed tree} over \(\varSigma\) is an element in \(\T_{\varSigma}(\DN_+)\), i.e.\@ inner nodes are symbols in \(\varSigma\), and leaves are positive integers, such that each leaf occurs at most once.
    For each inner node in an indexed tree, the children are ordered by the least value that occurs in a leaf.
    The notation for positions, indexing, and substitution are inherited from trees; the yield of an indexed tree is its set of leaves instead of a string.
    The set of all indexed trees over \(\varSigma\) is denoted by \(\itrees_\varSigma\).

    \paragraph{Substitution.}
    Let \(X\) be a finite set, \(\varSigma\) some alphabet that is disjoint from \(X\), and \(c\) a string in \((X \cup \varSigma)^*\).
    A (first-order) \deflab{substitution} of \(X\) in \(c\) defined by an \(X\)-indexed family \((v_x \in \varSigma^* \mid x \in X)\) is denoted by \(c[v]\) and yields the string in \(\varSigma^*\) obtained from \(c\) by replacing every occurrence of symbols \(x \in X\) by \(v_x\).
    To avoid explicitly defining a family for a substitution, we may also denote the substitution by enumerating the values in the form \(c[x_1=v_{x_1}, \ldots, x_{k}=v_{x_k}]\) where \(k\) is some natural number and \(X = \{x_1, \ldots, x_k\}\) with \(x_i \neq x_j\) for each pair \(i,j \in [k]\) if \(i\neq j\).
    If the string \(c\) does not contain some variables in \(\X\), we may omit them in this notation.

    The concept of \deflab<substitution>{second-order substitution} introduces a second layer by parameterizing the values before inserting them.
    For the scope of this thesis, the number of parameters for each value is limited to exactly one to keep the definition simple.
    Let's consider a single distinct variable \(y \notin X\), a set of shallow trees \(X(\varSigma^*) = \{ x(w) \mid x \in X, w \in \Sigma^* \}\) and a string \(c\) in \((X(\varSigma^*) \cup \varSigma)^*\).
    A second-order substitution of \(X\) in \(c\) defined by an \(X\)-indexed family \((v_x \in (\varSigma \cup \{y\})^* \mid x \in X)\) yields the string in \(\varSigma^*\) obtained from \(c\) by replacing each occurrence of shallow trees \(x(w) \in X(\varSigma^*)\) by \(v_x[y=w]\).
    We specifically stress that the variable \(y\) may be an element in \(\varSigma\), and the result \(c[v]\) might also contain it.

    Because trees are just specific strings, both definitions of substitution are easily lifted to trees and sequences of trees as long as the variables do not coincide with parentheses.
    Let \(\varGamma\) be an alphabet distinct from \(X\), and \(\{ (, ) \} \cap (X \cup \{y\}) = \emptyset\), and \(y \notin \varSigma\).
    Now, the variable occurrences are of the form \(X(\T_\varSigma(\varGamma)^*) = \{x(t_1, \ldots, t_k) \mid k \in \DN, t_1, \ldots, t_k \in \T_{\varSigma}(\varGamma)\}\) where each element has a root in \(X\), in the following the set is abbreviated by \(\hat{X}\).
    A second-order substitution of \(X\) in \(c \in \T_\varSigma(\varGamma \cup \hat{X})^*\) defined by an \(X\)-indexed family \((v_x \in \T_\varSigma(\varGamma \cup \{y\})^* \mid x \in X)\) yields a sequence of trees in \(\T_\varSigma(\varGamma)*\) obtained from \(c\) by replacing each occurrence of \(x(v) \in \hat{X}\) by \(v_x[y=v]\).
    The variable \(\y\) may be an element of \(\varGamma\).

    \begin{example}[Substitution]
        Consider an alphabet \(\Sigma = \{\text{a}, \ldots, \text{g}\}\) of (terminal) symbols and two sets of variables \(\X = \{\x_1, \x_2, \x_3\}\) and \(\Y = \{\y\}\).
        In first-order substitutions, all occurring variables in a string are replaced by argument strings, e.g.
        \begin{align*}
            \x_1 \, \text{b} \, \x_2 \, \text{f} \, \x_3\,[\x_1=\varepsilon, \x_2=\text{cde}, \x_3=\text{g}] &= \text{bcdefg} \\
            \x_1 \, \text{bb} \, \x_1 \, [\x_1=\text{a}] &= \text{abba}
        \end{align*}
        In second-order substitutions, there is an argument with each variable \(x\) in \(\X\) that may be inserted for the variable \(\y\) in the value for \(x\), e.g.
        \begin{align*}
            \text{a} \, \x_1(\text{cc}) \, \text{a} \, [\x_1=\text{b}\,\y\,\text{b}] = \text{a} \, (\text{b}\,\y\,\text{b})[\y=\text{cc}] \, \text{a} &= \text{abccba} \\
            \x_1(\text{bb}) \, \x_1(\varepsilon) \, [\x_1=\text{a}\,\y\,\text{a}] = (\text{a}\,\y\,\text{a})[\y=\text{bb}] (\text{a}\,\y\,\text{a})[\y=\varepsilon] &= \text{abbaaa}
        \end{align*}

        The following case considers trees over \(\Sigma=\{\cn{vp}\}\) and \(\Gamma=\{\tn{was}, \tn{carried}, \y\}\):
        \begin{align*}
            \cn{vp}( \x_1(\y) \, \tn{was} )[\x_1=\cn{vp}(\tn{carried}\,\y)]
                &= \cn{vp}( \cn{vp}(\tn{carried}\,\y)[\y=\y] \, \tn{was} ) \\
                &= \cn{vp}(\cn{vp}(\tn{carried}\,\y)\,\tn{was})
        \end{align*}
    \end{example}

    \section{Modeling Natural Languages using Formal Methods}\label{sec:preliminaries:ctrees}
    In the syntactic analysis of sentences in natural languages, we deal with \deflab[ctree]{constituent tree}[constituent trees].
    They illustrate a hierarchy of sub-phrases in the sentence, where a category labels each element in the hierarchy, called \deflab<constituent tree>[csymbol]{constituent symbol}.
    These sub-phrases do not necessarily form contiguous regions in the sentence:
    If such a tree contains a node with a non-contiguous sub-phrase, we call it \deflab<constituent tree>{discontinuous}; otherwise, it is \deflab<constituent tree>{continuous}.
    For instance, \cref{fig:pre:ctree} illustrates a discontinuous constituent tree.
    The symbols at inner nodes, like \(\cn{s}\) and \(\cn{vp}\), are constituent symbols.
    The lower node labeled by \(\cn{vp}\) governs a noncontiguous region in the sentence: \emph{where} and \emph{carried out}.

    \begin{figure}
        \null\hfill
        \subfile{figures/example-constituents-with-annotated-components.tex}
        \hfill\null

        \caption{\label{fig:pre:ctree}
            Discontinuous constituent tree for the phrase \emph{where the survey was carried out}.
            The tree is illustrated in two slightly different ways:
                The left one focuses on the discontinuous sentence structure by recreating the token order with crossing branches in the constituent structure.
                The right one corresponds to the definition of constituent trees in this section and separates the sequence of tokens in the sentence and the \abrv{pos} symbols from the constituent structure using sentence positions as leaves in the constituent structure.
            In both illustrations, the path to its lexical head is double-struck for each node in the constituent structure.
        }
    \end{figure}

    The first layer of nodes in a constituent tree above phrase positions contains distinct symbols called \emph{part of speech} (\abrv{pos}) tags.
    They are mutually exclusive to the constituent symbols found in upper layers, i.e., each symbol occurring at the first layer must be a \abrv{pos} tag and no other constituent symbol, and each symbol above the first layer must not be a \abrv{pos} symbol.
    They indicate the most primitive categorization of individual tokens in the sentence.
    In our example tree, the symbols \(\cn{wrb}\) and \(\cn{pt}\) are \abrv{pos} tags.

    Now, let us formalize the above.
    We assume a set \(\Sigma\) of words, punctuation marks, parentheses, etc.\@ that are used to form sentences; we call the elements in \(\Sigma\) \deflab{token}[tokens].
    Additionally, there are two alphabets: The set \(\varPi\) of \abrv{pos} tags and the set \(\varGamma\) of constituent symbols.
    \(\Sigma^*\) denotes the set of all strings over these tokens, a superset that contains all grammatical sentences (and also everything else that consists of the tokens in \(\Sigma\)).

    A \emph{constituent tree} is a triple \((t, p, w)\) where
    \begin{compactitem}
        \item \(w\) is a string of tokens in \(\varSigma^*\),
        \item \(t\) is an indexed tree over \(\varGamma\) such that each position in \([|w|]\) occurs exactly once as a leaf, and
        \item \(p\) is a string of \abrv{pos} tags in \(\varPi^{|w|}\).
    \end{compactitem}
    In each constituent tree, we distinguish three parts: the sequence of tokens, the \abrv{pos} tag above each token, and the \deflab<constituent tree>[cstruct]{constituent structure} that contains the constituent symbols and defines the hierarchical structure.
    To accommodate discontinuous constituents, each constituent structure is defined as an indexed tree over \(\varGamma\) so that each leaf is the position of a token.

    A \emph{head assignment for $(t,p,w)$} is a function \(h\colon \pos(t) \to \DN_+\) assigning each inner node position in the constituent structure \(t\) to one of the leaves at or below it.
    For each \(\rho \in \lpos\), the value \(h(\rho)\) is \(\xi(\rho)\); for each \(\rho \in \npos(t)\), $h$ must assign the value \(h(\rho)\) to a child inner node as well, i.e. \[
        h(\rho) \in \{ h(\rho') \mid \rho' \in \children_t(\rho) \} \text{.}
    \]
    The \emph{lexical head assignment for $(t,p,w)$} is such a head assignment.
    For each (sub-)phrase within \(w\) that is governed by the node at position \(\rho \in \npos(t)\), it determines the word that is critical for the constituent symbol \(t(\rho)\).
    We consider the lexical head assignment a complementary and fixed object for each constituent tree; the identifier \(\head_{(t,p,w)}\) denotes the lexical head assignment for \((t,p,w)\).
    The leaf \(\head_{(t,p,w)}(\rho)\) is called the \emph{lexical head} of the node at \(\rho\).
    When the lexical head assignment is considered in an illustration of a constituent tree, there is exactly one double-struck path exiting each inner node of the constituent structure.
    The assigned lexical head is found by following the double-struck edges down to a leaf.
    For instance, \cref{fig:pre:ctree} shows such an illustration and the lexical head of its root is \(4\) (representing the fourth word ``was'' in $w$).
    Let us assume a binary constituent structure \(t\) and an inner node position \(\rho \in \npos(t)\).
    If the leaf \(\head_{(t,p,w)}(\rho)\) is located at or below position \(\rho \cdot 1\), then the subtree at \(\rho \cdot 2\) is called the \deflab[dependent]{dependent of $\rho$}; vice versa, the subtree at \(\rho \cdot 1\) is called dependent of $\rho$ if \(\head_{(t,p,w)}(\rho)\) is at or below \(\rho \cdot 2\).

    A \emph{treebank} is a finite set of constituent trees.
    They are assembled by linguists for texts collected from news articles, written speeches, etc., and annotated with their constituent structure.
    In the scope of this thesis, treebanks form data sets used for learning models that can predict constituent structures for input sentences.
    The process of predicting a constituent structure for a given input sentence is called \emph{parsing}, and a used model is called \emph{parser}.
    In \emph{\(k\) best parsing}, a sequence of (at most) \(k \in \DN_+\) constituent trees is predicted and each is complemented with a confidence value.
    \emph{Reranking} is a process that uses such a collection of potential constituent trees for a sentence and produces new confidence values.
    It is used as a refinement for a set of candidate constituent trees and often involves more ambitious or orthogonal methods to the underlying \(k\) best parser.

    Besides constituent trees, there are also \emph{dependency trees} that express a syntactic analysis of natural language.
    They present grammatical relations between phrases within a sentence, such as arguments, adjuncts, complements, and sometimes even more fine-grained ones, like distinctions between subjects and objects.
    These relations are illustrated in a tree where each phrase is represented by its lexical head.
    There are similar phenomena as (dis-)continuity in dependency trees. They are referred to as \emph{(un-)projectivity}.
    We will not engage in dependency trees in this work. Some references are discussed in \cref{sec:literature:beyond}.

    \subsection{Assessing the Prediction of Constituent Structures}
    When assessing the quality of a parser, we measure two dimensions:
    \begin{inparaenum}
        \item the time needed for the prediction process, and
        \item the accuracy of predicted structures.
    \end{inparaenum}
    For both, there is a treebank that is reserved for the evaluation (called \emph{test set}).
    The process for measuring the first dimension is rather intuitive:
        The time is taken for the prediction of a constituent structure for each sentence in the test set.
    This time is reported either as the sum for the whole test set or as quotient \(\frac{\text{number of sentences}}{\text{parsing time}}\) for the number of parsed sentences per second.

    The \emph{evalb}-style labeled \emph{f1-score} \citep{Black91,Col97} was established as the standard for measuring the accuracy of predicted constituent structures.
    This score is computed as follows:
    The sets of predicted and the gold constituent structures are mapped each into a set that contains a tuple \((\xi, A, \mathit{yd})\) for each constituent structure \(\xi\) and each of its inner nodes with label \(A\) and yield \(\mathit{yd}\).
    According these two sets, \(P\) for all predictions and \(G\) for the gold constituent trees, the precision (\(p = \frac{|P \cap G|}{|P|}\)), recall (\(r = \frac{|P \cap G|}{|G|}\)) and f1-score (\(\frac{2\cdot p\cdot r}{p + r}\)) are computed in the usual fashion.
    The \abrv{pos} tags are often part of the prediction but not assessed as part of the f1-score.
    They are rather reported separately as the accuracy of the predicted \abrv{pos} tags compared to the gold ones.
    The Python tool \emph{discodop} supplies an implementation for both the f1-score of constituent structures and the accuracy of \abrv{pos} tags \citep{CraSchBod16}.

    \begin{figure}
        \null\hfill
        \begin{tikzpicture}[baseline=(t1mid.base)]
            \begin{scope}[every node/.style={inner sep=2pt, font=\small}, level distance=3ex, level 3/.style={sibling distance=2em}]
                \node {\cn{sbar}}
                child { node (t1) {\cn{s}}
                    child { node {\cn{vp}}
                        child { node (t1mid) {\cn{vp}}
                            child { node {\cn{wh}} child {
                                    node {1}}}
                            child { node {5}}
                            child { node {\cn{prt}} child {
                                    node {6}}}}
                        child { node {4}}}
                    child { node {\cn{np}}
                        child { node {2}}
                        child { node {3}}}};
            \end{scope}
            \begin{scope}[every node/.style={inner sep=2pt, font=\small}, level distance=3ex]
                \node[right=4.5cm of t1] (t2) {\cn{s}}
                [sibling distance=5em]
                child { node {vp}
                    [sibling distance=1em]
                    child { node (t2mid) {1}}
                    child { node {4}}
                    child { node {5}}
                    child[sibling distance=1.5em] { node {\cn{prt}} child {
                            node {6}}}}
                child { node {\cn{np}}
                    [sibling distance=1em]
                    child { node {2} }
                    child[sibling distance=1.5em] { node {\cn{nn}} child { node {3}}}};
            \end{scope}
            \node[left=2em of t1] {\(\xi_\text{G}\):};
            \node[left=2em of t2] {\(\xi_\text{P}\):};
        \end{tikzpicture}
        \hfill
        \begin{minipage}{.3\linewidth}
            \small
            \begin{align*}
                \mathit{precision} &= \tfrac{4}{5} \\
                \mathit{recall} &= \tfrac{4}{7} &
                \mathit{f1} &= \tfrac{2}{3} \\
            \end{align*}
        \end{minipage}
        \hfill\null

        \begin{minipage}{.35\linewidth}
            \small
            \begin{align*}
                G = \{
                &(\cn{sbar}, [6]),
                (\cn{s}, [6]), \\
                &(\cn{vp}, \{1,4,5,6\}),\\
                &(\cn{vp}, \{1,5,6\}),
                (\cn{np}, \{2,3\}), \\
                &(\cn{wh}, \{1\}),
                (\cn{prt}, \{6\}) \}
            \end{align*}
        \end{minipage}
        \hfill
        \begin{minipage}{.3\linewidth}
            \small
            \begin{align*}
                P = \{
                &(\cn{s}, [6]), \\
                &(\cn{vp}, \{1,4,5,6\}),\\
                &(\cn{np}, \{2,3\}),
                (\cn{nn}, \{3\}) \\
                &(\cn{prt}, \{6\}) \}
            \end{align*}
        \end{minipage}
        \hfill
        \begin{minipage}{.3\linewidth}
            \small
            \begin{align*}
                P \cap G = \{
                &(\cn{s}, [6]), \\
                &(\cn{vp}, \{1,4,5,6\}),\\
                &(\cn{np}, \{2,3\}) \\
                &(\cn{prt}, \{6\}) \}
            \end{align*}
        \end{minipage}

        \caption{
            Example for the f1-score computation for a single pair of gold (\(\xi_\text{G}\)) and predicted (\(\xi_\text{P}\)) constituent structures.
            The sets of constituent tuples for both trees and the set of common constituents are illustrated below.
            The upper right corner shows the precision, recall, and f1-score values.
        }
    \end{figure}

    \section{Empirical Predictions}
    Under the broad term \deflab{prediction}[(empirical) prediction], we subsume each process that takes an input of predetermined shape and assigns it to an output (called \emph{label}) based on previously seen data and prior knowledge.
    In the following, we clarify some basic terms used throughout the thesis in conjunction with empirical predictions.
    For more detailed elaborations in this field, refer to \Citet{Moh12,Dau17}.

    \paragraph{Tasks, Models, and Learning.}
    We start with the broadest concepts in the field of machine learning.
    A \emph{task} defines a general setting of the prediction, including the sets of possible inputs and labels and an assessment function for the predictions.
    Since the domains of inputs and labels strongly depend on available data, it is also a part of the task.

    A \deflab<empirical prediction>{model}[(prediction) model] is a specification that defines how each input is mapped to a label.
    Each model chooses a label that optimizes a \deflab<empirical prediction>{confidence} value (e.g., a score or probability), so we view each model as a function that computes this value for each input and label.
    The model specifies variables (called \deflab<prediction!model>{parameters}) steering the computation of confidence values.
    Let us denote the set of inputs by \(X\), the labels by \(Y\), and the domain of confidence values by \(\mathcal{C}\) with a total order \(\rel \subseteq \mathcal{C} \times \mathcal{C}\).
    We view each model as a parametrized function \(\varphi_\theta\colon X \times Y \to \mathcal{C}\) where \(\theta\) are some parameters (or variables) in an associated domain \(\Theta\).\todo{prelim: parametrized function}
    In our case, the domain of confidence values are real numbers, and the order \(\rel\) is \(\leq\), such that the confidence is interpreted as a score (i.e.\@ greater is better).
    The prediction for an input \(x\) and a model \(f\) with parameters \(\theta\) is any value in \(\argmax^{\rel}_{y \in Y} \phi_\theta(x, y)\).

    Generally, there are two types of models: \deflab<prediction!model>{generative models} and \deflab<prediction!model>{discriminative models}.
    While generative models define a joint probability distribution of inputs and labels, discriminative models define probabilities or scores for labels conditioned on the input.
    Generative models can generate samples of pairs containing in- and outputs.
    Discriminative models, on the other hand, cannot do this in general, as they do not model the input structure.

    \emph{Learning} is a process that finds an appropriate parametrization for a model in a task.
    We differentiate between two major types of learning: \emph{supervised} and \emph{unsupervised}.
    In the first case, the data contains the inputs and labels; in the latter, it consists only of input.
    Some intermediate types exist between the two, including self-supervised and semi-supervised, where a large amount of unlabeled data is complemented with a small amount of labeled data.

    \paragraph{Parameters and Training.}
    We distinguish two essential sets of parameters: \deflab<prediction!model!parameters>{hyperparameters} and \deflab<prediction!model!parameters>{model parameters}.
    Hyperparameters determine details about the model and the training process. %, to some extent, the model and the training objective can also be seen as hyperparameters.
    They are not trained but are chosen for the training either by prior knowledge or by trying different combinations and manually evaluating their results.
    Model parameters are variables defined by a model. They are tuned during the training process to match a training objective.

    \deflab<prediction>{training}[Training] is the process of tuning the model parameters in terms of a \deflab<prediction!training>{training objective}.
    A training objective assesses the assignment of confidence values by a model in a sample of input and output pairs and defines a criterion for optimal parameters.
    In some cases, it is possible to compute the model parameters that optimize the closed-form training objective.
    In other cases, iterative processes change the parameters to improve upon the criterion gradually.
    Some delicate nuances between the terms learning and training are sometimes used interchangeably.
    While both describe a process that fits a model to available data, training is specifically about tuning model parameters using labeled examples.
    Learning is a little broader, including each process that finds appropriate hyperparameters.


    \paragraph{Data and splits.}
    The \deflab<prediction>{data sets} contain pairs of input and output that we use to deduce a prediction model.
    In this context, the output (i.e., the to-be predicted) component of these instances is referred to as \deflab<prediction!data sets>{gold}.
    The data instances are used in two roles: for training and for assessing a prediction model.
    The data sets are usually split into three parts to accommodate these roles:
    \begin{inparaenum}
        \item a \deflab<prediction!data sets>{training portion} that is solely used to train a model,
        \item a \deflab<prediction!data sets>{development portion} that is used to monitor the model during the training process and assess the selection of hyperparameters and
        \item a \deflab<prediction!data sets>{test portion} that is used to assess a final model.
    \end{inparaenum}
    Specific partitions are shared for standard data sets used within a community to enable fair comparisons between publications.
    In such publications, the results of different models are compared using the same test portions of the same data sets.

    There are two distinct examples of empirical predictions in (or at least near) the scope of this thesis.

    \begin{example}[Statistical parsing in the empirical prediction framework.]
        One task that we consider is discontinuous constituent parsing with the \negra{} treebank.
        The domain of inputs is the set of sequences over tokens (that preferably coincide with German words), and the domain of labels is the set of constituent trees where the constituent and \abrv{pos} symbols are defined by the \negra{} treebank.
        Each predicted label can be assessed to a correct tree via the f$_1$-score.
        Learning this task is supervised because the treebank contains the sentences and their constituent trees.

        Parsing is a process that predicts a constituent structure for a given sequence of tokens.
        Statistical grammars have proven successful in this setting in the past.
        It defines a probability distribution for constituent trees \(p\colon \mathcal{C} \to \DR\), usually by the sum over all derivations per constituent tree and by the product of all rules' weights per derivation.
        This is formalized in the equation \[
        p(\xi) = \sum_{d \in D(\xi)} \prod_{\rho \in \pos(d)} \hat{p}(d(\rho))
        \]
        where \(\xi \in \mathcal{C}\) is a constituent tree, \(D(\xi)\) is the set of all derivations in the grammar for the constituent tree \(\xi\), and \(\hat{p}\) is a weight function for the rules in the grammar.
        The function \(\hat{p}\) is defined such that the term \(\prod_{\rho \in \pos(d)} \hat{p}(d(\rho))\) is a probability distribution over all derivation trees \(d \in D\) in the grammar.
        As the sequence of tokens \(w\) is part of the constituent tree \(\xi\), this is trivially equivalent to a joint probability distribution \(p(\xi) = p(w, \xi)\), which is characteristic for a generative model.
        The values assigned by the function \(\hat{p}\) form the model parameters. They are restricted to probability distributions.
        Probabilistic grammars can generate constituent trees (trivially in tandem with a sequence of tokens) by sampling rules probabilistically.
    \end{example}

    \begin{example}[Supertagging in the empirical prediction framework.]
        In the case of parsing with supertagging, an integral part of the parsing process is an assembly of a small grammar by choosing some rules out of a predetermined set of rule blueprints (this specific process is called supertagging).
        The process may also be described using a generative model.
        But in practice, we use specific neural networks for this task, as they have proven very accurate in the past; they are textbook examples of discriminative models.
        They define many hyperparameters that determine the \emph{architecture} (i.e., the shape of the neural network), e.g.\@, the used word embeddings, its depth or the number of parameters in a particular layer, and many details about the training process, e.g.\@ the learning rate or some condition that ends the iterative training process.
        During the training process, a treebank is transformed into tuples of the token sequence and the optimal choice of blueprints for each constituent tree.
        The training objective maximizes the likelihood of the blueprints conditioned on the input tokens.
    \end{example}

    \subsection{Neural Networks}\label{sec:preliminaries:nn}
    As mentioned in the last example, we use neural networks to implement the prediction of a sample of supertags for each position input sentence.
    They are a family of parametrized functions that emerged in the last decades and proved extremely useful in many tasks, including ones settled in natural language processing, e.g.\@ in the field of semantic analysis like sentiment analysis \citep{sentiment} or relation extraction \citep{relationextraction}, in the field of syntax analysis like dependency \citep{dependencyparsing} or constituency parsing \citep{FerGom20a}, or in high-level tasks like question answering \citep{Devlin2019}, summarization \citep{bertgeneration}, or dialogue systems \citep{dialoguesystems}.
    As in the parent section, we will only briefly overview the concepts needed to understand the implementations discussed in this thesis.
    For further elaborations in neural networks for natural language processing, consult \citet{Gol22}.

    We consider a neural network (short \abrv{ann} for \emph{artificial} neural network) a parametrized function where the outputs are vectors of real numbers, i.e.\@ they are of the form \(f_\theta\colon X \to \DR^n\) where \(\theta\) denotes the parameters, \(X\) the set of inputs and \(n > 0\) is the output dimension.
    There is a set of well-studied \abrv{ann}s that are used as \emph{modules} and combined to form new \abrv{ann}s by composition, addition, or concatenating their results.
    Such a combined collection of modules is called an \emph{architecture}.
    In the following, some of these modules are briefly explained.

    \subsubsection{Embeddings}
    Embeddings are used to assign vectors to any other type of object.
    These assigned values are the foundation for computations in other modules, which usually assume real vectors as input.
    The simplest embedding form is \deflab<>{static embeddings}.
    Let us consider a set of input objects \(X\), a bijection \(\mathit{idx}\colon X \to [|X|]\) and an output dimension \(n \in \DN_+\).
    A static embedding for \(X\) of dimension \(n\) is a parametrized function \(f_W\colon X \to \DR^n\) which assumes a matrix of parameters \(W \in \DR^{(|X| \times n)}\) and assigns a row vector to each element in \(X\), i.e.\@ \(f_W(x) = W_{\mathit{idx}(x)}\) for each \(x \in X\).
    In some applications, adding a vector for unknown elements, e.g.\@, for unknown tokens that do not occur in the data used for training, is sensible.
    If the input is a sequence, static embeddings are applied element-wise.
    E.g.\@ a sequence of tokens \(\sigma_1 \cdots \sigma_n\) is embedded as a sequence of vectors \((f_W(\sigma_i) \mid i \in [n])\).

    Some architectures complement element-wise token embeddings by \emph{character-wise embeddings} \citep[e.g.\@][]{kiperwasser2016simple,Akb18}.
    These are recurrent modules (cf.\@ \cref{sec:preliminaries:nn:recurrent}) that process the sequence of characters and yield a vector of specified length for each token (which, in turn, requires a static embedding for the set of characters).
    This type of module is associated with the handling of unknown words with the following intuition:
        If a token does not occur in the training data, it has no specific element-wise embedding.
        However, the character-wise embedding might have learned some sensible information based on similar tokens in the training data.

    For sequence processing, \emph{positional embeddings} is another extension used to complement static embeddings.
    They define a function that maps the position of each sequence element into the output dimension.
    In contrast to both methods above, these are often \emph{not} trained but chosen immutably.
    E.g.\@ in the case of \abrv{bert}, a series of sine and cosine functions in different frequencies is used as positional embedding.

%    Recently, specialized methods for sequence embedding that also consider positional and contextual information in the assignment were published, such as \abrv{bert} \citep{Devlin2019} and flair embeddings \citep{Akb19}.
%    However, they are (in some cases rather large) compositions of \abrv{ann} modules for sequence tagging themselves, using a transformer and \abrv{lstm} units, respectively.

    \subsubsection{Linear Layers, Activation Functions, and Multi-Layer Perceptrons}
    After embeddings, linear layers are essential in \abrv{ann}s.
    They are a linear transformation of vectors from a specified input to an output dimension.
    Formally, let \(m, n \in \DN_+\) by some input and output dimensions.
    Then a linear layer is a function \(f_{(W,b)}\colon R^m \to R^n\) where the matrix \(W \in R^{(m\times n)}\) and the bias vector \(b\in R^n\) are parameters and \(f_{(W,b)}(x) = Wx+b\).

    Activation functions are a general term for non-linear functions used between compositions and on top of linear layers.
    Some options implement desirable properties or interpretations in some settings, such as the following:
    \begin{enumerate}
        \item
            A \emph{Rectified Linear Unit (ReLU)} is easy to compute and used in large architectures to save computational power.
            It is defined for each vector \(x\in R^n\) for any dimension \(n \in \DN_+\) by cutting-off negative values: \(\mathrm{relu}(x) = (\max(0, x_i) \mid i \in [n])\).
        \item
            The \emph{sigmoid function} is applied point-wise to each component of a vector and assigns values in the interval between \(0\) and \(1\) as follows: \(\mathrm{sigmoid}(x) = (\frac{1}{1-e^{x_i}} \mid i \in [n])\) for each \(x \in \DR^n\) and \(n \in \DN_+\).
            These values are often interpreted as one value of a binary probability distribution.
        \item
            The \emph{softmax function} also maps each value in a vector to the interval between \(0\) and \(1\).
            They are normalized such that they sum to \(1\) and are often interpreted as a probability distribution over the set of vector indices.
            It is defined as follows: \(\mathrm{softmax}(x) = (\frac{e^{x_i}}{z} \mid i \in [n])\) for each \(x\in \DR^n\) and \(n \in \DN_+\) where \(z = \sum_{i \in [n]} e^{x_i}\).
    \end{enumerate}
    The latter two are more dominantly used as the top-most modules because of their interpretation as probabilities.
    A \emph{multi-layer perceptron} is a composition of multiple linear layers with activation functions.


    \subsubsection{Neural Networks for Sequence Processing}\label{sec:preliminaries:nn:recurrent}
    Recurrent modules are a foundation for processing sequence inputs in neural networks.
    They are defined such that each input vector is complemented by the previous output, starting with an initialization vector for the first input.
    Formally, this concept of recurrence uses some inherent module \(f_\theta\colon \DR^{n+m} \to \DR^m\) parametrized by some \(\theta\) and incrementally computes, for an input sequence \(x_1, \ldots, x_n \in \DR^n\), a sequence of outputs \(g_{(f, \theta, h_0)}(x_1, \ldots, x_n) = (h_i = f_\theta(x_i \cdot h_{i-1}) \in \DR^m \mid i \in [n])\) where \(h_0 \in \DR^m\) is an initial value and parameter of the recurrent module \(g\).
    Linear short-term memory (short \abrv{lstm}) cells \citep{Hoc97} were established as a quasi-standard for these inherent modules.
    In addition to the previous output \(h_{i-1}\) that is used to compute \(h_i\), they also carry a cell state between the computations for each position, which is not part of the output.
    Bidirectional recurrent modules combine the vectors obtained by two recurrent modules, one for the sequence from left to right and the other in the opposite direction, by concatenating the vectors for each position.

    The transformer architecture \citep{vaswani2017attention} was established as an alternative to recurrent neural networks for sequence processing.
    It relies on the self-attention mechanism to extract and combine sequence positions based on their embedding vectors, often composed of static sub-word and positional embeddings.
    The architecture is especially well-known as the foundation of large language models.

    \subsubsection{Training}
    The parameters in an \abrv{ann} are tuned to fit an objective for labeled examples in a training set using \emph{statistical gradient descent} (\abrv{sgd} for short).
%    Such an objective is a function that rates the prediction for each example compared to the ground truth and accumulates the results in the whole set.
%    A prominent example of such an objective is \emph{cross entropy loss}, often used when the label's confidence values are interpreted as a probability distribution.
%    It is the sum of negative logarithms over all confidence values; the term \emph{loss} indicates that the value should be minimized during training.
%    During the training process, the value is computed for a small portion of the data (called \emph{batch}), and the parameters are updated using a partial derivative of the value concerning the weight.
%    These updates are scaled by a (typically small) real value, called \emph{learning rate}, and repeated a fixed number of times or until a particular criterion is met.
%    There are several extensions to the baseline training process that aim to accelerate or stabilize the training process or improve the prediction quality; we use the following:
    Such an objective often takes the form of a \emph{loss} value that should be minimized during the training process; in our case, it is \emph{cross entropy loss} (i.e.\@ the negative log-likelihood of the confidence value for the ground truth label in all examples).
    There are several extensions and hyperparameters to the training; among them, we use the following:
    \begin{itemize}
        \item
            \abrv{Sgd} is an iterative process; each iteration is usually performed using a small amount of data, called \emph{batch}.
            The amount of iterations to process the whole training set is called an \emph{epoch}. Usually, the total number of iterations for training is given in epochs.
            A small real value scales the computed gradient, called \emph{learning rate}.
        \item
            The gradients used to update the parameters in each iteration are modified using the \emph{Adam} algorithm \citep{adam,adamw} that implements momentum and adaptive gradients.
            Both aim to accelerate and stabilize the training process.
        \item
            Some measures that aim to improve the ability to generalize from training examples among them are \emph{weight decay} and \emph{dropout} \citep{dropout}.
            Both are quantified in terms of real values:
                A term for the magnitude of all parameters is added to the loss and scaled by weight decay. This discourages high values for parameters associated with being tied explicitly to training instances (called overfitting).
                Dropout is the probability of omitting the value (and replacing it by \(0\)) at an index in the vector computed after selected modules.
    \end{itemize}

    \subsubsection{Pretrained Modules}
    As mentioned, some modules are shared in the community, such as pre-trained static embeddings.
    Usually, they are \emph{generic}, i.e.\@, designed to be used in many different tasks; in that context, such a specific task is called \emph{downstream}.
    In natural language processing, using pre-trained modules has become quasi-standard.
    The reason for that is twofold:
    \begin{compactitem}
        \item
            Training costs time and power; using pre-trained modules skips at least a part of the training and saves both.
        \item
            These generic modules are often trained using vast corpora, magnitudes larger than most for downstream tasks such as constituent parsing.
            Thus, they contain information that is impossible to obtain from a downstream training set.
    \end{compactitem}
    When \emph{large language models} are used as modules in an architecture, the term \emph{fine-tuning} is used for training downstream tasks.
    Because of the additional information, fine-tuning such a model for a downstream task in an otherwise supervised manner is referred to as \emph{semi-supervised} training.

    The two most prominent instances of pre-trained models that we encountered during the work that led up to this thesis are the following:
    \begin{compactitem}
        \item
            Word vector embeddings are pre-trained static word embeddings.
            Syntactic word representations, such as \emph{word2vec} \citep{word2vec} and \emph{fasttext} \citep{fasttext}, are both based on the continuous bag of words model and aim to map words that are used in similar contexts to similar vectors.
            Semantic frameworks, like WordNet \citep{wordnet}, aim to map words with similar meanings to similar vectors using training with large semantic word networks.
        \item
            As the name suggests, large language models (short \abrv{llm}) are not only single modules but large architectures for sequence processing.
            They are usually pre-trained using various tasks that involve missing word predictions and are associated with the field of language understanding.
            The vector assigned to each token in a sequence by an \abrv{llm} is context-sensitive:
                It does not only depend on the token itself but on its position and all other tokens in the sequence as well.
                This is achieved using positional embeddings and transformer architectures.
            The breakthrough for \abrv{llm} as a component in \abrv{ann} was the release of \abrv{bert} \citep{Devlin2019}.
            Since then, \abrv{bert} and its derivatives were adopted into models to solve various tasks on language understanding \citep[they experiment with models for question answering, entailment recognition, sentiment analysis, i.a.]{Devlin2019, roberta}, language generation \citep[they experiment with models for summarization, translation, i.a.]{bertgeneration}, information extraction \citep[they experiment with models for named entity recognition, relation extraction]{biobert} and also our field of research: constituent parsing \citep{Cor20, FerGom20a, FerGom22,Coa21, Sun22}.
            It is still used in state-of-the-art models today.
    \end{compactitem}

    \subsubsection{Architectures}\label{sec:models}
    We implement the prediction of supertags using two architectures:
    \begin{enumerate}
        \item
            The first one is pretty common in the \abrv{nlp} community for token-wise classification tasks and was used, e.g., by \citet{vaswani2016supertagging,StaSte20,Cor20}.
            It contains two modules for embedding, a static word embedding and character embeddings, which are followed by some layers of bidirectional recurrent modules utilizing \abrv{lstm}, and finally an \abrv{mlp}\todo{or linear?}; \cref{fig:architecture:supervised} shows an illustration.
            It does not rely on pre-trained modules and implements supervised learning to predict supertags.
        \item
            The second architecture consists of a pre-trained \abrv{llm} that is topped by one feed-forward layer (this design is pretty standard and recommended for classification predictions using \abrv{llm} embeddings, e.g., by \citet{Devlin2019}).
            If we consider the \abrv{llm} as a black box, it is conceptually extremely simple yet effective, but its strength relies upon the complex design and pretraining of the \abrv{llm}.
            As noted before, using such a pre-trained model is considered semi-supervised learning for predicting supertags.
    \end{enumerate}
    In both cases, the vectors computed by the final layers are interpreted as probability distributions using the softmax function.
    The objective of the training is to minimize cross-entropy loss concerning the gold supertag blueprints.

    \begin{figure}
        \centering
        \subfile{figures/supervised-architecture.tex}
        \caption{\label{fig:architecture:supervised}
            Illustration of the supervised \abrv{ann} architecture for the supertag prediction.
            Each box with circles in different shades of gray represents a vector; boxes directly put on top of each other are concatenated vectors; arrows indicate the dependencies for the computed vectors.
            The input sentence (bottom) is embedded using static word embeddings and character-based embeddings; the latter are illustrated to the right and consist of static character embeddings fed into a bidirectional \abrv{lstm}.
            The embeddings are topped by bidirectional \abrv{lstm} layers (in this illustration 2) and a linear layer such that the input size is the number of supertag blueprints.
        }
    \end{figure}


    \ifSubfilesClassLoaded{%
        \printindex
        \bibliography{../references}%
    }{}
\end{document}
