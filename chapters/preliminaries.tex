\documentclass[../document.tex]{subfiles}

\begin{document}
    \chapter{Preliminaries}\label{chap:preliminaries}
    This chapter deals with the mathematical foundations and notation used throughout all chapters.
    \Cref{sec:preliminaries:math} starts with the most basic notions.
    \Cref{sec:preliminaries:trees} deals with all concepts involved in string and tree structures.
    And finally, \cref{sec:preliminaries:ctrees} sets these terms into the context of syntactic parsing of natural languages.


    \section{Mathematical Notation}\label{sec:preliminaries:math}
    Sets are considered with the usual notation \(\subseteq\), \(\subset\), \(\cup\), \(\cap\) and \(\setminus\) for the inclusive and exclusive subset relation, as well as the operations for union, intersection and difference.
    \(\emptyset\) is the empty set and \(\power(A)\) is the power set, i.e.\@ the set containing all subsets, of some set \(A\).
    The sets of integers, naturals (including zero), positives and rationals are denoted by $\DZ$, $\DN$, $\DN_+$ and $\DR$, respectively.
    The \deflab{range} of integers between the pair \(n, m \in \DZ\) with \(n \leq m\) is denoted by \([n,m] = \{i \in \DZ \mid n \leq i \leq m\}\).
    For any positive integer \(n\), the notion \([n]\) abbreviates \([1,n]\), and \([0]\) is the empty set \(\emptyset\).

    \paragraph{Relations.}
    Let \(A\) and \(B\) be some sets.
    The cross product \(A \times B\) is the set of all tuples \(\{(a,b) \mid a \in A, b\in B\}\).
    A \deflab{relation} from \(A\) to \(B\) is a set \(R \subseteq A \times B\).
    In case \(A\) and \(B\) coincide, we call \(R\) an \deflab<relation>{endorelation} on \(A\).
    For two elements \(a \in A\) and \(b \in B\), the term \(a \mathrel{R} b\) denotes the boolean statement \((a,b) \in R\) and \(a \not\mathrel{R} b\) denotes \((a,b) \notin R\).
    A \deflab{total order} on \(A\) is an endorelation \(\rel\) on \(A\) such that for each \(a, b \in A\)
    \begin{inparaenum}
        \item if \(a = b\) then \(a \rel b\),
        \item otherwise either \(a \rel b\) and \(b \not\rel a\), or \(b \rel a\) and \(a \not\rel b\).
    \end{inparaenum}

    \paragraph{Functions.}
    A (total) function from \(A\) to \(B\) is a relation from \(A\) to \(B\) such that for each \(a \in A\), there is exactly one \(b\) with \((a,b) \in f\).
    As usually, we denote the type of such a function by \(A \to B\), a quantification by \(f\colon A \to B\), and the argument-value relation by \(f(a) = b\).
    A partial function from \(A\) to \(B\) is a subset \(f \subseteq A \times B\) such that for each \(a \in A\), there is at most one \(b\) with \((a,b) \in f\).
    The type of a partial function is denoted with a upper half arrow e.g. \(f\colon A \parto B\).
    The image of a (total or partial) function is the set of elements in \(B\) that are assigned to any value in \(A\), denoted my \(\im(f) = \{b\in B \mid (A \times \{B\}) \cap f \neq \emptyset\}\).
    The domain of definition of a partial function is the set of elements in \(A\) with an assigned value in \(B\), denoted by \(\dom(f) = \{a\in A \mid (\{a\} \times B) \cap f \neq \emptyset\}\).
    In some contexts, a partial function \(f\) is just handled in the same manner as a total function from \(\dom(f)\) to \(B\) without further notice.

    \paragraph{Families.}
    An \(A\)-indexed \deflab{family} over \(B\) is a function \(f\colon A \to B\), often  denoted in a builder notation of the form \(\big(\mathrm{term}(a) \mid a \in A\big)\).
    The left-hand side \(\mathrm{term}(a)\) may assume any term and/or equation that depends on the index variable \(a\) to express an element of \(B\), and the right-hand side expresses the domain of the index variable \(a\).
    There may be multiple index variables, in that case \(A\) is a cross product of the index variables' domains.
    E.g.\@ if there are two indices \(x\) and \(y\) in the domains \(X\) and \(Y\), then we may also denote them individually in the builder notation as follows: \(\big( f(x,y) \mid x \in X, y \in Y \big)\).
    As an alternative to the builder notation, we also denote the argument-value relation in a form similar to the usual function notation, e.g.\@ an \(A\)-indexed family \(f\) is defined such that \(f_a = \mathrm{term}(a)\) for each \(a \in A\), where \(\mathrm{term}(a)\) is some term that may use the variable \(a\).
    Usually, we identify a family by some variable name and access its elements using this identifier and the index variables in its subscript or in function notation.
    If the index set is defined in the builder notation or clear from the context, it the term \(A\)-indexed may be dropped and leaves the term family.

    Families are used implicitly in terms where an associative and commutative binary operation is lifted to a collection of elements.
    For example, the union of \(k\) sets \(\big(U_i \mid i \in [k]\big)\) may be written as \(\bigcup_{i \in [k]} U_i\).
    Besides the union (\(\bigcup\)), we will use such operators for the sum (\(\sum\)), product (\(\prod\)), minimum (\(\min\)) and maximum (\(\max\)).
    If the values and index set of a family coincide in such a notation, then the index is omitted completely.
    For example, we write \(\min [k]\) instead of \(\min_{i \in [k]} i\).
    The minima and maxima are determined according to the usual numerical order.
    In case we encounter different objects or we use any other total ordering \(\unlhd\), we denote it as a superscript, e.g. \(\min^{\unlhd}\).
    \(\argmin\) yields all indices with minimal value, for instance \(\argmin_{i \in [k]} v_i\) is the subset \(M\) of \([k]\) such that there are no .

    An \(A\)-indexed \deflab<family>{partition} of \(B\) is a \(A\)-indexed family \(p\) over subsets in \(\power(B)\) such hat:
    \begin{inparaenum}
        \item For each pair \(a, a' \in A\) of distinct elements \(a \neq a'\), the assigned subsets are disjoint: \(p(a) \cap p(a') = \emptyset\).
        \item The union of all subsets \(\bigcup_{a \in A} p(a)\) is the set \(B\).
    \end{inparaenum}

    \paragraph{Sequences.}
    A (finite) \deflab{sequence} over \(A\) of length \(k \in \DN\) is a \([k]\)-indexed family over \(A\); \(A^k\) denotes all sequences over \(A\) of length \(k\).
    When a collection of \(k\) elements in \(A\) is quantified, they are usually enumerated with an ellipsis and separated by commas; this notation of the form \(a_1, \ldots, a_k \in A\) defines an implicit sequence \((a_i \in A \mid i \in [k])\).
    With a collection of fixed elements \(a_1, \ldots, a_k \in A\), the sequence \((a_i \in A \mid i \in [k])\) is abbreviated by an ellipsis without commas \(a_1 \ldots a_k\) (when there is a clear enumeration of items, like the index of the variables \(a_1, \ldots, a_k\)).
    An occurrence of the single element \(a_1\) may also denote the singleton sequence (of length 1).
    The \deflab<sequence>{empty sequence} (of length 0) is denoted by \(\varepsilon\).
    The set of all finite sequences over \(A\) is \(A^* = \bigcup_{k \in \DN} A^k\).
    \(A^+\) denotes the set of all non-empty sequences \(\bigcup_{k \in \DN_+} A^k\).
    The length of a sequence \(w \in A^*\) is denoted by \(|w|\).
    For any pair of sequences \(u,v \in A^*\), the \deflab<sequence>{concatenation} \(u \cdot v\) is the sequence \(w\in A^{|u|+|v|}\) such that \(w_i=u_i\) for each \(i \in [|u|]\) and \(w_{|u|+j} = v_j\) for each \(j \in [|v|]\).
    The concatenation is abbreviated by dropping the dot, and it is taken advantage of its associativity to drop parenthesis in terms, so that e.g. \(w_1 w_2 w_3 w_4\) denotes \(((w_1 \cdot w_2) \cdot w_3) \cdot w_4\)
    The concatenation of a whole collection of sequences \((w_i \in A^* \mid i \in [k])\) in order of their indices is denoted with an ellipsis \(w_1 \ldots w_k\).

    A \deflab<sequence>{subsequence} \(v \in A^*\) in \(w\in A^*\) is a sequence of elements occurring consecutively within \(w\).
    The subsequence \(v\) may be located within \(w\) by a pair of indices for its start \(s \in [|w|+1]\) and its end \(e \in [s, |w|+1]\) such that \(v_i = w_{s-1+i}\) for each \(i \in [e-s])\).
    Whenever such a subsequence in \(w\) is identified using its location between the indices \(s\) and \(e\), it is denoted by \(w_{s:e}\).
    If one of the two indices is left empty, it denotes a prefix or postfix of the sequence \(w\).
    E.g. \(w_{:s}\) are the first \(s-1\) items and \(w_{s:}\) are the trailing \(|w|-s-1\) items in \(w\).

    Let \(\unlhd\) be a total order on \(A\), the \deflab<relation>[lexorder]{lexicographic order} on \(A^*\), denoted by \(\unlhd^*\), is defined such that
    \begin{compactenum}
        \item \(w \unlhd^* w \cdot v\) for each \(w, v \in A^*\), and
        \item \(w\cdot a\cdot u \rel^* w\cdot b\cdot v \) for each \(a,b \in A\) with \(a \rel b\) and each \(w,u,v \in A^*\).
    \end{compactenum}
    For example, \(\leq^*\) is the lexicographic order over sequences of numerical values where \(\varepsilon \leq^* 1\:1\:2\) and \(1\:1\:2\leq^*1\:2\).


    \section{Strings and Trees}\label{sec:preliminaries:trees}
    \paragraph{Alphabets and Strings.}
    An \deflab{alphabet} is a non-empty and finite set, its elements are called \deflab<alphabet>{symbol}[symbols].
    Usually, uppercase Greek letters are used to denote alphabets, like \(\varSigma\), \(\varPi\) or \(\varGamma\).
    A \deflab<sequence>{string} is a (finite) sequence over an alphabet \(\varSigma\).
    The set of all strings over \(\varSigma\) is \(\varSigma^*\).
    The notation for sequences is also used for strings.
    Subsequences in a string are called \emph{substrings}.

    \paragraph{Trees.}
    \deflab{tree}[Trees] are particular strings that contain opening and closing parentheses to denote nesting hierarchies.
    For any alphabet \(\varSigma\) and finite set \(\varGamma\), the set of trees \(\T_\varSigma(\varGamma)\) is defined inductively: it is the smallest subset \(T\) of \((\varSigma \cup \varGamma \cup \{ (, )\})^*\) such that
    \begin{inparaenum}
        \item \(\varGamma \subseteq T\) and
        \item for each symbol \(\sigma \in \varSigma\), natural number \(k\) and trees \(t_1, \ldots, t_k \in T\), the string \(\sigma(t_1 \cdots t_k)\) is in \(T\).
    \end{inparaenum}
    As such, trees are finite structures, but unlike strings, we do not consider empty trees.
    In case \(\varGamma = \emptyset\), trees exclusively contain symbols in \(\varSigma\); then the set \(\T_\varSigma(\varGamma)\) is abbreviated by \(\T_\varSigma\).

    A \deflab<tree>{node} in the tree \(t \in \T_\varSigma(\varGamma)\) is an occurrence of a symbol in \(\varSigma \cup \varGamma\).
    If a node a accompanied by a sequence of trees denoted in parentheses behind the symbol, these trees are called the \deflab<tree!node>{child}[children] of the node.
    Vice versa, from viewpoint of a child node, the node before the wrapping parenthesis is called the \deflab<tree!node>{parent}.
    Each other node within the parenthesis is called a \deflab<tree!node>{sibling}.
    The leftmost node is called the \deflab<tree>{root} of the tree; each node without children is called a \deflab<tree>{leaf} of the tree; and each node with children is called an \deflab<tree>[inode]{inner node}.
    The \deflab<tree!node>{rank} of a node is its number of children, \(\rank(t)\) denotes the rank of the root in \(t\).
    Each leaf is of rank zero; nodes of rank one are called \deflab<tree!node>{unary}, those of rank two are called \deflab<tree!node>{binary}.
    A tree is called \deflab<tree>[bintree]{binary}, if each inner node has exactly two successors.

    Each illustration of a tree usually starts with the root node at the top, below that are its children connected by edges, this scheme recurs until the leaves at the bottom.
    \cref{fig:pre:ctree} shows an example.

    \paragraph{Positions.}
    A \deflab<tree>{position} in \(t\) is a sequence $w \in \DN^*$ that identifies a node in the tree.
    Starting with the root, each integer in \(w\) indicates to the index of a child whose root is the reference for the remaining integers in \(w\).
    Alas, the set of positions, denoted by \(\pos(t)\), is defined recursively over the structure of trees:
    \begin{inparaenum}
        \item if \(t \in \varSigma\), then it is \(\pos(t) = \{\varepsilon\}\), and
        \item if \(t = \sigma(t_1 \cdots t_k)\) for some \(\sigma\in \varSigma\) and \(t_1, \ldots, t_k \in \T_\varSigma\), then it is \(\pos(t) = \{\varepsilon\} \cup \bigcup_{i\in[k]} \{i\}\cdot \pos(t_i)\).
    \end{inparaenum}
    The set of all inner node positions in \(t\) is denoted by \(\npos(t)\), the set of leaf positions is \(\lpos(t)\).
    Let \(\rho \in \pos(t)\) be some position.
    Each subsequence in \(t\) that is also a tree in \(T_\varSigma\) is called a \deflab<tree>{subtree} in \(t\).
    The subtree below a position \(\rho\) is denoted by \(t|_\rho\).
    The \deflab<tree>{ancestor}[ancestors] of \(\rho\) are all prefixes of the sequence \(\rho\), denoted by \(\ancestors(\rho) = \{ \rho_{:e} \mid e \in [|\rho|+1] \}\).
    The \deflab<tree>{descendant}[descendants] of \(\rho\) in \(t\) are all positions in the subtree below \(\rho\), denoted by \(\descendants_t(\rho) = \{ \rho \cdot \rho' \in \pos(t) \mid \rho' \in \DN^* \}\).
    The symbol at a position \(\rho\) in \(t\) is denoted  \(t(\rho)\).

    A \deflab<tree>{subforest} of \(t\) is a sequence of trees occurring consecutively (as siblings) within \(t\).
    A subforest may be located within \(t\) using a position \(\rho\) accompanied by a concluding child index that is greater or equal to the last child index in \(\rho\).
    Let \(i, e \in \DN_+\) and \(\rho' \in \pos(t)\) such that \(\rho = \rho' \cdot i\) and \(e \in [i,\rank(t|_{\rho'})+1]\).
    Analogously to the location of subsequences, the notation \(t|_{\rho:e}\) denotes the subforest in \(t\) at positions \(\rho' \cdot i, \rho' \cdot (i+1), \ldots, \rho \cdot (e-1)\).

    \paragraph{Indexed Trees.}
    In \deflab<tree>[itree]{indexed tree}[indexed trees], the symbols of an alphabet \(\varSigma\) occur only at inner nodes.
    All leaves are positive integers such that each occurs at most once in each tree.
    For each inner node in an indexed tree, the children occur ordered by their least leaf.
    The notation for positions, indexing, substitution are inherited from trees; the yield of an indexded tree is its set of leaves instead of a string.
    The set of all indexed trees over \(\varSigma\) is denoted by \(\itrees^\varSigma\).

    \paragraph{Substitution.}
    Let \(X\) be a finite set, \(\varSigma\) some alphabet that is disjoint from \(X\), and \(c\) a string in \((X \cup \varSigma)^*\).
    A (first-order) \deflab{substitution} of \(X\) in \(c\) defined by an \(X\)-indexed family \((v_x \in \varSigma^* \mid x \in X)\) is denoted by \(c[v]\) and yields the string in \(\varSigma^*\) obtained from \(c\) by replacing every occurrence of symbols \(x \in X\) by \(v_x\).
    To avoid explicitly defining a family for a substitution, we may also denote the substitution by enumerating the values in the form \(c[x_1=v_{x_1}, \ldots, x_{k}=v_{x_k}]\) where \(k\) is some natural number and \(X = \{x_1, \ldots, x_k\}\) with \(x_i \neq x_j\) for each pair \(i,j \in [k]\) if \(i\neq j\).
    If the string \(c\) does not contain some variables in \(\X\), we may omit them in this notation.

    The concept of \deflab<substitution>{second-order substitution} introduces a second layer by parameterizing the values before inserting them.
    For the scope of this thesis, the number of parameters for each value is limited to exactly one to keep the definition simple.
    Let's consider a single distinct variable \(y \notin X\), a set of shallow trees \(X(\varSigma^*) = \{ x(w) \mid x \in X, w \in \Sigma^* \}\) and a string \(c\) in \((X(\varSigma^*) \cup \varSigma)^*\).
    A second-order substitution of \(X\) in \(c\) defined by an \(X\)-indexed family \((v_x \in (\varSigma \cup \{y\})^* \mid x \in X)\) yields the string in \(\varSigma^*\) obtained from \(c\) by replacing each occurrence of shallow trees \(x(w) \in X(\varSigma^*)\) by \(v_x[y=w]\).
    We specifically stress that the variable \(y\) may be an element in \(\varSigma\) and the result \(c[v]\) might also contain it.

    Because trees are just specific strings, both definitions of substitution are easily lift to trees and sequences of trees, as long as the variables do not coincide with parentheses.
    Let \(\varGamma\) be an alphabet distinct from \(X\), and \(\{ (, ) \} \cap (X \cup \{y\}) = \emptyset\), and \(y \notin \varSigma\).
    Now, the variable occurrences are of the form \(X(\T_\varSigma(\varGamma)^*) = \{x(t_1, \ldots, t_k) \mid k \in \DN, t_1, \ldots, t_k \in \T_{\varSigma}(\varGamma)\}\) where each element has a root in \(X\), in the following the set is abbreviated by \(\hat{X}\).
    A second-order substitution of \(X\) in \(c \in \T_\varSigma(\varGamma \cup \hat{X})^*\)\todo{The set of leaves is not finite, either adapt def. of trees or allow only leaves as arguments. \(\T_{\varSigma \cup X}(\varGamma)\) is not an option.} defined by an \(X\)-indexed family \((v_x \in \T_\varSigma(\varGamma \cup \{y\})^* \mid x \in X)\) yields a sequence of trees in \(\T_\varSigma(\varGamma)*\) obtained from \(c\) by replacing each occurrence of \(x(v) \in \hat{X}\) by \(v_x[y=v]\).
    The variable \(\y\) may be an element of \(\varGamma\).

    \begin{example}[Substitution]
        Consider an alphabet \(\Sigma = \{\text{a}, \ldots, \text{g}\}\) of (terminal) symbols and two sets of variables \(\X = \{\x_1, \x_2, \x_3\}\) and \(\Y = \{\y\}\).
        In first-order substitutions, all occurring variables in a string are replaced by argument strings, e.g.
        \begin{align*}
            \x_1 \, \text{b} \, \x_2 \, \text{f} \, \x_3\,[\x_1=\varepsilon, \x_2=\text{cde}, \x_3=\text{g}] &= \text{bcdefg} \\
            \x_1 \, \text{bb} \, \x_1 \, [\x_1=\text{a}] &= \text{abba}
        \end{align*}
        In second-order substitutions, there is an argument with each variable \(x\) in \(\X\) that may be inserted for the variable \(\y\) in the value for \(x\), e.g.
        \begin{align*}
            \text{a} \, \x_1(\text{cc}) \, \text{a} \, [\x_1=\text{b}\,\y\,\text{b}] = \text{a} \, (\text{b}\,\y\,\text{b})[\y=\text{cc}] \, \text{a} &= \text{abccba} \\
            \x_1(\text{bb}) \, \x_1(\varepsilon) \, [\x_1=\text{a}\,\y\,\text{a}] = (\text{a}\,\y\,\text{a})[\y=\text{bb}] (\text{a}\,\y\,\text{a})[\y=\varepsilon] &= \text{abbaaa}
        \end{align*}

        The following case considers trees over \(\Sigma=\{\cn{vp}\}\) and \(\Gamma=\{\tn{was}, \tn{carried}, \y\}\):
        \begin{align*}
            \cn{vp}( \x_1(\y) \, \tn{was} )[\x_1=\cn{vp}(\tn{carried}\,\y)]
                &= \cn{vp}( \cn{vp}(\tn{carried}\,\y)[\y=\y] \, \tn{was} ) \\
                &= \cn{vp}(\cn{vp}(\tn{carried}\,\y)\,\tn{was})
        \end{align*}
    \end{example}

    \section{Modeling Natural Languages using Formal Methods}\label{sec:preliminaries:ctrees}
    In the syntactic analysis of sentences in natural languages, we deal with \deflab[ctree]{constituent tree}[constituent trees].
    They illustrate a hierarchy of sub-phrases in the sentence, where each element in the hierarchy is labeled by a category, called \deflab<constituent tree>[csymbol]{constituent symbol}.
    These sub-phrases do not necessarily form contiguous regions in the sentence:
    If such a tree contains a node with a non-contiguous sub-phrase, we call it \deflab<constituent tree>{discontinuous}; otherwise it is \deflab<constituent tree>{continuous}.
    E.g.\@ \cref{fig:pre:ctree} illustrates a discontinuous constituent tree.
    The symbols at inner nodes, like \(\cn{s}\) and \(\cn{vp}\), are constituent symbols.
    The lower node labeled by \(\cn{vp}\) governs a noncontiguous region in the sentence: \emph{where} and \emph{carried out}.

    \begin{figure}
        \null\hfill
        \subfile{figures/example-constituents.tex}
        \hfill\null

        \caption{\label{fig:pre:ctree}
            Discontinuous constituent tree for the phrase \emph{where the survey was carried out}.
            The tree is illustrated with crossing branches, so that the leaves appear ordered.
            For each constituent, the path to its lexical head is double-struck.
        }
    \end{figure}

    The first layer of nodes in a constituent tree above phrase positions contains distinct symbols, called \emph{part of speech} (\abrv{pos}) tags.
    They are mutually exclusive to constituent symbols in upper layers, i.e.\@ each symbol occurring at the first layer must be a \abrv{pos} tag and no other constituent symbol, and each symbol above the first layer must not be a \abrv{pos} symbol.
    They indicate the most primitive categorization of individual tokens in the sentence.
    In our example tree, the symbols \(\cn{wrb}\) and \(\cn{pt}\) are \abrv{pos} tags.

    Within a constituent tree, we consider a \emph{lexical head} as the critical token for a syntactic category (i.e. the constituent symbol) of a phrase.
    For each inner node in the constituent tree, there is exactly one position in the sentence predestined for this role.
    Vice versa, a position may be the lexical head of multiple inner nodes.
    However, there is one condition: The lexical head must either be a leaf directly below the node or a lexical head of a child node.

    Now, let us formalize the above.
    We assume an alphabet of \deflab{token}[tokens] \(\Sigma\), i.e.\@ words, punctuation marks, parentheses, etc.\@ that are used to form sentences, and two additional alphabets, \(\varPi\) of \abrv{pos} tags, and \(\varGamma\) of constituent symbols.
    \(\Sigma^*\) denotes the set of all strings over these tokens, a superset that contains all grammatical sentences (and also everything else that consists of the tokens in \(\Sigma\)).

    A \emph{constituent tree} is a triple \((t, p, w)\) where
    \begin{compactitem}
        \item \(w\) is a string of tokens in \(\varSigma^*\),
        \item \(t\) is an indexed tree over \(\varGamma\) such that each position in \([|w|]\) occurs exactly once as a leaf, and
        \item \(p\) is a string of \abrv{pos} tags in \(\varPi^{|w|}\).
    \end{compactitem}
    In each constituent tree, we distinguish three parts: the sequence of tokens, the \abrv{pos} tag above each token, and the \deflab<constituent tree>[cstruct]{constituent structure} that contains the constituent symbols an defines the hierarchical structure.
    To accommodate discontinuous constituents, each constituent structure is defined as an indexed tree over \(\varGamma\) so that each leaf is the position of a token.

    \(\head(t)\) denotes the leaf that indexes the lexical head for the root in the constituent structure \(t\).
    When a tree is illustrated and its lexical heads are considered, then, for each inner node in the constituent structure, the path to its lexical head is double-struck.
    \Cref{fig:pre:ctree} shows an example.
    %    We call each direct child that does not contain the lexical head a \emph{modifier}.

    A \emph{treebank} is a finite set of constituent trees.
    They are assembled by linguists for texts collected from news articles, written speeches, etc.\@ and annotated with their constituent structure.
    In the scope of this thesis, treebanks form data sets used for learning models that are able to predict constituent structures for input sentences.
    The process of learning a model is called \emph{training}, predicting constituent structures is called \emph{parsing}, and the used models are called \emph{parser}.

    Besides constituent trees, there are \emph{dependency trees} that express a syntactic analysis of natural language.
    They present grammatical relations between phrases within a sentence, such as arguments, adjuncts and complements, sometimes even more fine-grained ones like distinctions between subjects and objects.
    These relations are illustrated in a tree where each phrase is represented by its head.
    There are similar phenomena as (dis-)continuity in dependency trees, they are referred to as \emph{(un-)projectivity}.
    We will not engage in dependency trees in this work, some references are discussed in \cref{sec:literature}.

    \section{Empirical Predictions}
    Under the broad term \deflab{prediction}[(empirical) prediction], we subsume each process that assigns an output of predetermined shape to an input based on previously seen data consisting and prior knowledge.
    In our cases, such an assignment is determined by choosing an output that optimizes a \deflab<empirical prediction>{confidence} value (e.g. a score or probability) among a predefined set of possible outputs.
    The following defines some important terms that are used in conjunction with empirical predictions.
    Here, we will cover some basic terms that are used throughout this thesis.
    For more detailed elaborations in this field, refer to \Citet{Moh12,Dau17}.

%    \paragraph{Prediction discipline}
%    There are certain tasks that do not only fall under the abstract label \emph{prediction}, but also share other similarities.
%    Some models may have been proven advantageous in their setting, we can build upon these experiences.
%    \begin{asparaitem}
%        \item
%            We call a prediction a \emph{classification}, if it assigns a single label in a predetermined set to the input.
%        \item
%            \emph{Sequence tagging} assigns a single label for each position in an input sequence, it can be seen as a classification for each element in the input.
%        \item
%
%    \end{asparaitem}

    \paragraph{Models.}
    A \deflab<empirical prediction>{model}[(prediction) model] defines the domain of inputs, outputs, confidence values and variables as well as how confidence values are determined.
    The variables (called \deflab<prediction!model>{parameters}) steer the computed confidence values.
    Consider a model as a parametrized function \(\varphi_\theta\colon X \times Y \to \mathcal{C}\) where \(\theta\) are some parameters (or variables) in an associated domain \(\Theta\), \(X\) is the set of inputs, \(Y\) is the set of outputs and \(\mathcal{C}\) the domain of confidence values, and a total order on \(\mathcal{C}\) denoted by \(\rel\).\todo{prelim: parametrized function}
    Usually, the domain of confidence values are real numbers and the order \(\rel\) is \(\leq\), such that the confidence is interpreted as a score.
    In the following, we will silently assume this coincidence.

    In general, we distinguish two types of models: \deflab<prediction!model>{generative models} and \deflab<prediction!model>{discriminative models}.
    While generative models define a joint probability distribution of in- and outputs, discriminative models define a probability or score of their output conditioned on the input.
    Generative models can be used to generate samples of pairs containing in- and outputs.
    Discriminative models on the other hand, are not able to do this in general, as they do not model the structure of the input.

    \begin{example}[Statistical parsing in the empirical prediction framework.]
        Parsing is a process that predicts a constituent structure for a given sequence of tokens.
        A statistical grammar formalism is a model that has proven successful in this setting in the past.\cite{}
        It defines a probability distribution for constituent trees \(p\colon \mathcal{C} \to \DR\), usually by the sum over all derivations per constituent tree and by the product of all rules' weights per derivation.
        This is formalized in the equation \[
            p(\xi) = \sum_{d \in D(\xi)} \prod_{\rho \in \pos(d)} \hat{p}(d(\rho))
        \]
        where \(\xi \in \mathcal{C}\) is a constituent tree, \(D(\xi)\) is the set of all derivations in the grammar for the constituent tree \(\xi\), and \(\hat{p}\) is a weight function for the rules in the grammar.
        The function \(\hat{p}\) is defined such that the term \(\prod_{\rho \in \pos(d)} \hat{p}(d(\rho))\) is a probability distribution over all derivation trees \(d \in D\) in the grammar.
        As the sequence of tokens \(w\) is part of the constituent tree \(\xi\), this is trivially equivalent to a joint probability distribution \(p(\xi) = p(w, \xi)\), which is characteristic for a generative model.
        The values assigned by the function \(\hat{p}\) form the model parameters, they are restricted to form probability distributions.
        Probabilistic grammars can easily be used to generate constituent trees (again, trivially in tandem with a sequence of tokens) by probabilistically sampling rules.
    \end{example}

    \begin{example}[Sequence tagging in the empirical prediction framework.]
        \todo[inline]{dazu müssten erst \abrv{ann} erklärt werden}
    \end{example}

    \paragraph{Parameters.}
    We distinguish two essential sets of parameters: \deflab<prediction!model!parameters>{hyper parameters} and \deflab<prediction!model!parameters>{model parameters}.
    Hyper parameters determine details about the model and training process itself. %, to some extend, the model and the training objective can also be seen as hyper parameters.
    They are not trained upon, but are chosen for the training either by prior knowledge or by trying different combinations and manually evaluating their results.
    Model parameters are variables defined by a model that are tuned during the training process to match a training objective.

    \paragraph{Training.}
    \deflab<prediction>{training}[Training] is the process of tuning the parameters of a model in terms of a \deflab<prediction!training>{training objective}.
    A training objective assesses the assigned confidence values of a model in a sample of input and output pairs and defines a criterion for optimal parameters.
    In some cases, it is possible to compute the model parameters that optimize the training objective in closed form.
    In other cases, there are iterative processes that change the parameters to gradually improve upon the criterion.
    \todo{supervised vs. semi-supervised (i.e. what are pretrained models and what is fine-tuning) vs. unsupervised training}

    \paragraph{Data and splits.}
    The \deflab<prediction>{data sets} contain pairs of input and output that we use to deduce a prediction model.
    In this context the, the output (i.e. the to-be predicted) component of these instances is referred to as \deflab<prediction!data sets>{gold}.
    The data instances are used in two roles: for training, and for assessing a prediction model.
    The data sets are usually split into three parts to accommodate these roles:
    \begin{inparaenum}
        \item a \deflab<prediction!data sets>{training portion} that is solely used to train a model,
        \item a \deflab<prediction!data sets>{development portion} that is used to monitor the model during the training process and assess the selection of hyper parameters, and
        \item a \deflab<prediction!data sets>{test portion} that is used to assess a final model.
    \end{inparaenum}
    For common data sets that are used within a community, specific partitions are shared to enable fair comparisons between publications.
    In such publications, the results of different models are compared using the same test portions of the same data sets.

    There are two distinct examples for empirical predictions in (or at least near) the scope of this thesis.

    \begin{example}[Statistical parsing in the empirical prediction framework.]
        Parsing is a process that predicts a constituent structure for a given sequence of tokens.
        Statistical grammar formalisms are models that have proven successful in this setting in the past.
        They define a joint probability for each sentence and constituent tree, and therefore are generative models.
        The specific grammar formalism, certain restrictions of the probability distributions, or details about the grammar induction (such as binarization) may form the set of hyper parameters.
        The process of training tunes the probability assignment for each rule to maximize the likelihood in a given treebank.
        In case of naive probabilistic grammars, this is computed in closed form by relative frequency estimation.
    \end{example}

    \begin{example}[Supertagging in the empirical prediction framework.]
        In the case of parsing with supertagging, an integral part of the parsing process is an assembly of a small grammar by choosing some rules out of a predetermined set of rule blueprints (this specific process is called supertagging).
        The process may also be described using a generative model.
        But in practice, we use specific neural networks for this task, as they have proven very accurate in the past; they are textbook examples for discriminative models.
        They define many hyper parameters that determine the \emph{architecture} (i.e. the shape of the neural network), e.g.\@ the used word embeddings, its depth or the number of parameters in a certain layer, and many details about the training process, e.g.\@ the learning rate or some condition that ends the iterative training process.
        During the training process, a treebank is transformed into tuples of the token sequence and the optimal choice of blueprints for each constituent tree.
        The training objective maximizes the likelihood of the blueprints conditioned on the input tokens.
    \end{example}

    \subsection{Neural Networks}
    As mentioned in the last example, we use neural networks to implement certain prediction models.
    They are a family of parametrized functions that emerged in the last decades and proved extremely useful in many tasks, including ones settled in natural language processing. \todo{ref examples}
    As in the parent section, we will only give a brief overview for the concepts needed to understand further sections of this theses.
    For further elaborations in the field of neural networks for natural language processing, consult \cref{Gol17}.

    In general, we consider a neural network (short \abrv{ann} for \emph{artificial} neural network) a parametrized function  where the outputs are vectors of real numbers, i.e.\@ they are of the form \(f_\theta\colon X \to \DR^n\) where \(\theta\) denotes the parameters, \(X\) the set of inputs and \(n > 0\) is the output dimension.
    There is a set of well studied \abrv{ann}s that are used as \emph{modules} to form new \abrv{ann}s by composition; such a composition is called an \emph{architecture}.
    In the following, some of these modules are briefly explained.

    \subsubsection{Embeddings}
    Embeddings are used to assign vectors to any other type of objects.
    These assigned values are the foundation for computations in other modules, which usually assume real vectors as input.
    The most simple form of embeddings are \deflab<>{static embeddings}.
    Let us consider a set of input objects \(X\), a bijection \(\mathit{idx}\colon X \to [|X|]\) and an output dimension \(n \in \DN_+\).
    A static embedding for \(X\) of dimension \(n\) is a parametrized function \(f_W\colon X \to \DR^n\) which assumes a matrix of parameters \(W \in \DR^{(|X| \times n)}\) and assigns a row vector to each element in \(X\), i.e.\@ \(f_W(x) = W_{\mathit{idx}(x)}\) for each \(x \in X\).
    In some applications, it is sensible to add a vector for unknown elements, e.g.\@ for unknown tokens that do not occur in the data used for training.
    If the input is a sequence, static embeddings are often applied element-wise.
    E.g.\@ a sequence of tokens \(\sigma_1 \cdots \sigma_n\) is usually embedded as a sequence of vectors \((f_W(\sigma_i) \mid i \in [n])\).
    There are pretrained token embeddings that have proven convenient in many tasks and are shared in the community, such as fasttext \citep{} and word2vec \citep{}.

    Some architectures complement element-wise token embeddings by \emph{character-wise embeddings}. \citep{}
    These are recurrent modules (cf.\@ \cref{sec:preliminaries:nn:recurrent}) that process the sequence of characters and yield a vector of specified length for each token (which, in turn, requires a static embedding for the set of characters).
    This type of modules is associated with the handling of unknown words with the following intuition:
        If a token does not occur in the training data, there is no specific element-wise embedding for it.
        But the character-wise embedding might have learned some sensible information based on similar tokens in the training data.

    Recently, specialized methods for sequence embedding that do also consider positional and contextual information in the assignment were published, such as \abrv{bert} \citep{Devlin2019} and flair embeddings \citep{Akb19}.
    However, they are (in some cases rather large) compositions of \abrv{ann} modules for sequence tagging themselves, using a transformers and \abrv{lstm} units, respectively.

    \subsubsection{Linear Layers, Activation Functions and Multi-Layer Perceptrons}
    After embeddings, linear layers are an essential part in \abrv{ann}s.
    They are a linear transformation of vectors from an a specified input to an output dimension.
    Formally, let \(m, n \in \DN_+\) by some input and output dimensions.
    Then a linear layer is a function \(f_{(W,b)}\colon R^m \to R^n\) where the matrix \(W \in R^{(m\times n)}\) and the bias vector \(b\in R^n\) are parameters and \(f_{(W,b)}(x) = Wx+b\).

    Activation functions is a general term for non-linear functions that are used in between compositions of linear layers such that the result of the composition is not linear.
    There are some options that implement desirable properties or interpretations in some settings, such as the following:
    \begin{enumerate}
        \item
            A \emph{Rectified Linear Unit (ReLU)} is very easy to compute and used in large architectures to save computational power.
            It is defined for each vector \(x\in R^n\) for any dimension \(n \in \DN_+\) by cutting-off negative values: \(\mathrm{relu}(x) = (\max(0, x_i) \mid i \in [n])\).
        \item
            The \emph{sigmoid function} is applied point-wise to each component of a vector and assigns values in the interval between \(0\) and \(1\) as follows: \(\mathrm{sigmoid}(x) = (\frac{1}{1-e^{x_i}} \mid i \in [n])\) for each \(x \in \DR^n\) and \(n \in \DN_+\).
            These values are often interpreted as one value of a binary probability distribution.
        \item
            The \emph{softmax function} also maps each value in a vector to the interval between \(0\) and \(1\).
            They are normalized such that they sum to \(1\) and are therefore often interpreted as a probability distribution over the set of indices of the vector.
            It is defined as follows: \(\mathrm{softmax}(x) = (\frac{e^{x_i}}{z} \mid i \in [n])\) for each \(x\in \DR^n\) and \(n \in \DN_+\) where \(z = \sum_{i \in [n]} e^{x_i}\).
    \end{enumerate}
    The latter two are also used as outer-most modules in many architecture, specifically because of their interpretation as probabilities.

    A \emph{multi-layer perceptron} is a composition of multiple linear layers with activation functions.


    \subsubsection{Recurrent Neural Networks}
    Recurrent modules are a foundation for the processing of sequence inputs in neural networks.
    They are defined, such that each input vector is complemented with the previous output, starting with an initialization vector for the first input.
    Formally, this concept of recurrence uses some inherent module \(f_\theta\colon \DR^{n+m} \to \DR^m\) parametrized by some \(\theta\) and incrementally computes, for an input sequence \(x_1, \ldots, x_n \in \DR^n\), a sequence of outputs \(g_{(f, \theta, h_0)}(x_1, \ldots, x_n) = (h_i = f_\theta(x_i \cdot h_{i-1}) \in \DR^m \mid i \in [n])\) where \(h_0 \in \DR^m\) is an initial value and parameter of the recurrent module \(g\).

    Linear short-term memory (short \abrv{lstm}) cells \citep{Hoc97} established as a quasi-standard for these inherent modules.
    In addition to the previous output \(h_{i-1}\) that is used to compute \(h_i\), they also carry a cell state between the computations for each position, which is not part of the output.

    Bidirectional recurrent modules combine the vectors obtained by two recurrent modules, one for the sequence from left to right and the other in the opposite direction, by concatenating the vectors for each position.


    \subsubsection{Transformers}
    The transformer architecture \citep{vaswani2017attention} became a standard tool in sequence processing with neural networks, and in some cases even superseded recurrent modules in the last few years. \citep{}
    Specifically in the form of pretrained models that are shared in the community, they have shown impressive accuracy. \citep{}
    Transformers do not employ the concept of recurrence, but are based around \emph{self-attention} and \emph{positional embeddings} to process sequences.


    \subsection{Assessing the Prediction of Constituent Structures}
%    When dealing with the assessment of parsers, we compare predicted constituent structures to the ones found in treebanks.
%    In this context, the structures found in these treebanks are called \emph{gold} or ground truth.
%    In general, it is desired to avoid using the same gold constituent trees for learning the parser as well as for the assessment.
%    Therefore, the treebanks are partitioned into distinct parts for training and assessment; usually there are three parts:
%    \begin{inparaenum}
%        \item a \emph{training portion}]that is solely used to learn a parsing model,
%        \item a \emph{development portion} that is used to monitor the model \emph{during} the training process, and
%        \item a \emph{test portion} that is used to assess the model \emph{after} the training process.
%    \end{inparaenum}
%    Such a partition is called a \emph{split}.
%    For each treebank that is publicly available, there is also a split that is shared in the parsing community.
%    So when different parsers are compared for the same treebank, they were trained and assessed using the same sets of trees.
%
    When assessing the quality of a parser, we measure two dimensions:
    \begin{inparaenum}
        \item the time needed for the prediction process, and
        \item the accuracy of predicted structures.
    \end{inparaenum}
    The process for measuring the first dimension is rather intuitive:
    The time is taken for the prediction of a constituent structure for each sentence in the test portion of a treebank.
    This time is reported either as sum for the whole test portion or as quotient \(\frac{\text{number of sentences}}{\text{parsing time}}\) for the number of parsed sentences per second.

    The evalb-style labeled \emph{f1-score} \citep{Black91,Col97} established as the standard for measuring the accuracy of predicted constituent structures.
    This score is computed as follows:
    The sets of predicted and the gold constituent structures are mapped each into a set that contains a tuple \((\xi, A, \mathit{yd})\) for each constituent structure \(\xi\) and each of its inner nodes with label \(A\) and yield \(\mathit{yd}\).
    According these two sets, \(P\) for all predictions and \(G\) for the gold constituent trees, the precision (\(p = \frac{|P \cap G|}{|P|}\)), recall (\(r = \frac{|P \cap G|}{|G|}\)) and f1-score (\(\frac{2\cdot p\cdot r}{p + r}\)) are computed in the usual fashion.
    The \abrv{pos} tags are often part of the prediction but not assessed as part the f1-score.
    They are rather reported separately as the accuracy of the predicted \abrv{pos} tags compared to the gold ones.
    The python tool \emph{discodop} supplies an implementation for both, the f1-score of constituent structures and the accuracy of \abrv{pos} tags. \citep{CraSchBod16}

    \begin{figure}
        \null\hfill
        \begin{tikzpicture}[baseline=(t1mid.base)]
            \begin{scope}[every node/.style={inner sep=2pt, font=\small}, level distance=3ex, level 3/.style={sibling distance=2em}]
                \node {\cn{sbar}}
                child { node (t1) {\cn{s}}
                    child { node {\cn{vp}}
                        child { node (t1mid) {\cn{vp}}
                            child { node {\cn{wh}} child {
                                    node {1}}}
                            child { node {5}}
                            child { node {\cn{prt}} child {
                                    node {6}}}}
                        child { node {4}}}
                    child { node {\cn{np}}
                        child { node {2}}
                        child { node {3}}}};
            \end{scope}
            \begin{scope}[every node/.style={inner sep=2pt, font=\small}, level distance=3ex]
                \node[right=4.5cm of t1] (t2) {\cn{s}}
                [sibling distance=5em]
                child { node {vp}
                    [sibling distance=1em]
                    child { node (t2mid) {1}}
                    child { node {4}}
                    child { node {5}}
                    child[sibling distance=1.5em] { node {\cn{prt}} child {
                            node {6}}}}
                child { node {\cn{np}}
                    [sibling distance=1em]
                    child { node {2} }
                    child[sibling distance=1.5em] { node {\cn{nn}} child { node {3}}}};
            \end{scope}
            \node[left=2em of t1] {\(\xi_\text{G}\):};
            \node[left=2em of t2] {\(\xi_\text{P}\):};
        \end{tikzpicture}
        \hfill
        \begin{minipage}{.3\linewidth}
            \small
            \begin{align*}
                \mathit{precision} &= \tfrac{4}{5} \\
                \mathit{recall} &= \tfrac{4}{7} &
                \mathit{f1} &= \tfrac{2}{3} \\
            \end{align*}
        \end{minipage}
        \hfill\null

        \begin{minipage}{.35\linewidth}
            \small
            \begin{align*}
                G = \{
                &(\cn{sbar}, [6]),
                (\cn{s}, [6]), \\
                &(\cn{vp}, \{1,4,5,6\}),\\
                &(\cn{vp}, \{1,5,6\}),
                (\cn{np}, \{2,3\}), \\
                &(\cn{wh}, \{1\}),
                (\cn{prt}, \{6\}) \}
            \end{align*}
        \end{minipage}
        \hfill
        \begin{minipage}{.3\linewidth}
            \small
            \begin{align*}
                P = \{
                &(\cn{s}, [6]), \\
                &(\cn{vp}, \{1,4,5,6\}),\\
                &(\cn{np}, \{2,3\}),
                (\cn{nn}, \{3\}) \\
                &(\cn{prt}, \{6\}) \}
            \end{align*}
        \end{minipage}
        \hfill
        \begin{minipage}{.3\linewidth}
            \small
            \begin{align*}
                P \cap G = \{
                &(\cn{s}, [6]), \\
                &(\cn{vp}, \{1,4,5,6\}),\\
                &(\cn{np}, \{2,3\}) \\
                &(\cn{prt}, \{6\}) \}
            \end{align*}
        \end{minipage}

        \caption{
            Example for the f1-score computation for a single pair of gold (\(\xi_\text{G}\)) and predicted (\(\xi_\text{P}\)) constituent structures.
            The sets of constituent tuples for both trees and the set of common constituents are illustrated below them.
            The upper right corner shows the values for precision, recall and f1-score.
        }
    \end{figure}

    \ifSubfilesClassLoaded{%
        \printindex
        \bibliography{../references}%
    }{}
\end{document}
