\documentclass[../document.tex]{subfiles}

\begin{document}
    \chapter{Preliminaries}\label{sec:preliminaries}
    This chapter deals with the mathematical foundations and notation used throughout all chapters.
    \Cref{sec:preliminaries:math} starts with the most basic notions.
    \Cref{sec:preliminaries:trees} deals with all concepts involved in string and tree structures.
    \Cref{sec:preliminaries:ctrees} sets these terms into the context of syntactic parsing of natural languages.
    Finally, \cref{sec:predictions} overviews some aspects of machine learning that we use to model a framework for supertagging and parsing.

    \section{Mathematical Notation}\label{sec:preliminaries:math}
    Sets are considered with the usual notation \(\subseteq\), \(\subset\), \(\cup\), \(\cap\) and \(\setminus\) for the inclusive and exclusive subset relation, as well as the operations for union, intersection, and difference, respectively.
    The symbol \(\emptyset\) denotes the empty set and \(\power(A)\) is the power set, i.e., the set containing all subsets, of some set \(A\).
    The sets of integers, naturals (including zero), positive integers, and reals are denoted by $\DZ$, $\DN$, $\DN_+$, and $\DR$, respectively.
    The \deflab{range} of integers between the pair \(n, m \in \DZ\) with \(n \leq m\) is denoted by \([n,m] = \{i \in \DZ \mid n \leq i \leq m\}\).
    For any positive integer \(n\), the notion \([n]\) abbreviates \([1,n]\), and \([0]\) is the empty set \(\emptyset\).

    \paragraph{Relations.}
    Let \(A\) and \(B\) be some sets.
    The cross product \(A \times B\) is the set of all tuples \(\{(a,b) \mid a \in A, b\in B\}\).
    A \deflab{relation}[relation from \(A\) to \(B\)] is a set \(R \subseteq A \times B\).
    In case \(A\) and \(B\) coincide, we call \(R\) an \deflab{endorelation}[endorelation on \(A\)].
    For two elements \(a \in A\) and \(b \in B\), the term \(a \mathrel{R} b\) denotes the Boolean statement \((a,b) \in R\) and \(a \not\mathrel{R} b\) denotes \((a,b) \notin R\).
    A \deflab{total order}[total order on \(A\)] is an endorelation \(\rel\) on \(A\) such that for each \(a, b \in A\)
    \begin{inparaenum}
        \item if \(a = b\) then \(a \rel b\),
        \item otherwise either \(a \rel b\) and \(b \not\rel a\), or \(b \rel a\) and \(a \not\rel b\).
    \end{inparaenum}

    \paragraph{Functions.}
    A \deflab{function}[(total) function from \(A\) to \(B\)] is a relation \(f\) from \(A\) to \(B\) such that for each \(a \in A\), there is exactly one \(b\) with \((a,b) \in f\).
    As usual, we denote the type of such a function by \(A \to B\), a quantification by \(f\colon A \to B\), and the argument-value relation by \(f(a) = b\).
    A \deflab{partial function}[partial function from \(A\) to \(B\)] is a subset \(f \subseteq A \times B\) such that for each \(a \in A\), there is at most one \(b\) with \((a,b) \in f\).
    The type of a partial function is denoted with an upper half arrow, for instance, \(f\colon A \parto B\).
    The \deflab{image of a function}[image of a (total or partial) function \(f\)] is the set of elements in \(B\) that are assigned to any value in \(A\), denoted by \(\im(f) = \{b\in B \mid (A \times \{b\}) \cap f \neq \emptyset\}\).
    The \deflab{domain of a function}[domain of a partial function \(f\)] is the set of elements in \(A\) with an assigned value in \(B\), denoted by \(\dom(f) = \{a\in A \mid (\{a\} \times B) \cap f \neq \emptyset\}\).
    In some contexts, a partial function \(f\) is handled in the same manner as a total function from \(\dom(f)\) to \(B\) without further notice.

    \paragraph{Families.}
    An \deflab{indexed family}[\(A\)-indexed family over \(B\)] is a function \(f\colon A \to B\), often  denoted in a builder notation of the form \(\big(\mathrm{term}(a) \mid a \in A\big)\).
    The left-hand side \(\mathrm{term}(a)\) may assume any term, equation and/or identifier that depends on the index variable \(a\) to express an element of \(B\), and the right-hand side expresses the domain of the index variable \(a\).
    There may be multiple index variables. In that case, \(A\) is a cross-product of the index variables' domains.
    For instance, if there are two indices \(x\) and \(y\) in the domains \(X\) and \(Y\), then we often denote them individually in the builder notation as follows: \(\big( f(x,y) \mid x \in X, y \in Y \big)\).
    As an alternative to the builder notation, we also denote the argument-value relation in a form similar to the usual function notation, for instance, an \(A\)-indexed family \(f\) is defined such that \(f_a = \mathrm{term}(a)\) for each \(a \in A\), where \(\mathrm{term}(a)\) is some term that contains the variable \(a\).
    Usually, we identify a family by some variable name and access its elements using this identifier and the index variables in its subscript or function notation.
    If the index set is defined in the builder notation or clear from the context, the term \(A\)-indexed is dropped, and we just call such an object a \defabrv*{indexed family}{family}.

    Consider a set \(\varTheta\) and a \(\varTheta\)-indexed family of functions \((f_\theta\colon A \to B\mid \theta \in \varTheta)\).
    We call such a family a \deflab{parametrized function}[$\varTheta$-parametrized function]; it is denoted in the usual function notation \(f_\theta\colon A \to B\).
    For example, a polynomial of degree two is a \(\DN^3\)-parametrized function \(g_{(a,b,c)}\colon \DR \to \DR\) with \(g_{(a,b,c)}(x) = a\cdot x^2 + b\cdot x + c\).

    Families are used implicitly in terms where an associative and commutative binary operation is lifted to a collection of elements.
    For example, the union of \(k\) sets \(\big(U_i \mid i \in [k]\big)\) is denoted in the form \(\bigcup_{i \in [k]} U_i\).
    Besides the union (\(\bigcup\)), we will use such operators for the sum (\(\sum\)), product (\(\prod\)), minimum (\(\min\)) and maximum (\(\max\)).
    If the values and index set of a family coincide in such a notation, then the index is omitted completely.
    For example, we write \(\min [k]\) instead of \(\min_{i \in [k]} i\).
    The minima and maxima are determined according to the usual numerical order.
    In case we encounter different objects or we use any other total ordering \(\unlhd\), we denote it as a superscript, for instance, \(\min^{\unlhd}\).
    The symbol \(\argmin\) denotes the set of all indices with minimal value, for instance, \(\argmin_{i \in [k]} v_i\) is the subset \(M \subseteq [k]\) such that each pair \((\hat{i}, j) \in M \times [k]\) satisfies \(v_{\hat{i}} \leq v_j\).
    Vice versa, \(\argmax\) yields the indices for maximal values.

    An \deflab{partition}[\(A\)-indexed partition of \(B\)] is an \(A\)-indexed family \(p\) over subsets in \(\power(B)\) such that:
    \begin{inparaenum}
        \item For each pair \(a, a' \in A\) of distinct elements \(a \neq a'\), the assigned subsets are disjoint: \(p(a) \cap p(a') = \emptyset\).
        \item The union of all subsets \(\bigcup_{a \in A} p(a)\) is the set \(B\).
    \end{inparaenum}

    \paragraph{Sequences.}
    An \deflab{integer index set} \(I\) is a subset of \(\DN_+\) such that
    \begin{inparaenum}[]
        \item either \(1 \in I\) or \(I = \emptyset\), and
        \item for each \(i \in \DN_+ \setminus \{1\}\), if \(i \in I\) then \(i-1 \in I\).
    \end{inparaenum}
    Let \(I\) denote such an integer index set.
    A \deflab{sequence}[sequence over \(A\)] is an \(I\)-indexed family over \(A\).
    If there is a \(k\in \DN_+\) such that \(I = [k]\), then we call it either a \emph{finite sequence over \(A\)} or, more specifically, a \emph{sequence over \(A\) of length \(k\)}, otherwise an \emph{infinite sequence}.
    \(A^k\) denotes all sequences over \(A\) of length \(k\).
    When a collection of \(k\) elements in \(A\) is quantified, they are usually enumerated with an ellipsis and separated by commas; this notation of the form \(a_1, \ldots, a_k \in A\) defines the family \((a_i \in A \mid i \in [k])\).
    With such a collection of fixed elements \(a_1, \ldots, a_k \in A\), we sometimes emphasize its meaning as a sequence by denoting the elements in the form of an ellipsis without commas \(a_1 \ldots a_k\) (when there is a clear enumeration of items, like the numerical index of the variables \(a_1, \ldots, a_k\)).
    An occurrence of the single element \(a_1\) may also denote the singleton sequence (of length 1).
    The \emph{empty sequence} (of length 0) is denoted by \(\varepsilon\).
    The \emph{set of all finite sequences over \(A\)} is \(A^* = \bigcup_{k \in \DN} A^k\).
    \(A^+\) denotes the set of all non-empty sequences \(\bigcup_{k \in \DN_+} A^k\).
    The \emph{length} of a sequence \(w \in A^*\) is denoted by \(|w|\).
    For any pair of sequences \(u,v \in A^*\), the \deflab{concatenation} \(u \cdot v\) is the sequence \(w\in A^{|u|+|v|}\) such that \(w_i=u_i\) for each \(i \in [|u|]\) and \(w_{|u|+j} = v_j\) for each \(j \in [|v|]\).
    The concatenation is abbreviated by dropping the dot, and it takes advantage of its associativity to drop parenthesis in terms, so that e.g. \(w_1 w_2 w_3 w_4\) denotes \(((w_1 \cdot w_2) \cdot w_3) \cdot w_4\).
    The concatenation of a whole collection of sequences \((w_i \in A^* \mid i \in [k])\) in order of their indices is denoted with an ellipsis \(w_1 \ldots w_k\).  

    A \deflab{subsequence} \(v \in A^*\) in \(w\in A^*\) is a sequence of elements occurring consecutively within \(w\).
    A non-empty subsequence \(v\) is located within \(w\) by a pair of indices for its start \(s \in [|w|]\) and its end \(e \in [s,|w|]\) such that \(v_i = w_{s-1+i}\) for each \(i \in [e-s+1]\); that is, \(v_1 \ldots v_{e-s+1} = w_s \ldots w_e\).
    Whenever such a non-empty subsequence in \(w\) is identified using its location between the indices \(s\) and \(e\), it is denoted by \(w_{s:e}\); we assume the empty subsequence, if this notation is used with any \(s < 1\) or \(e < s\).
    If one of the two indices is left empty, it denotes a \deflab{prefix} or \deflab{postfix}[postfix in \(w\)], respectively.
    For instance, \(w_{:s}\) is the prefix in \(w\) containing the first \(s\) elements, and \(w_{s:}\) denotes the postfix in \(w\) that contains the trailing \(|w|-s+1\) elements.
    In the case of \(w\in A^+\) and for each \(e \in [|w|]\), the subsequence \(w_{:e-1}\) is called \deflab<prefix>{proper prefix}[proper prefix in \(w\)]; analogously, for each \(s\in [|w|]\), the subsequence \(w_{s+1:}\) is a \deflab<postfix>{proper postfix}[proper postfix in \(w\)].

    Let \(\unlhd\) be a total order on \(A\), the \deflab{lexicographic order}[lexicographic order on \(A^*\)], denoted by \(\unlhd^*\), is defined such that
    \begin{inparaenum}
        \item \(w \unlhd^* w \cdot v\) for each \(w, v \in A^*\), and
        \item \(w\cdot a\cdot u \rel^* w\cdot b\cdot v \) for each \(a,b \in A\) with \(a \rel b\) and each \(w,u,v \in A^*\).
    \end{inparaenum}
    For example, \(\leq^*\) is the lexicographic order over sequences of numerical values where \(\varepsilon \leq^* 1\:1\:2\) and \(1\:1\:2\leq^*1\:2\).
    
    We extend the operators \(\argmax\) (and \(\argmin\)) to determine ordered sequences of \(k\) maximal (or minimal, respectively) elements, for each \(k \in \DN_+\).
    In this case, we denote the length \(k\) of these sequences in parentheses as a superscript of the operator.
    In the case that there is also a specified (non-numeric) total order, we denote both as superscripts in the form \(\argmax^{(k), \unlhd}\).
    For instance, the expression \(\argmin^{(5)}_{i \in [n]} p_i\) is the set of all sequences over \([n]\) of length \(m = \min \{5, n\}\) where each is of the form \(i_1 i_2 \ldots i_m\) and determined such that \(p_{i_1} \leq p_{i_2} \leq \ldots \leq p_{i_m}\) and \(p_{i_m} \leq p_j\) for each \(j \in [n]\setminus\{i_1,\ldots,i_m\}\).

    Consider a finite set of integers \(I \subset \DZ\).
    We call the set \(I\) \deflab<contiguous>{contiguous set}[contiguous] if there is a least element \(\mathit{min} = \min I\) and a greatest element \(\mathit{max} = \max I\) such that \(I = [\mathit{min}, \mathit{max}]\) includes exactly the integers in the interval between them.
    A \deflab<contiguous>{contiguous sequence} is a non-empty sequence over \(\DZ\) of some length \(n \in \DN_+\) of the form \(i (i+1) (i+2) \ldots (i+n)\); that is, the leftmost element \(i\) is the least, and the rightmost element \(i+n\) is the greatest in the sequence, and the elements between them increase by one at each position.

    \section{Strings and Trees}\label{sec:preliminaries:trees}
    \paragraph{Alphabets and Strings.}
    An \deflab{alphabet} is a non-empty and finite set. Its elements are called \deflabs{symbol}.
    Usually, uppercase Greek letters are used to denote alphabets, like \(\varSigma\), \(\varPi\), or \(\varGamma\).
    A \deflab{string} is a finite sequence over a set of symbols \(\varSigma\).
    The set of all strings over \(\varSigma\) is \(\varSigma^*\).
    The notation for sequences is also used for strings.
    Subsequences in a string are called \deflabs{substring}.

    \paragraph{Trees.}
    \deflab{tree}[Trees] are particular (finite) sequences that contain opening and closing parentheses to denote nesting hierarchies.
    For any alphabet \(\varSigma\) and set \(\varGamma\) with \((\varSigma \cup \varGamma) \cap \{(,)\} = \emptyset\), the set of trees \(\T_\varSigma(\varGamma)\) is defined inductively: it is the smallest subset \(T\) of \((\varSigma \cup \varGamma \cup \{ (, )\})^*\) such that
    \begin{inparaenum}
        \item \(\varGamma \subseteq T\) and
        \item for each symbol \(\sigma \in \varSigma\), positive integer \(k \in \DN_+\) and trees \(t_1, \ldots, t_k \in T\), the sequence \(\sigma(t_1 \cdots t_k)\) is in \(T\).\footnote{
            Each symbol \(\sigma \in \varSigma\) in a tree may occur with an arbitrary amount of successors denoted in parentheses after it.
            The same symbol may even occur multiple times in the same tree with varying amounts of successors.
            Such trees are called \emph{unranked trees} in the literature \citep[cf.\@][Sec.\@ 8.2]{Comon08}, as opposed to \emph{ranked trees} where each symbol is considered with a fixed amount of children.
        }
    \end{inparaenum}
    As such, trees are finite structures, but unlike strings, we do not consider empty trees.

    A \deflab{node in a tree}[node in the tree \(t \in \T_\varSigma(\varGamma)\)] is an occurrence of a symbol in \(\varSigma \cup \varGamma\) within \(t\); it is a combination of the symbol \(\sigma\) and the position in the sequence that it occurs at.
    If a node \(\rho\) in \(t\) is accompanied by a sequence of trees denoted in parentheses behind the symbol, these trees are called the \deflab{child trees/nodes}[children] or \deflab{successor trees/nodes}[successors] of \(\rho\); analogously their roots are called child/successor nodes of \(\rho\).
    Vice versa, from the viewpoint of a child node, the node before the wrapping parenthesis is called the \deflab{parent node}[parent].
    Each other node within the same set of parentheses and at the same level of nesting is called a \deflab{sibling}.
    The leftmost node is called the \deflab{root node}[root] of the tree; each node without children is called a \deflab{leaf node}[leaf] of the tree; and each node with at least one child is called an \deflab{inner node}.
    In case \(\varGamma \subseteq \varSigma\), the symbols at the leaves are not distinguished from those occurring at inner nodes; then the set \(\T_\varSigma(\varGamma)\) is abbreviated by \(\T_\varSigma\).
    The sequence of all symbols at leaves in a tree \(t\) from left to right is called the \deflab{yield}[yield of \(t\)] and denoted by \(\yield(t)\).
    The \deflab<rank>{rank of a node} is the number of its children, \(\rank(t)\) denotes the rank of the root in \(t\).
    Each leaf is of rank zero; nodes of rank one are called \deflab<unary>{unary node}[unary], and those of rank two are called \deflab<binary>{binary node}[binary].
    A tree is called \deflab<binary>{binary tree}[binary] if each inner node has exactly two successors.

    Each illustration of a tree usually starts with the root node at the top. Below are its children connected by edges; this scheme recurs until the leaves are at the bottom.
    \Cref{fig:pre:ctree} shows an example.

    \paragraph{Tree Positions.}
    A \deflab[position]{position in a tree}[position in a tree \(t \in \T_\varSigma(\varGamma)\)] is a sequence $w \in \DN^*$ that identifies a node within \(t\).
    Starting with the root, each integer in \(w\) indicates the index of a child that acts as the reference for the remaining integers in \(w\).
    Hence, the set of positions in \(t\), denoted by \(\pos(t)\), is defined recursively over its structure:
    \begin{inparaenum}
        \item if \(t \in \varGamma\), then it is \(\pos(t) = \{\varepsilon\}\), and
        \item if \(t = \sigma(t_1 \cdots t_k)\) for some \(\sigma\in \varSigma\) and \(t_1, \ldots, t_k \in \T_\varSigma(\varGamma)\), then it is \(\pos(t) = \{\varepsilon\} \cup \bigcup_{i\in[k]} \{i\}\cdot \pos(t_i)\).
    \end{inparaenum}
    Let \(\rho \in \pos(t)\) be some position.
    Each subsequence in \(t\) that is also a tree in \(\T_\varSigma(\varGamma)\) is called a \deflab{subtree}[subtree in \(t\)].
    The subtree at a position \(\rho\) is denoted by \(t|_\rho\).
    In case that \(\rho\) is the position of a leaf (i.e.\@ \(t_\rho \in \varGamma\)), we call \(\rho\) a leaf position; otherwise it is an inner node position.
    The set of all inner node positions in \(t\) is denoted by \(\npos(t)\), and the set of leaf positions is \(\lpos(t)\).
    The \deflab{ancestor}[ancestors of \(\rho\)] are all proper prefixes of the sequence \(\rho\), denoted by \(\ancestors(\rho)\).
    If \(\rho\) is of the form \(\rho = p_1 \ldots p_\ell\) for some \(\ell \in \DN_+\) and \(p_1, \ldots, p_\ell \in \DN_+\), then \(\ancestors(\rho) = \{ p_1 \ldots p_i \mid 0 \leq i \leq \ell-1 \}\).
    The \deflab{descendant}[descendants of \(\rho\) in \(t\)] are all positions in the subtree below \(\rho\), denoted by \(\descendants_t(\rho) = \{ \rho \cdot \rho' \in \pos(t) \mid \rho' \in \pos(t|_\rho) \}\).
    The symbol at a position \(\rho\) in \(t\) is denoted  \(t(\rho)\).
    We use the same terminology for position as for nodes, for instance, the terms child position, parent position, etc.\@ are used interchangeably in the same manner; \(\children_t(\rho) = \{\rho\} \cdot \DN_+ \cap \pos(t)\) denotes the set of child positions below each position \(\rho \in \pos(t)\), and \(\parent(\rho) = \rho_{:|\rho|-1}\) the parent position for each \(\rho \in {\DN_+}^+\).
    Two positions \(\rho, \rho' \in \pos(t)\) are called \deflab{independent tree positions}[independent], if \(\rho\) is neither a descendant nor an ancestor of \(\rho'\).

    \begin{lemma}\label{lem:firstviasecond}
        Let \(\xi \in \T_{\varSigma}(\varGamma)\) be a binary tree over some alphabet \(\varSigma\) and a set \(\varGamma\).
        And let \((f_\rho \in \lpos(\xi) \mid \rho \in \npos(\xi))\) denote a family of leaf positions such that, for each inner node position \(\rho \in \npos(\xi)\), the position \(f_\rho\) is the leftmost leaf in the right subtree below \(\rho\).
        Then
        \begin{inparaenum}
            \item \(f_\rho\) is well-defined for each \(\rho \in \npos(\xi)\) and
            \item for each pair \(\rho, \rho' \in \npos(\xi)\), \(\rho \neq \rho' \iff f_\rho \neq f_{\rho'}\).
        \end{inparaenum}
    \end{lemma}

%    \begin{proof}
%        The element \(f_\rho\) is obtained from \(\rho\) by appending a distinct postfix \(\omega \in {2}\cdot{1}^*\).
%        \begin{enumerate}
%            \item
%                As \(\rho\) is an inner node position in the binary (and finite) tree \(\xi\), there must be such a leaf position \(\rho \cdot w \in \lpos(\xi)\).
%            \item
%                The direction \(\rho \neq \rho' \Leftarrow f_\rho \neq f_{\rho'}\) follows from (i).
%                By the form of the postfixes \(\omega\) in \(f_\rho\) and \(\omega'\) in \(f_{\rho'}\) as above, they are unambiguously distinguishable from their prefix \(\rho\) and \(\rho'\).
%                Now, we have \(\rho \cdot \omega \neq \rho' \cdot \omega'\) and two cases:
%                \begin{compactitem}
%                    \item If \(\omega = \omega'\), then clearly follows \(\rho \cdot \omega \neq \rho' \cdot \omega' \Rightarrow \rho \neq \rho'\).
%                    \item If \(\omega \neq \omega'\), then, since \(f\) is well-defined, also \(\rho\) and \(\rho'\) must be unequal. \qedhere
%                \end{compactitem}
%        \end{enumerate}
%    \end{proof}

    \paragraph{Indexed Trees.}
    An \deflab{indexed tree}[indexed tree over \(\varSigma\)] is a tree \(t \in \T_{\varSigma}(\DN_+)\) such that \begin{inparaenum}
        % \item the value \(t(\rho)\) at each leaf \(\rho\in\lpos(t)\) occurs at most once in \(t\), and
        \item the value at each leaf occurs exactly once in \(t\), and
        \item for each inner node \(\rho \in \npos(t)\), the children of \(\rho\) are ordered according to the least value occurring at a leaf below them.
    \end{inparaenum}
    That is, in each indexed tree, the symbols in \(\varSigma\) occur exclusively at inner nodes and the values in \(\DN_+\) occurring at leaves are unique.
    The notation for positions, indexing, and substitution are inherited from trees; the yield of an indexed tree is its set of leaves instead of a string.
    The right half of \cref{fig:pre:ctree} shows such an indexed tree.
    The set of all indexed trees over \(\varSigma\) is denoted by \(\itrees_\varSigma\).

    \paragraph{Substitution.}
    Let \(X\) be a finite set of variable symbols, \(\varSigma\) some alphabet that is disjoint from \(X\), and \(c\) a string in \((X \cup \varSigma)^*\).
    A (first-order) \deflab<substitution>{first-order substitution}[substitution of \(X\) in \(c\)] is defined by an \(X\)-indexed family \((v_x \in \varSigma^* \mid x \in X)\), denoted by \(c[v]\), and yields the string in \(\varSigma^*\) obtained from \(c\) by replacing the occurrence of each variable symbol \(x \in X\) by \(v_x\).
    To avoid explicitly defining a family for each substitution, we often denote the substitution by enumerating the value \(v_x\) for each variable \(x\) occurring in \(c\) in the form \(c[x_1=v_{x_1}, \ldots, x_{k}=v_{x_k}]\) where \(k \in \DN\) and \(x_1, \ldots, x_k \in X\) are the variables occurring in \(c\) with \(x_i \neq x_j\) for each pair \(i,j\in [k]\) if \(i\neq j\).

    The concept of \deflab<substitution>{second-order substitution} introduces a second layer by parameterizing the values before inserting them.
    For the scope of this thesis, the number of parameters for each value is limited to exactly one to keep the definition simple.
    Let's consider a single distinct variable \(y \notin X\), a set \(X(\varSigma^*) = \{ x(w) \mid x \in X, w \in \varSigma^* \}\) and a string \(c\) in \((X(\varSigma^*) \cup \varSigma)^*\); we call each element in \(X(\varSigma^*)\) a \deflab<substitution>{substitution site}.
    A second-order substitution of \(X\) in \(c\) defined by an \(X\)-indexed family \((v_x \in (\varSigma \cup \{y\})^* \mid x \in X)\) yields the string in \(\varSigma^*\) obtained from \(c\) by replacing each substitution site \(x(w) \in X(\varSigma^*)\) by \(v_x[y=w]\).
    We specifically stress that the variable \(y\) may be an element in \(\varSigma\), and the result \(c[v]\) might also contain it.

    Because trees are just specific strings, both definitions of substitution are easily lifted to trees and sequences of trees as long as the variables do not coincide with parentheses.
    Let \(\varGamma\) be an alphabet distinct from \(X\), and \(\{ (, ) \} \cap (X \cup \{y\}) = \emptyset\), and \(y \notin \varSigma\).
    Now, the variable occurrences are of the form \(X(\T_\varSigma(\varGamma)^*) = \{x(t_1, \ldots, t_k) \mid x \in X, k \in \DN, t_1, \ldots, t_k \in \T_{\varSigma}(\varGamma)\}\) where each element has a root in \(X\); in the following the set is abbreviated by \(\hat{X}\).
    A second-order substitution of \(X\) in \(c \in \T_\varSigma(\varGamma \cup \hat{X})^*\) defined by an \(X\)-indexed family \((v_x \in \T_\varSigma(\varGamma \cup \{y\})^* \mid x \in X)\) yields a sequence of trees in \(\T_\varSigma(\varGamma)^*\) obtained from \(c\) by replacing each occurrence of \(x(v) \in \hat{X}\) by \(v_x[y=v]\).
    The variable \(y\) may be an element of \(\varGamma\).

    \begin{example}
        Consider an alphabet \(\Sigma = \{\text{a}, \ldots, \text{g}\}\) of (terminal) symbols and two sets of variables \(\X = \{\x_1, \x_2, \x_3\}\) and \(\Y = \{\y\}\).
        In first-order substitutions, all occurring variables in a string are replaced by argument strings, for instance as follows:
        \begin{align*}
            \x_1 \, \text{b} \, \x_2 \, \text{f} \, \x_3\,[\x_1=\varepsilon, \x_2=\text{cde}, \x_3=\text{g}] &= \text{bcdefg} \\
            \x_1 \, \text{bb} \, \x_1 \, [\x_1=\text{a}] &= \text{abba}
        \end{align*}
        In second-order substitutions, there is an argument given in parentheses behind each variable \(x\) in \(\X\) that replaces the variable \(\y\) in its argument, such as the following examples:
        \begin{align*}
            \text{a} \, \x_1(\text{cc}) \, \text{a} \, [\x_1=\text{b}\,\y\,\text{b}] = \text{a} \, (\text{b}\,\y\,\text{b})[\y=\text{cc}] \, \text{a} &= \text{abccba} \\
            \x_1(\text{bb}) \, \x_1(\varepsilon) \, [\x_1=\text{a}\,\y\,\text{a}] = (\text{a}\,\y\,\text{a})[\y=\text{bb}] (\text{a}\,\y\,\text{a})[\y=\varepsilon] &= \text{abbaaa}
        \end{align*}

        The following case considers trees over \(\Sigma=\{\cn{vp}\}\) and \(\Gamma=\{\tn{was}, \tn{carried}, \y\}\):
        \begin{align*}
            \cn{vp}( \x_1(\y) \, \tn{was} )[\x_1=\cn{vp}(\tn{carried}\,\y)]
                &= \cn{vp}( \cn{vp}(\tn{carried}\,\y)[\y=\y] \, \tn{was} ) \\
                &= \cn{vp}(\cn{vp}(\tn{carried}\,\y)\,\tn{was})
        \end{align*}
    \end{example}

    \section{Modeling Natural Languages using Formal Methods}\label{sec:preliminaries:ctrees}
    In the syntactic analysis of sentences in natural languages, we deal with \emph{constituent trees}.
    Each constituent tree illustrates a hierarchy of sub-phrases in a sentence, where a category symbol labels each element in the hierarchy, called \deflab{constituent symbol}.
    These sub-phrases do not necessarily form contiguous regions in the sentence:
    If such a tree contains a node with a non-contiguous sub-phrase, we call it \deflab{discontinuous}; otherwise, it is \deflab{continuous}.
    For instance, \cref{fig:pre:ctree} illustrates a discontinuous constituent tree.
    The symbols at inner nodes, like \(\cn{s}\) and \(\cn{vp}\), are constituent symbols.
    The lower node labeled by \(\cn{vp}\) governs a noncontiguous region in the sentence: \emph{where} and \emph{carried out}.

    \begin{figure}
        \null\hfill
        \subfile{figures/example-constituents-with-annotated-components.tex}
        \hfill\null

        \caption{\label{fig:pre:ctree}
            Discontinuous constituent tree for the phrase \emph{where the survey was carried out}.
            The tree is illustrated in two slightly different ways:
                The left one focuses on the discontinuous sentence structure by recreating the token order with crossing branches in the constituent structure.
                The right one corresponds to the definition of constituent trees in this section and separates the sequence of tokens in the sentence and the \abrv{pos} symbols from the constituent structure using sentence positions as leaves in the constituent structure.
            In both illustrations, the path to its lexical head is double-struck for each node in the constituent structure.
        }
    \end{figure}

    The first layer of nodes in a constituent tree above phrase positions contains distinct symbols called \deflabs{part-of-speech tag} \defabrv{part-of-speech tag}{\abrv{pos} tag}.
    They are mutually exclusive to the constituent symbols found in upper layers; that is, each symbol occurring at the first layer must be a \abrv{pos} tag and no other constituent symbol, and each symbol above the first layer must not be a \abrv{pos} tag.
    They indicate the most primitive categorization of individual tokens in the sentence.
    In our example tree, the symbols \(\cn{wrb}\) and \(\cn{pt}\) are \abrv{pos} tags.

    Now, let us formalize the above.
    We assume a set \(\varSigma\) of words, punctuation marks, parentheses, etc.\@ that are used to form sentences; we call the elements in \(\varSigma\) \deflabs{token}.
    Additionally, there are two alphabets: The set \(\varPi\) of \abrv{pos} tags and the set \(\varGamma\) of constituent symbols.
    \(\varSigma^*\) denotes the set of all strings over these tokens, a superset that contains all grammatical sentences (and also everything else that consists of the tokens in \(\varSigma\)).

    A \deflab{constituent tree} is a triple \((t, p, w)\) where
    \begin{compactitem}
        \item \(w\) is a string of tokens in \(\varSigma^*\),
        \item \(t\) is an indexed tree over \(\varGamma\) such that the set of values occurring at the leaves is \([|w|]\), and
        \item \(p\) is a string of \abrv{pos} tags in \(\varPi^{|w|}\).
    \end{compactitem}
    The component \(t\) is called \deflab{constituent structure}.
    Its leaves indicate an alignment to the positions in the string \(w\) and enable us to denote discontinuous constituents, as shown in \cref{fig:pre:ctree}.
    The set of all such constituent trees is denoted by \(\Xi_{\varGamma, \varPi, \varSigma}\).

    For each inner node at position \(\rho \in \npos(t)\), we consider exactly one position in \(\yield(t|_\rho)\) as most significant for the constituent symbol \(t(\rho)\), this position is called the \deflab{lexical head}[lexical head of \(t|_\rho\)].
    The \deflab{lexical head assignment}[lexical head assignment for a constituent tree $(t,p,w)$] is a function \(h\colon \pos(t) \to \DN_+\) that assigns each inner node position \(\rho \in \npos(t)\) to the lexical head of \(t|_\rho\), and each leaf position \(\rho \in \lpos(t)\) to the leaf \(t(\rho)\).
    For each inner node position \(\rho \in \npos(t)\), the value \(h(\rho)\) must be assigned to a child of \(\rho\) as well, that is, \[
        h(\rho) \in \{ h(\rho') \mid \rho' \in \children_t(\rho) \} \text{.}
    \]
    As the lexical heads have a distinct linguistic connotation, we view the lexical head assignment as a fixed function for each constituent tree \((t,p,w)\); the identifier \(\head_{(t,p,w)}\) denotes the lexical head assignment for \((t,p,w)\).
    When the lexical head assignment is considered in an illustration of a constituent tree, there is exactly one double-struck path exiting each inner node of the constituent structure.
    The assigned lexical head is found by following the double-struck edges down to a leaf.
    For instance, \cref{fig:pre:ctree} shows such an illustration and the lexical head of its root is \(4\) (representing the fourth word ``was'' in $w$).
    Let us assume a binary constituent structure \(t\) and an inner node position \(\rho \in \npos(t)\).
    If the leaf \(\head_{(t,p,w)}(\rho)\) is located at or below position \(\rho \cdot 1\), then the subtree at \(\rho \cdot 2\) is called the \deflab{dependent}[dependent of $\rho$]; vice versa, the subtree at \(\rho \cdot 1\) is called dependent of $\rho$ if \(\head_{(t,p,w)}(\rho)\) is at or below \(\rho \cdot 2\).

    A \deflab{treebank} is a finite set of constituent trees.
    They are assembled by linguists for texts collected from news articles, written speeches, etc., and annotated with their constituent structure.
    In the scope of this thesis, treebanks form data sets used for learning models that can predict constituent structures for input sentences.
    The process of predicting a constituent structure for a given input sentence is called \deflab{parsing}, and a used model is called \emph{parser}.
    In \emph{\(n\)-best parsing}, a sequence of (at most) \(n \in \DN_+\) constituent trees is predicted and each is complemented with a confidence value.
    \deflab{reranking}[Reranking] is a process that uses such a collection of potential constituent trees for a sentence and produces new confidence values.
    It is used as a refinement for a set of candidate constituent trees and often involves more ambitious or orthogonal methods to the underlying \(n\)-best parser.

    Besides constituent trees, there are also \deflabs{dependency tree} that express a syntactic analysis of natural language.
    They present grammatical relations between phrases within a sentence, such as arguments, adjuncts, complements, and sometimes even more fine-grained ones, like distinctions between subjects and objects.
    These relations are illustrated in a tree where each phrase is represented by its lexical head.
    There are similar phenomena as (dis-)continuity in dependency trees. They are referred to as \emph{(un-)projectivity}.
    We will not engage in dependency trees in this work. Some references are discussed in \cref{sec:literature:beyond}.

    \subsection{Assessing the Prediction of Constituent Structures}
    When assessing the quality of a parser, we measure two essential dimensions:
    \begin{inparaenum}
        \item the time needed for the prediction process, and
        \item the accuracy of predicted structures.
    \end{inparaenum}
    The process for measuring the first dimension is rather intuitive:
        The time is taken for the prediction of a constituent structure for each sentence in the test set.
    This time is reported either as the sum for the whole test set or as quotient \(\frac{\text{number of sentences}}{\text{parsing time}}\) for the number of parsed sentences per second.

    The labeled \deflab{f1-score}\footnote{
        The reference implementation for the computation of the labeled f1-score is called \abrv{evalb} \citep{Black91}; sometimes the score also called \abrv{evalb}.
    } \citep{Black91,Col97} has been established as the standard for measuring the accuracy of predicted constituent structures.
    This score is computed as follows:
    The sets of predicted and the gold\footnote{
        The term \emph{gold} refers to the data that is supplied in a treebank (cf.\@ \pageref{pg:def:gold}).
        The gold constituent structures and gold \abrv{pos} tags are the ``correct'' ones  that we compare our predictions to.
    } constituent structures are mapped each into a set that contains a tuple \((w, A, \mathit{yd})\) for each constituent structure \(\xi\) with the sequence of tokens \(w\) and each of its inner nodes with label \(A\) and yield \(\mathit{yd}\).
    According to these two sets \(P\) for all predictions and \(G\) for the gold constituent structures, the precision (\(p = \frac{|P \cap G|}{|P|}\)), recall (\(r = \frac{|P \cap G|}{|G|}\)) and f1-score (\(\frac{2\cdot p\cdot r}{p + r}\)) are computed in the usual fashion.
    The \abrv{pos} tags are often part of the prediction but not assessed as part of the f1-score.
    They are rather reported separately as the accuracy of the predicted \abrv{pos} tags compared to the gold ones.
    The Python tool \texttt{disco-dop} supplies an implementation for both the f1-score of constituent structures and the accuracy of \abrv{pos} tags \citep{CraSchBod16}.

    \begin{figure}
        \null\hfill
        \begin{tikzpicture}[baseline=(t1mid.base)]
            \begin{scope}[every node/.style={inner sep=2pt, font=\small}, level distance=3ex, level 3/.style={sibling distance=2em}]
                \node {\cn{sbar}}
                child { node (t1) {\cn{s}}
                    child { node {\cn{vp}}
                        child { node (t1mid) {\cn{vp}}
                            child { node {\cn{wh}} child {
                                    node {1}}}
                            child { node {5}}
                            child { node {\cn{prt}} child {
                                    node {6}}}}
                        child { node {4}}}
                    child { node {\cn{np}}
                        child { node {2}}
                        child { node {3}}}};
            \end{scope}
            \begin{scope}[every node/.style={inner sep=2pt, font=\small}, level distance=3ex]
                \node[right=4.5cm of t1] (t2) {\cn{s}}
                [sibling distance=5em]
                child { node {vp}
                    [sibling distance=1em]
                    child { node (t2mid) {1}}
                    child { node {4}}
                    child { node {5}}
                    child[sibling distance=1.5em] { node {\cn{prt}} child {
                            node {6}}}}
                child { node {\cn{np}}
                    [sibling distance=1em]
                    child { node {2} }
                    child[sibling distance=1.5em] { node {\cn{nn}} child { node {3}}}};
            \end{scope}
            \node[left=2em of t1] {\(\xi_\text{G}\):};
            \node[left=2em of t2] {\(\xi_\text{P}\):};
        \end{tikzpicture}
        \hfill
        \begin{minipage}{.3\linewidth}
            \small
            \begin{align*}
                \mathit{precision} &= \tfrac{4}{5} \\
                \mathit{recall} &= \tfrac{4}{7} &
                \mathit{f1} &= \tfrac{2}{3} \\
            \end{align*}
        \end{minipage}
        \hfill\null

        \begin{minipage}{.35\linewidth}
            \small
            \begin{align*}
                G = \{
                &(w, \cn{sbar}, [6]),
                (w, \cn{s}, [6]), \\
                &(w, \cn{vp}, \{1,4,5,6\}),\\
                &(w, \cn{vp}, \{1,5,6\}), \\
                &(w, \cn{np}, \{2,3\}), \\
                &(w, \cn{wh}, \{1\}), \\ \\
                &(w, \cn{prt}, \{6\}) \}
            \end{align*}
        \end{minipage}
        \hfill
        \begin{minipage}{.3\linewidth}
            \small
            \begin{align*}
                P = \{
                &(w, \cn{s}, [6]), \\
                &(w, \cn{vp}, \{1,4,5,6\}),\\ \\
                &(w, \cn{np}, \{2,3\}),\\ \\
                &(w, \cn{nn}, \{3\}), \\
                &(w, \cn{prt}, \{6\}) \}
            \end{align*}
        \end{minipage}
        \hfill
        \begin{minipage}{.3\linewidth}
            \small
            \begin{align*}
                P \cap G = \{
                &(w, \cn{s}, [6]), \\
                &(w, \cn{vp}, \{1,4,5,6\}),\\ \\
                &(w, \cn{np}, \{2,3\}), \\ \\ \\
                &(w, \cn{prt}, \{6\}) \}
            \end{align*}
        \end{minipage}

        \caption{
            Example for the f1-score computation for a single pair of gold (\(\xi_\text{G}\)) and predicted (\(\xi_\text{P}\)) constituent structures, both for the sequence of tokens \(w = \textit{where the survey was carried out}\).
            The sets of constituent tuples for both trees and the set of common constituents are illustrated below.
            The upper right corner shows the precision, recall, and f1-score values.
        }
    \end{figure}

    \section{Empirical Predictions}\label{sec:predictions}
    Under the broad term \deflab{prediction}[(empirical) prediction], we subsume each process that takes an input of predetermined shape and assigns it to an output (called \emph{label}) based on previously seen data and prior knowledge.
    In the following, we clarify some basic terms used throughout the thesis in conjunction with empirical predictions.
    For more detailed elaborations in this field, we encourage all readers to consult the reference books by \citet{Moh12} and \citet{Dau17}.

    \paragraph{Tasks, Models, and Learning.}
    We start with the broadest concepts in the field of machine learning.
    A \deflab{task} defines a general setting of the prediction, including the sets of possible inputs and labels and an assessment function for the predictions.
    Since the domains of inputs and labels strongly depend on available data, it is also a part of the task.

    A \deflab{model}[(prediction) model] is a specification that defines how each input is processed to predict a label.
    This prediction chooses a label that optimizes a computed \deflab{confidence} value (e.g., a score or probability) for a given input, so we view each model as a function that computes this value for each input and label.
    The model specifies variables (called \deflabs{parameter}) steering the computation of confidence values.
    Let us denote the set of inputs by \(X\), the labels by \(Y\), and the domain of confidence values by \(\mathcal{C}\) with a total order \(\rel\) on \(\mathcal{C}\).
    We view each model as a \(\varTheta\)-parametrized function \(\varphi_\theta\colon X \times Y \to \mathcal{C}\) where \(\theta\) are parameters (or variables) in some associated domain \(\varTheta\).
    In our case, the domain of confidence values are real numbers, and the order \(\rel\) is \(\leq\), such that the confidence is interpreted as a score (i.e., greater is better).
    The prediction for an input \(x\) and a model \(\varphi_\theta\) with parameters \(\theta\) is a value in \(\argmax^{\rel}_{y \in Y} \varphi_\theta(x, y)\).

    Generally, there are two types of models: \deflabs{generative model} and \deflabs{discriminative model}.
    While generative models define a joint probability distribution of inputs and labels, discriminative models define probabilities or scores for labels conditioned on the input.
    Generative models can generate samples of pairs containing in- and outputs.
    Discriminative models, on the other hand, cannot do this in general, as they do not model the input structure.

    \deflab{learning}[Learning] is a process that finds an appropriate parametrization for a model in a task.
    We differentiate between two major types of learning: \deflab{supervised learning}[supervised] and \deflab{unsupervised learning}[unsupervised].
    In the first case, the data contains the inputs and labels; in the latter, it consists only of the input.
    Some intermediate types exist between the two, including self-supervised and \deflab{semi-supervised learning}[semi-supervised], where a large amount of unlabeled data is complemented with a small amount of labeled data.

    \paragraph{Sequence Prediction and Tagging.}
    \emph{Sequence prediction} is a coarse category of tasks where the set of inputs consists of sequences.
    In such cases, a model is a function of the form \(X^* \times Y \to \mathcal{C}\), where \(X\) now denotes the set of individual elements in the input sequences.
    \deflab{tagging}[Tagging], on the other hand, is a category of tasks within sequence prediction where the set of outputs are sequences as well, and the outputs are of the same length as the input.
    Such a model for tagging is a function of the form \(\big( \bigcup_{i \in \DN} X^i \times Y^i \big) \to \mathcal{C}\), where now also \(Y\) is a set of individual elements in the output sequences; these elements of \(Y\) are called \emph{tags}.
    In this thesis, we focus on \deflabs{parallel tagging model}.
    In such models, the confidence values are computed independently for each output position.
    For each parallel tagging model \(\varphi\), each input \(\vec{x} \in X^k\) of length \(k \in \DN_+\), each tag \(y\in Y\) and position \(i \in [k]\), we denote the score of the tag \(y\) at position \(i\) of the output in the form \(\varphi^{(i)}(\vec{x}, y)\).

    \paragraph{Parameters and Training.}
    We distinguish two essential sets of parameters: \deflabs{hyperparameter} and \deflabs{model parameter}.
    Hyperparameters determine details about the model and the training process. %, to some extent, the model and the training objective can also be seen as hyperparameters.
    They are not trained but are chosen for the training either by prior knowledge or by trying different combinations and manually evaluating their results.
    Model parameters are variables defined by a model (cf.\@ the parameters \(\theta\) for the model \(\varphi_{\theta}\) above). They are tuned during the training process to match a training objective.

    \deflab{training}[Training] is the process of tuning the model parameters in terms of a \deflab{training objective}.
    A training objective assesses the assignment of confidence values by a model in a sample of input and output pairs and defines a criterion for optimal parameters.
    For instance, such an objective can take the shape of maximizing a cumulative confidence function \(c\colon \varTheta \times \power(X \times Y) \to \mathcal{C}\) of the form \(c(\theta, T) = \sum_{(x,y) \in T} \varphi_\theta(x, y)\), where \(T \subseteq X \times Y\) is a sample of gold\label{pg:def:gold} input and output pairs; the training process then aims for \(\argmax^{\rel}_{\theta \in \varTheta} c(\theta, T)\).
    In some cases, it is possible to compute the model parameters that optimize the training objective in closed form.
    In other cases, iterative processes change the parameters to improve upon the criterion gradually.
    Despite that the terms learning and training are sometimes used interchangeably, there are some delicate nuances between them.
    While both describe a process that fits a model to available data, training is specifically about tuning model parameters using labeled examples.
    Learning is a little broader, including processes that find appropriate hyperparameters.

    \paragraph{Data and splits.}
    The \deflabs{data set} contain pairs of input and output that we use to deduce a prediction model.
    In this context, the output (i.e., the to-be predicted) component of these instances is referred to as \deflab{gold data}[gold].
    The data instances are used in two roles: for training and for assessing a prediction model.
    The data sets are usually split into three parts to accommodate these roles:
    \begin{inparaenum}
        \item a \deflab{training portion} that is solely used to train a model,
        \item a \deflab{development portion} that is used to monitor the model during the training process and assess the selection of hyperparameters and
        \item a \deflab{test portion} that is used to assess a final model.
    \end{inparaenum}
    Specific partitions are shared for standard data sets used within a community to enable fair comparisons between publications.
    In such publications, the results of different models are compared using the same test portions of the same data sets.

    \begin{example}
        One task that we consider is discontinuous constituent parsing with the \negra{} treebank \citep{Skut98}.
        Consider the alphabets of constituent symbols \(\varGamma\), and \abrv{pos} symbols \(\varPi\) in the \negra{} treebank as well as a set of tokens \(\varSigma\).
        The domain of inputs is the set of sequences over tokens \(\varSigma^*\), and the domain of labels is the set of constituent trees \(\Xi_{\varGamma, \varPi, \varSigma}\).
        There are distinct sections in the \negra{} treebank that act as training, development, and test portions.

        Parsing is a process that predicts a constituent structure for a given sequence of tokens.
        Statistical grammars have proven successful in this setting in the past.
        Such a statistical grammar defines a probability distribution for constituent trees \(p_{\theta}\colon \Xi_{\varGamma, \varPi, \varSigma} \to \DR\), usually by the sum over all derivations per constituent tree and by the product of all rules' weights per derivation.
        This is formalized in the equation \[
        p_{\theta}(\xi) = \sum_{d \in D(\xi)} \prod_{\rho \in \pos(d)} \theta(d(\rho))
        \]
        where \(\xi \in \Xi_{\varGamma, \varPi, \varSigma}\) is a constituent tree, \(D(\xi)\) denotes the set of all derivations in the grammar for the constituent tree \(\xi\), and \(\theta\) is a weight function for the rules in the grammar.
        The function \(\theta\) is defined such that the term \(\prod_{\rho \in \pos(d)} \theta(d(\rho))\) is a probability distribution over all derivation trees \(d\) in the grammar.
        As the sequence of tokens \(w \in \varSigma^*\) is a part of the constituent tree \(\xi\), the function \(p_{\theta}\) is trivially equivalent to a joint probability distribution \(\hat{p}_\theta\colon \Xi_{\varGamma, \varPi, \varSigma} \times \varSigma^* \to \DR\) that assigns \(\hat{p}_\theta(\xi, w) = p_\theta(\xi)\) if \(w\) is the sequence of tokens in \(\xi\) and \(0\) otherwise.
        Such a form of joint probability distribution over input and output tuples is characteristic for a generative model.
        The values assigned by the function \(\theta\) act as the model parameters; they are restricted to form probability distributions.
        One approach at training with probabilistic grammars is \emph{maximum likelihood estimation} defined by the objective \(\argmax_{\theta \in \varTheta} \prod_{\xi \in T} p_{\theta}(\xi)\) where \(T\) denotes the training portion of the treebank, and \(\varTheta\) is a set of viable probability distributions.
        Learning this task is supervised because the training portion \(T\) contains sentences as well as their constituent trees.
    \end{example}

    \subsection{Neural Networks}\label{sec:preliminaries:nn}
    We use \deflabs{artificial neural network} (short: \defabrv*{artificial neural network}{\abrv{ann}}, or just \defabrv*{artificial neural network}{neural network}) to implement the prediction of supertags for each position of an input sentence.
    \abrv{Ann}s emerged in the last decades and proved extremely useful in many tasks, including ones settled in natural language processing, for instance in the field of semantic analysis like sentiment analysis \citep{sentiment} or relation extraction \citep{relationextraction}, in the field of syntax analysis like dependency \citep{dependencyparsing} or constituency parsing \citep{FerGom20a}, or in high-level tasks like question answering \citep{Devlin2019}, summarization \citep{bertgeneration}, or dialogue systems \citep{dialoguesystems}.
    We will only briefly overview the concepts needed to understand the implementations discussed in this thesis; the specific instances used in the experiments are discussed in \cref{sec:models}.
    For further elaborations in neural networks for natural language processing, we recommend the reference by \citet{Gol22}.

    In their most primitive form, we consider each neural network as a \(\varTheta\)-parametrized function of the form \(f_\theta\colon X \to \DR^n\) where \(\varTheta\) is some domain of variables, \(X\) is a set of inputs, and the outputs are real-valued vectors of length \(n \in \DN_+\).
    There is a set of well-studied \abrv{ann}s that are used as \deflabs{module} and combined to form new \abrv{ann}s by composition, addition, or concatenation of their results.
    Each module defines suitable sets of inputs \(X\) as well as a parameter domain \(\varTheta\). 
    A collection of such modules is called an \deflab{architecture}.
    In the following, some common modules are briefly explained.

    \subsubsection{Embeddings}
    Embeddings are used to assign vectors to arbitrary sets of input objects.
    These assigned values are the foundation for computations in other modules, which usually assume real vectors as input.
    The simplest type of embeddings are \deflabs{static embedding}.
    Let us consider a finite set of input objects \(X\) and an output dimension \(n \in \DN_+\).
    A static embedding for \(X\) of dimension \(n\) is a parametrized function \(f_W\colon X \to \DR^n\) that assumes a matrix of parameters \(W = (W_x \in \DR^n \mid x \in X)\) and assigns a vector to each element in \(X\); that is, \(f_W(x) = W_{x}\) for each \(x \in X\).
    In some applications, it is advised to add a common vector for unknown elements, for instance, for all unknown tokens that do not occur in the training data.
    If the input is a sequence, static embeddings are applied element-wise.
    For example, a sequence of tokens \(\sigma_1 \cdots \sigma_n\) is embedded as a sequence of vectors \((f_W(\sigma_i) \mid i \in [n])\).

    Some architectures complement element-wise token embeddings by \deflabs{character-wise embedding} \citep[e.g.\@][]{kiperwasser2016simple,Akb18}.
    These are recurrent modules (see ``Neural Networks for Sequence Processing'' below) that process the sequence of characters and yield a vector of specified length for each individual token in the input sequence.
    This type of embedding aims to improve the handling of unknown words in the following sense:
        If a token does not occur in the training data, it has no specific element-wise embedding.
        However, the character-wise embedding might have learned some reasonable information based on similar tokens in the training data.

    \deflab{positional embedding}[Positional embeddings] offer another extension used to complement static embeddings for sequence processing.
    A position embedding defines a function that maps the position of each sequence element into the output dimension.
    In contrast to both methods above, these are often not trained but chosen immutably.
    For instance in the case of \abrv{bert} (a specific large language model), a series of sine and cosine functions in different frequencies is used as positional embedding.

    \subsubsection{Linear Layers, Activation Functions, and Multi-Layer Perceptrons}
    After embeddings, linear layers are an essential part of \abrv{ann}s.
    They are a linear transformation of vectors from a specified input to an output dimension.
    Formally, let \(m, n \in \DN_+\) be some input dimension and output dimension, respectively.
    Then a linear layer is a function \(f_{(W,b)}\colon \DR^m \to \DR^n\) such that \(f_{(W,b)}(x) = Wx+b\), and the matrix \(W \in \DR^{(m\times n)}\) as well as the bias vector \(b\in \DR^n\) are parameters.

    Activation functions are a general term for non-linear functions used between compositions of modules and on top of linear layers.
    Some options implement desirable properties or interpretations, such as the following:
    \begin{enumerate}
        \item
            A \deflab{rectified linear unit}[rectified linear unit] \defabrv{rectified linear unit}{ReLU} is computationally inexpensive and used in large architectures to save computational power.
            It is defined for each vector \(x\in R^n\) for any dimension \(n \in \DN_+\) by cutting-off negative values: \(\mathrm{relu}(x) = (\max(0, x_i) \mid i \in [n])\).
        \item
            The \emph{sigmoid function} is applied point-wise to each component of a vector and assigns values in the interval between \(0\) and \(1\) as follows: \(\mathrm{sigmoid}(x) = (\frac{1}{1-e^{x_i}} \mid i \in [n])\) for each \(x \in \DR^n\) and \(n \in \DN_+\).
            Each of these values is usually interpreted as one value in a binary probability distribution.
        \item
            The \deflab{softmax}[softmax function] also maps each value in a vector to the interval between \(0\) and \(1\).
            They are normalized such that they sum to \(1\) and are usually interpreted as a probability distribution over the set of vector indices.
            It is defined as follows: \(\mathrm{softmax}(x) = (\frac{e^{x_i}}{z} \mid i \in [n])\) for each \(x\in \DR^n\) and \(n \in \DN_+\) where \(z = \sum_{i \in [n]} e^{x_i}\).
    \end{enumerate}
    The latter two are more dominantly used as the top-most modules because of their interpretation as probabilities.
    A \deflab{multi-layer perceptron} \defabrv{multi-layer perceptron}{\abrv{mlp}} is a composition of multiple linear layers with activation functions between them.


    \subsubsection{Neural Networks for Sequence Processing}\label{sec:preliminaries:nn:recurrent}
    Recurrent modules are a foundation for processing sequences with neural networks.
    They are defined such that each vector in an input sequence is complemented by a previous output, starting with an initialization vector.
    Formally, this concept of recurrence uses some inherent module \(f_\theta\colon \DR^{n+m} \to \DR^m\) parametrized by some \(\theta\) and incrementally computes, for an input sequence \(x_1, \ldots, x_\ell \in \DR^n\), a sequence of outputs \(g_{(f, \theta, h_0)}(x_1, \ldots, x_\ell) = (h_i = f_\theta(x_i \cdot h_{i-1}) \in \DR^m \mid i \in [\ell])\) where \(h_0 \in \DR^m\) is an initial value and parameter of the recurrent module \(g\).
    \deflab{long short-term memory}[Long short-term memory] \defabrv{long short-term memory}{\abrv{lstm}} cells \citep{Hoc97} have established as a quasi-standard for these inherent modules.
    In addition to the previous output \(h_{i-1}\) that is used to compute \(h_i\), they also carry a cell state between the computations for each position, which is not part of the output.
    Bidirectional recurrent modules combine the vectors obtained by two recurrent modules, one for the sequence from left to right and the other in the opposite direction, by concatenating the vectors for each position.

    The \deflab{transformer} architecture \citep{vaswani2017attention} acts as an alternative to recurrent neural networks for sequence processing.
    It relies on the self-attention mechanism to extract and combine sequence positions based on their embedding vectors, often composed of static sub-word and positional embeddings.
    The architecture is particularly well-known as the foundation of large language models.

    \subsubsection{Training}\label{sec:preliminaries:nn:training}
    The parameters in an \abrv{ann} are tuned to fit an objective for labeled examples in a training set using \emph{statistical gradient descent} \defabrv{statistical gradient descent}{\abrv{sgd}}.
%    Such an objective is a function that rates the prediction for each example compared to the ground truth and accumulates the results in the whole set.
%    A prominent example of such an objective is \emph{cross entropy loss}, often used when the label's confidence values are interpreted as a probability distribution.
%    It is the sum of negative logarithms over all confidence values; the term \emph{loss} indicates that the value should be minimized during training.
%    During the training process, the value is computed for a small portion of the data (called \emph{batch}), and the parameters are updated using a partial derivative of the value concerning the weight.
%    These updates are scaled by a (typically small) real value, called \emph{learning rate}, and repeated a fixed number of times or until a particular criterion is met.
%    There are several extensions to the baseline training process that aim to accelerate or stabilize the training process or improve the prediction quality; we use the following:
    Such an objective often takes the form of a \emph{loss} value that the training process aims to minimize within a training sample.
    We consider \deflab{cross entropy loss} (i.e., the negative log-likelihood of probability confidence values) for the prediction of supertags and \abrv{pos} tags for each position in the input sentence.
    There are several common extensions and hyperparameters to the training; among them, we use the following:
    \begin{itemize}
        \item
            \abrv{Sgd} is an iterative process; each iteration is usually performed using a small amount part of the training data, called \deflab{batch}.
            The amount of iterations to process the whole training set is called an \deflab{epoch}. Usually, the total number of iterations for training is given in epochs.
            A small real value scales the computed gradient, called \deflab{learning rate}.
        \item \deflab{learning rate scheduler}[Learning rate schedulers] adapt the learning rate over the course of the iterative training process.
            We use \deflab{AnnealOnPlateau}, which reduces the learning rate by a constant factor \(\lambda \in \DR\) (with \(0\leq\lambda\leq1\)) if the prediction accuracy does not improve within \(p\in\DN_+\) epochs.
            The positive integer \(p\) is called \emph{patience}.
        \item
            The gradients used to update the parameters in each iteration are modified using the \deflab{Adam optimizer}[Adam] algorithm \citep{adam,adamw} that implements momentum and adaptive gradients.
            Both aim to accelerate and stabilize the training process.
        \item
            Some measures aim to improve the ability to generalize from training examples; among them are \deflab{weight decay} and \deflab{dropout} \citep{dropout}.
            Both are quantified in terms of real values:
                Weight decay adds a term to the loss that contains the magnitudes of all parameters.
                The phrase ``weight decay'' is used to refer to a scaling factor for this term.
                It discourages high values for individual parameters, which are generally associated with being tied to specific training instances (called overfitting).
                Dropout is the probability of omitting values (and replacing them by \(0\)) in output vectors of specific modules during individual iterations in the training process.
    \end{itemize}

    \subsubsection{Pretrained Modules}
    Some modules are shared in the community, such as pre-trained static embeddings.
    Usually, they are \emph{generic}; that is, they are designed to be used in many diverse tasks. In that context, such a specific task is called \deflab{downstream}.
    In natural language processing, using pre-trained modules has become a quasi-standard.
    The reason for that is twofold:
    \begin{itemize}
        \item
            Training costs time and power; using pre-trained modules skips a significant part of the training and saves both.
        \item
            These generic modules are often trained using vast corpora, magnitudes larger than most for downstream tasks such as constituent parsing.
            Thus, they contain information that is impossible to obtain from a downstream training set.
    \end{itemize}
    When \emph{large language models} are used as modules in an architecture, the term \deflab{fine-tuning} is used for training downstream tasks.
    Because of the additional information, fine-tuning such a model for a downstream task in an otherwise supervised manner is referred to as \emph{semi-supervised} training.

    The two most prominent instances of pre-trained models that we encountered during the work that led up to this thesis are the following:
    \begin{itemize}
        \item
            Word vector embeddings are pre-trained static word embeddings.
            Syntactic word representations, such as \emph{word2vec} \citep{word2vec} and \emph{fasttext} \citep{fasttext}, are both based on the continuous bag of words model and aim to map words that are used in similar contexts to similar vectors.
            Semantic frameworks, like WordNet \citep{wordnet}, aim to map words with similar meanings to similar vectors using training with large semantic word networks.
        \item
            As the name suggests, \deflabs{large language model} \defabrv{large language model}{\abrv{llm}} are not only single modules but large architectures for sequence processing.
            Such models are usually pre-trained regarding multiple tasks associated with the field of language understanding, for instance, missing word prediction and next sentence prediction in the case of \abrv{bert}.
            The vector assigned to each token in a sequence by an \abrv{llm} is considered context-sensitive:
                It does not only depend on the token itself but on its position and all other tokens in the sequence as well.
                This is achieved using positional embeddings and transformer architectures.
            The breakthrough for \abrv{llm} as a component in \abrv{ann} was the release of \abrv{bert} \citep{Devlin2019}.
            Since then, \abrv{bert} and its derivatives were adopted into models to solve various tasks on language understanding \citep[they experiment with models for question answering, entailment recognition, sentiment analysis, i.a.]{Devlin2019, roberta}, language generation \citep[they experiment with models for summarization, translation, i.a.]{bertgeneration}, information extraction \citep[they experiment with models for named entity recognition, relation extraction]{biobert} and also our field of research: constituent parsing \citep{Cor20, FerGom20a, FerGom22,Coa21, Sun22}.
            It is still used in state-of-the-art models today.
    \end{itemize}

    \ifSubfilesClassLoaded{%
        \printindex
        \bibliography{../references}%
    }{}
\end{document}
