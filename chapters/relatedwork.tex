\documentclass[../document.tex]{subfiles}

\begin{document}
    \chapter{Related and Concurrent Work}\label{sec:literature}
    This chapter gives a prospect around several branches in research that touch on this thesis' topic in a broad sense.
    In \cref{sec:literature:constituency}, we will focus on other approaches for discontinuous consituency parsing.
    It is split into several subsections where each but the last covers a general concept established in the field of parsing; the last contains approaches that do not fit into these traditional parsing concepts.
    \Cref{sec:literature:beyond} will open a window to discuss parsing approaches that do not consider the constituency but other relations worth analyzing in natural language.
    Specifically, it covers dependency parsing as an other aspect in the analyzation of syntax as well as some semantic analyzations.
    Other approaches that rely on the supertagging framework are discussed in an own section, as they were not primarily used in consituency parsing in the past and we want to underline their significance for this thesis.

    \section{Concurrent Work in Discontinuous Constiuent Parsing}\label{sec:literature:constituency}
    This section aims at describing and comparing different strategies to parse sentences into constituent structures.
    We will highlight \emph{data-driven} parsers, i.e.\@ those that use a treebank to extract grammar rules without human intervention \citep[c.f.\@ Section 1.1]{Kal10} in contrast to approaches that utilize \emph{hand-crafted} grammars.
    It categorizes relevant publications in the field and categorizes them into general concepts that we consider as traditional or established for parsing.
    Each category is dedicated a subsection that is shortly summarized in the following.
    
    The first, \cref{sec:literature:grammar}, is dedicated to parsing with underlying garmmars for natural languages that encode constituent structures.
    The parsing procedures in that section do not rely on any discriminative classifiers, they can all be considered as generative models.
    \Cref{sec:literature:chart} discusses approaches that rely on bottom-up or top-down parsing procedures that are common in parsing with grammars, but distinguish between derivations using more liberal weight functions as the usual assigned rule weights.
    All publications in the section use discriminative models to compute these weights, recent ones feature artificial neural networks.
    The parsers in \cref{sec:literature:transition} follow a different concept then the previous two.
    The transition systems discussed in this subsection define operations that are applied in sequence to construct trees.
    The parsing process with such a transition system then translates a sentence into a sequence of operations which is then evaluated into a constituent tree.
    Each referenced publication in the subsection uses a discriminative classifier to implement the translation process.
    Lastly, \cref{sec:literature:others} aggregates publications that do not fit into the previous categories.
    Specifically, we will cover three approaches that gained some traction in the discontinuous parsing community.
    As the previous two subsections, they are all built around discriminative models (artificial neural networks to be precise). 
    
    \subsection{Grammar-based Parsing}\label{sec:literature:grammar}
    Grammar formalisms are closely tied to the concept of constituency analysis in natural language.
    \citet{Cho56} laid the theoretic framework for the task and introduced grammar formalisms as a tool to explain the local relations within the constituent trees.
    Later, the constituency framework was extended to attribute discontinuous constituents which appeared to be necessary to describe certain natural language phenomena. \citep{Shieber85}
    There are several grammar formalisms that are used for parsing discontinuous constiuents.
    They are discussed under the following heading.
    After that, we will focus on three specific strategies for the extraction of extraction of grammars from discontinuous constituent treebanks that achieved remarkable results, each below their own heading.

    \subsubsection*{Grammar Formalisms}
    There is a whole range of grammar formalisms that are able to express discontinuous constituent structures and were also used for parsing in practice.
    \citet{VijWeiJos87,Weir88} formulated the class of \deflab{mildly context-sensitive grammar formalisms} that settles well between the classes of context-free grammars and context-sensitive grammars.
    The formalisms within the class combine the advantages of polynomial parsing complexity (in contrast to context-sensitive grammars) and the necessary expressive power for discontinuities (in contrast to context-free grammars).
    All of the grammar formalisms mentioned in this section are part of the class of mildly context-sensitive grammar formalisms.

    \defabrv{Tree-adjoining grammars}{\abrv{tag}}%
    \emph{Tree-adjoining grammars} (short: \abrv{tag}) \citep{JosLevTak75} are are built around two operations to combine trees:
    \begin{compactitem}
        \item a substitution operation that replaces a distinct leaf with an operand tree (as usual), and
        \item an adjunct operation that expands a distinct inner node with an operand tree such that its children are appended to a designated \emph{foot node} in the operand.
    \end{compactitem}
    Each rule of a \abrv{tag} is a tree where designated nodes indicate if one of the two operations can be applied.
    The adjunct operation contrasts \abrv{tag} from other tree grammar formalisms such as \emph{tree substitution grammars}.
    There are hand-crafted \abrv{tag} for English \citep{xtag01} and Korean \citep{xtag02} published by the \abrv{Xtag} research group \citep{Anoop00}, as well as data-driven procedures that extract grammars from treebanks, e.g.\@ by \citet{xia1999extracting,Bla18}.
    The latter use a heuristic approach to recognize which operation shall be used to introduce each node in the resulting constituent tree.
    Lexicalized \abrv{tag} was the formalism used for the introduction of supertagging. \citep{bangalore1999supertagging}
    The approach was still investigated in recent years for dependency parsing in German and French treebanks by \citet{Kas17,Bla18}.
    However, \abrv{tag} limited in their ability to express certain language phenomena: \citet{Becker91} identified occurrences of \emph{long distance scrambling} in German that \abrv{tag} fail to recognize.
    An extension to the formalism, called \emph{multi-component tree adjoining grammars} \citep{VijWeiJos87,Weir88}, can be used to accomodate them, but has yet to be seen in practice.

    \emph{Linear context-free rewriting systems} (short: \abrv{lcfrs}) were described in detail in \cref{sec:grammar:lcfrs}.
    \citet{VijWeiJos87,Weir88} introduced \abrv{lcfrs} as a unified formalism to capture the expressive power of mildly context-sensitive formalisms.
    As such, there is a range of formalisms whose capabilities are well included in the class of \abrv{lcfrs} languages:
    \begin{compactitem}
        \item \citet{VijWeiJos87} have shown that the string languages defined by the class of \abrv{lcfrs} include those of (mutli-component) \abrv{tag} and head grammars.
        \item \citet{vijay1994equivalence} proved the class also contains the string laguages of combinatory categorial grammars as well as linear indexed grammars.
        \item \citet{SekMatFujKas91} mentioned that multiple context-free grammars are equivalent to string \abrv{lcfrs}.
        \item \citet{boullier1998proposal} showed the equivalence of simple range concatenation grammars (short \abrv{srcg}) and \abrv{lcfrs}.
    \end{compactitem}
    From the latter two, there are two variants for the notation of compositions within the rules of \abrv{lcfrs}, one that matches the notation of multiple context-free grammars and one that is equal to range concatenation grammars.
    \citet{MaierSogaard08,MailKal10} investigated probabilistic \abrv{srcg} and \abrv{lcfrs} as formalisms for data-driven parsing discontinuous constituents.
    They extended well-known approaches for the extraction of context-free gramamrs, namely the extraction of treebank grammars, probabilistic weight assignment via maximum likelihood estimation, and binarization.
    
    \citet{CraSchBod16} introduced \emph{discontinuous tree subsitution grammars} (short: \abrv{dtsg}) as a generalization of tree substituion grammars \citep{} to the discontinuous case.
    The formalism is build around the tree substitution operation that replaces a designated leaf with an operand tree, similar to the substitution operation in \abrv{tag}.
    \citet{Cra11} introduced their parsing model with \abrv{lcfrs} as ``backbone'', suggesting that \abrv{dtsg} resemble \abrv{lcfrs} with latent annotations as it was also pointed out by \citet[Section 8.5.1]{Geb20}.
    Their data-oriented parser is described in more detail below the next heading.

    \defabrv{Combinatory categorial grammars}{\abrv{ccg}}%
    \emph{Combinatory categorial grammars} (short: \abrv{ccg}) were introduced by \citet{Ste11} as extension to categorial grammars (which are in turn equivalent to context-free grammars).
    Both formalisms are easily distinguished from the remainder mentioned in this section by their peculiar derivation relation based on combinatory logic.
    The meat of each categorial grammar is a lexicon that assigns lexical items to terms over constituent symbols and operators; each such term is called a category.
    The structure of each category indicates how it can be combined to other categories.
    They control the derivations within the grammar significantly; since they are assigned to the lexical items, the grammar formalism is inherently considered \emph{lexical}.
    \cite{Hoc07} published \emph{\abrv{ccg} bank}, a transformation of the Penn Treebank into \abrv{ccg} categories.
    Because of the complexity of the lexical categories, supertagging is suggesteted as an essential step to determine the assignment for input sentences \citep{Clark04}.
    Supertagging with \abrv{ccg} lexicons was thouroughly investigated \citep{clark2002supertagging, LewisSteedman14, vaswani2016supertagging, Kad18, StaSte20}.

    \subsubsection*{Data-Oriented Parsing}
    \citet{Bod92} introduced the concept for data-oriented parsing (short: \abrv{dop}) which aims to extract fine-grained but highly ambiguous grammars from constituent treebanks.
    At the heart of the approach they extract a tree substitution grammar rule for each fragment (i.e.\@ coherent set of nodes within a tree) in each constituent tree.
    The number of these fragments is exponential to the size of the treebank, so there were multiple investigations to overcome the computational burden of the model's complexity:
    \begin{itemize}
        \item \citet{Bod01} investigated several restrictions for the extracted rules defined solely on each fragment's properties, such as its depth or the number of lexical leaves.
        \item \citet{Ban10} implement a coarse-to-fine parser. They first use a treebank grammar to compute a coarse parse chart which is then used as a guiding mechanism for parsing with the finer (unrestricted) \abrv{dop} grammar.
        \item \citet{San11} introduced \emph{double-dop}, which restricts the rules to all fragments that appear at least twice in the treebank.
    \end{itemize}
    \citet{San11} did also investigate other parsing objectives, such as an approximation of the best weighted constituent tree instead of the best weighted derivation.
    \citet{Cra11,CraSchBod16} generalize these concepts to parsing discontinuous constituents.
    They achieved best results with their discontinuous double-dop model, which they implemented as a coarse-to-fine parser in multiple levels.

    \subsubsection*{Grammars with Latent Annotations}
    \citet{Mat05} introduced context-free grammars with latent annotations (short: \abrv{cfg-la}) by splitting nonterminals into two parts: an observable symbol (which takes the form of a constituent or \abrv{pos} symbol as in usual treebank grammars) and an unobservable \emph{latent annotation symbol}.
    They defined an EM algorithm to find a weight assignment that maximizes the likelihood of a treebank with respect to a grammar with a fixed set of latent annotation symbols.
    As these grammars are inherently ambiguous, the best weighted derivation may not coincide with the best weighted constituent tree; finding the latter is known to be intractable in practice \citep[they conclude that finding the best weighted constituent tree is \emph{NP-hard} in Section 3]{Mat05}.
    They give approximate solutions for the objective that work well enough, e.g.\@ by considering a fixed amount of \(n\)-best derivations.
    \citet{Petrov06} enhance the approach with an iterative extraction algorithm that automatically introduces new latent annoation symbols; in each iteration some of the symbols are \emph{split} by adding new symbols and copying all rules replacing the split nonterminal symbol with the new latent annotation.
    \citet{Geb20} extends the approach to the discontinuous case with hybrid grammars as underlying formalism.

    \subsubsection*{Pseudocontinuous Parsing}
    Some authors decided to avoid discontinuous structures and their expensive parsing procedures completely.
    In the area of pseudocontinuous parsing, the discontinuous treebanks are transormed into continuous ones with the aim to extract good-enough context-free grammars for constituent parsing.
    \citet{DubKel03} derive a continuous variant of the NeGra treebank by shifting subtrees of discontinuous nodes into their antecedents; this transformation is called \emph{node raising} and unfortunately it is not reversable in a straight-forward manner.
    \citet{levy-manning-2004-deep} contributed an algorithm that heuristically identifies the shifted nodes and is able to recover a discontinuous constituent tree.
    However they conjecture that this approach is not suited in processing languages with relatively free word order such as German, as the context-free transformation seemed a bad approximation in their experiments.
    \citet{boyd-2007-discontinuity} proposes an alternative transformation that splits each discontinuous node into continuous siblings.
    The process is reversed by identifying the split nodes (which are complemented by a special marker symbol) in the resulting derivations.
    In their experiments, they find that their reversion procedure is indeed unambiguous in their considered case of the TiGer treebank (and in contrast to the previous approach).
    Both approaches were investigated by \citet{hsu2010comparing}.
    They concluded that node raising is indeed preferrable as it maintains more of the constituent trees' structure.
    \citet{Ver16} was able to improve upon the node raising strategy by introducing unary nodes that identify the moved subtrees in tandem with markers that indicate their current location with respect to their origin.
    They implemented a parsers using the \abrv{pcfg-la} framework by \citet{Petrov06} and could achieve competetive results in their experiments with the TiGer treebank.
    
    % \subsection{Reranking}
    % The efficiency of parsing with grammar formalisms comes mostly from their locality assumptions, for instance each derivation's weight is computed using its rules' assingned weights indepently from one another.
    % \citet{} recognized is as a possible limitation, specifically in situations where mutliple derivations with the same set of rules occur (where derivation is assigned to the same weight).
    % They introduced the \emph{reranking} mechanism that assesses each constituent tree in a \(k\)-best sequence found using a probabilistic context-free grammar.
    % During this procedure, they compute a score based on extracted \emph{features} for each tree.
    % Their set of features includes very local properties, such as the grammar rules occurring in a tree, but also 
    

    \subsection{Bottom-up and Top-down Parsing with Discriminative Scoring}\label{sec:literature:chart}
    % parsing with CRF also falls into this category
    The parsing procedures accorring in this category first occurred in the field of parsing with grammar formalisms but utilizing rich functions to compute the weight of derivations.
    Similar to the introduction of reranking (c.f.\@ \cref{sec:reranking}), these functions are not limited to the locality assumptions of weighted grammars, and are often interpreted as a \emph{score} that assesses each derivation.
    \citet{Johnson99} presented a framework for \emph{maximum-entropy} models, where score functions are built around \emph{features}, i.e.\@ specific patterns that are counted within the constituent trees, very similarily to the reranking mechanism described by \citet{collins2001convolution}.
    The score is computed using a linear combination of the feature counts and interpreted as probabilty of the derivation conditioned on the sequence of tokens.
    \citet{Miyao02} extended the framework by a more efficient training procedure involving the computation of inside-outside-values.
    However, it still requires computing all possible derivations for each sentence in the training set and their score, which was very demanding at the time.
    \citet{Toutanova02,Clark04a} utilized the approach and implemented features that consider long-distance dependencies within their derivations of \emph{head-driven phrase structure grammars} (a formalism in the class of mildly context-sensive grammars, weakly equivalent to \abrv{tag}) and \abrv{ccg} respectively.
    \citet{Taskar04} criticized the maximum entropy models for their complex training procedure and proposed \emph{maximum-margin} training which drops the probabilistic interpretation but does only require computing the best derivation.
    \citet{Finkel08} proposed an interpretation of the scoring function as a \emph{conditional random field}, which defines factors for each rule in a context-free grammar derivation conditioned on the input sentence.
    Up until here, all approaches were all complementory to some grammar and scored its derivations.
    \citet{Turian06} introduced an algorithm that does not require any underlying grammar.
    Starting with the sequence of \abrv{pos} symbols, it incrementally constructs a constituent tree from a sequence of hypothesis trees (in bottom-up fashion) by choosing a subsequence which is combined with a new root node that maximizes the score for the new tree.
    \citet{Stern17} proposed the opposite direction: they presented a top-down parsing algorithm that starts with a root node and incrementally chooses a split index and two child constituent symbols for one within a list of hypothesis nodes.
    In contrast to the previous approach, their algorithm is \emph{greedy} and does not maintain an agenda and not able to backtrack from sub-optimal results.
    Like existing approaches, it features maximum-margin training, but drops the concept of explicitly defined features in favour of scores computed using artificial neural networks.
    \citet{Cor20,StaSte20} (independently but simultanously) proposed bottom-up procedures that are able to predict discontinuous constituent structures without underlying grammars.
    Yet, these are restricted to a small degree in discontinuity, specifically to fanout 2 like \abrv{tag}.

    \subsection{Transition Systems}\label{sec:literature:transition}
    Transition systems have a tradition in parsing, such \emph{shift-reduce parsers} in the field of deterministic context-free languages such as programming languages. \citep{}
    They are build around operations that incrementally construct a tree using two data structures for intermediate results:
    \begin{compactitem}
        \item a \emph{buffer} that initially contains the input sequence which will be consumed from left to right using some operations, and
        \item a \emph{stack} that is initially empty and contains a sequence of trees that are constructed using some operations.
    \end{compactitem}
    In the case of shift-reduce parsers, there are two operations
    \begin{compactitem}
        \item the \emph{shift} operation that removes the leftmost symbol on the buffer and pushes it to the rightmost position on the stack, and
        \item the \emph{reduce} operation that is parametrized with an alphabet symbol \(\sigma\) and a natural number \(k \ge 1\) and combines the rightmost \(k\) nodes on the stack to a new tree with root \(\sigma\).
    \end{compactitem}
    To counter the inherent abiguity in natural language, the transition systems used for constituent parsing are usually complemented using classifiers that predict the next operation conditioned on the current contents of both buffer and stack as well as previous operations. \citep{}
    These conditioned are usually implemented using features or artificial neural network embeddings extracted from those objects.
    \citet{liu-zhang-2017-order} investigated variations of the operations that change the usual post-order semantics (the reduce operation introduces a node \emph{after} shifting or constructing its children) of the transition system to pre- or in-order.
    \cite{kitaev-klein-2020-tetra} criticized the necessity for autoregressive classifiers in shift-reduce transition systems due to the uneven distribution of the two opertions troughout each parsing sequence.
    They introduced a transition system for continuous constituent trees that allows parallel predictions and achieves competetive results.
    So far these parser are designed exclusively for continuous constituent trees; in the following we will summerize some transition systems that were introduced for discontinuous constituent parsing.
    After that, we will focus on some aspects on the training of classifiers for transition systems.

    \subsubsection*{Transition Operations for Discontinuous Constituents}
    There are some approaches that extend the usual shift-reduce parsing for discontinuous constituent parsing.
    \citet{Verseley14b} repurposed the \emph{swap} operation, which was introduced by \citet{Nivre09} for nonprojective dependency parsing, in a transition system for constituent parsing.
    The operation transports the second right-most element on the stack back to the buffer.
    This creates a gap between the two last elements on the stack, which can than be reduced to form a discontinuous constituent.
    This approach was extended by \citet{Maier15} such that a single swap action may bundle the transport of multiple stack elements forming greater gaps.
    \citet{Maier16} also introduced an alternative transition system that utilizes a \emph{skip-shift} instead of the swap operation.
    The operation is parametrized with a natural number \(i \ge 1\) and shifts the \(i\)-th buffer element to the stack, directly forming a gap between the two last stack elements without the need to swap anything back.
    \citet{Coavoux17} formulate an alternative operation, called \emph{gap}, that moves a pointer on the stack to the left starting from the second right-most element.
    The pointer indicates the element that is combined with the right-most stack element during the next reduce operation, which will then form a discontinuous constituent if a gap operation was applied before. 
    After each shift or reduce operation the pointer is reset to the second right-most element.
    \citet{CoaCoh19} introduced a transition system that manages without a distinguished operation for discontinuities.
    They parametrize their reduce operation with an additional index on the stack to indicate the element that is combined with the right-most one during its application.

    \subsubsection*{Dynamic Oracles and Training}
    Before the classifier training for transition systems, each constituent tree in a treebank is translated into the operations to construct it.
    As mentioned earlier, the prediction of the next operation is usually conditioned on the current state of the transition system as well as the previous operations.
    As such, the operations must be predicted one after another while the operations are applied to be able to use the current state for the prediction of the next operation; such a prediction that depends on previous predictions is called \emph{autoregressive}.
    Generally, there are two opposite concepts dealing with prediction errors during training of autoregressive machine learning models:
    \begin{compactitem}
        \item \emph{teacher forcing} ignores the error and uses the gold operation sequence as input for the next prediction, and
        \item \emph{online learning} considers the erroneous prediction as input for the next steps.
    \end{compactitem}
    The latter of the two is considered to be able to generalize better to unseen data, as it exposes the classifier to examples that are not included in the training data.
    In the area of transition systems, there are two specific training approaches that are somewhat analogous to these two, referred two under the umbrella term \emph{oracles}.
    First, the \emph{static oracle} that supplies the classifier exclusively with the gold operation sequence during training.
    Using this, the classifier will only see the features as they are extracted from the treebank during the training procedure.
    \cite{Goldberg12} proposed a \emph{dynamic oracle} that considers the errors in earlier predictions and recommends an optimal operation that is applicable to reach the best possible constituent tree from the current state.
    Similarily to online learning, this leads to more configurations that are seen in the training procedure, specifically those that are not included in the original training data.
    \citet{Coavoux16,Cross16} generalized the concept of training with dynamic oracles to continuous constituent parsing.
    \citet{CoaCoh19} included a dynamic oracle with their transition system for discontinuous constituents.
    
    \subsection{Other Approaches}\label{sec:literature:others}
    This subsection gathers some discontinuous constituent parsers that are worth mentioning, but do not fit into the previously discussed traditional approaches.
    Nevertheless, we recognize some common trends among the parsers that allow us to compare them in the following.

    \subsubsection*{Continuous Parsing with Reordering}
    One way to illustrate discontinuous constituent trees is to extend a continuous tree in tandem with an alignment or reordering of its leaves (e.g.\@  as we did in \cref{def:ctree}).
    \citet{FerGon21a} exploit this to reduce parsing discontinuous constituents in two steps
    \begin{inparaenum}[]
        \item they first predict an alignment that re-arranges the sequence of input tokens, and then
        \item they use two state-of-the-art parsers for continuous constituents to parse with the re-arranged sequence of tokens.
    \end{inparaenum}
    Despite that there are some errors in the prediction of the alignment (the precision and recall of the relocated tokens are around $0.8$ in \abrv{dptb}, \abrv{negra} and TiGer), they report competetive parsing scores with their final results.
    \citet{Sun22} extend this approach by introducing a re-arrangement algorithm that does not rely on the prediction of specific sentence positions.
    They report more accurate alignment predictions and also more accurate overall results.

    \subsubsection*{Transforming Dependency Parses into Constituent Trees}
    \citet{Hall08} were the first to recognize structural relations between nonprojective dependency trees and discontinuous constituent trees and proposed a parsing procedure that leverages a dependency parser by transforming the resulting dependency into a constituent tree.
    Sepcfically, they formulate two transformations:
    \begin{inparaenum}[]
        \item one from (discontinuous) constituent trees into (nonprojective) dependency trees to convert a constituent treebank into a dependency treebank, and
        \item the aforementioned from (nonprojective) dependency trees into (discontinuous) constituent trees.
    \end{inparaenum}
    They conjecture that both transformations are lossless.
    \citet{Ferandez15} extended the approach by introducing lossy transformations from constituent trees into dependency trees, such that the dependency trees could be predicted more accurately.
    They reported more accurate overall results using their new transformations.

    \subsubsection*{Parsing as Sequence-to-Sequence Prediction}
    % also llm, where in-context learning is a special case
    Sequence-to-sequence prediction is a very general category of tasks in machine learning that involves the prediction of an output sequence for an input sequence of any possible length; intuitively, we can think of it as a translation.
    \citet{vinyals2015grammar} exploited a translation model to predict linearized constituent trees (i.e.\@ strings with nested levels parentheses) for input sentences.
    To their surprise, they achieved state-of-the-art results.
    \citet{FerGon21b} extended the appproach to the discontinuous case.
    They investigated multiple techniques for the linearization of constituent trees, among them also shift-reduce action sequences analogously to parsers based on transition systems as suggested by \citet{Ma2017DeterministicAF,liu-zhang-2017-encoder}.
    A recent investigation by \citet{bai2023constituency} exploits pretrained large language models (short: \abrv{llm}), such as \abrv{gpt4} \citep{openai2023gpt4}.
    These models were prompted with the task to parse an input sentence, either without context, or supplied with a few constituent trees as examples.
    They conclude that \abrv{llm} enhance the parsing quality for sequence-to-sequence models in case they are fine-tuned for the task.
    
    \section{Parsing Beyond Consituency Relations}\label{sec:literature:beyond}
    In our work, we exclusively investigated constituent trees as one representive for the syntactic analysis of natural language.
    Besides that, \emph{dependency relations} are of great interest in the area of syntax analysis.
    There are multiple types of dependency relations that were considered in the literature, some of them are related to constituency analysis.
    The following subsection gives an overview of the relations and shares some entry points for different approaches in parsing dependencies.
    After that, we dedicate a subsection to a different area, namely \emph{semantic representaitons}.
    It gives a small overview of some prepresentations and points to recent work in the area.
    \citet[Sections 18 and 19]{Jur23} give great surveys for these two topics in their book.

    \subsection{Dependency Parsing}
    Dependencies form a school of analyses that recognize syntactic structure in the form of relations between tokens.
    There are several different relations that are distinguished in the literature \citep[Table 3 shows the list of dependency relations annotated in the Universal Dependencies treebanks]{de2021universal} and their composition varies between specific treebanks and languages.
    For instance in the treebanks distributed with the CoNLL shared task in 2006, there are 82 dependency relations illustrated in the Chinese but only 7 in the Japanese treebank \citep[Table 1]{buchholz2006conll}.
    The specific relations and annotation styles are also sometimes a subject of debate. \citep{Gerdes2016DependencyAC,Rehbein2017UniversalDA,osborne2019status}
    The \emph{Universal Dependencies} project \citep{de2021universal} was established to build a multilingual set of treebanks with consistent relations and annotation schemes.

    The dependency relations within a sentence can be illustrated as a tree, such that the tokens of the sentence are the the inner nodes and leaves, and the relations are denoted as the directed relations between parent and child nodes.
    The type of relations are usually denoted at each arc between parent and child.
    Analogously to discontinuity in constituent trees, there is the concept of \emph{nonprojectivity} in dependency trees.
    Such a tree is called nonprojective, if there is a subtree whose nodes are not a coherent region in the sentence.

    As for constituent parsing, there are several different approaches for parsing dependencies.
    The following list shall only shortly summarize some techniques that share some aspects with the previously discussed styles constituency parsing:
    \begin{itemize}
        \item Several lexicalized grammar formalisms were considered for dependency parsing. Generally, each derivation in such a grammar formalism is interpreted as dependency tree in the same shape by replacing each grammar rule by its contained terminal symbol. \citet{chiang2000statistical} propose lexicalized tree-adjoining grammars as a formalism to cover constituency as well as dependency parsing at the same time. \citet{hockenmaier2002generative} argue that the quality of combinatory categorial grammar parsers should be compared using scores assessing the dependency structure rather then the parseval measure for constituents. This was adopted into the following publications involved in parsing with both formalisms, combinatory categorial grammars and tree-adjoining grammars \citep{Kas17,Bla18}. \citet{Clark04} discussed the importance of supertagging in parsing with lexical(ized) grammars, specifically \abrv{ccg}. To our knowledge, all recent publications in parsing with \abrv{ccg} or \abrv{tag} follow this approach as well. \citep{Kas17,LewisSteedman14}   
        \citet{kuhlmann2009treebank} proposed \abrv{lcfrs} extracted from (non-projective) dependency treebanks as grammar for dependency parasing.
        \item \citet{eisner-1996-three} defined a bottom-up parsing algorithm for dependency structures and proposed a stochastic interpretation to weight the (intermediate) parse items. However, their approach does not involve and explicit grammar. Recently, \citet{yang-tu-2022-headed} published a bottom-up dependency parser that involves an artificial neural network for scoring. However, both approaches are restricted to projective depdencies.
        \item Several transition systems have been proposed for depdency parsing. \citet{nivre-2003-efficient, yamada-matsumoto-2003-statistical} independently proposed transition systems based on the shift-reduce parsing algorithm for the task of dependency parsing. \citet{attardi-2006-experiments} were the first to introduce a set of operations that extends the previous transition system and allows the prediction of non-projective dependency relations. \citet{nivre-2009-non} proposed the \emph{swap} operation for the nonprojective case, which subsumes multiple operations that were distinguished by \citet{attardi-2006-experiments}. \citet{Goldberg12} introduced dynamic oracles that allow diverging from gold configurations during the training procedure for the projective case. \citet{gomez2014polynomial} presented their generalization of the dynamic oracle to the nonprojective case using the transition system by \citet{attardi-2006-experiments}.
    \end{itemize}
    There are some other approaches that are not covered in this summary, for example the reduction of dependency parsing to finding a maximum spanning tree in a graph by \citet{mcdonald-etal-2005-non}.
    All readers are encuraged to consult \citet{nivre2010dependency} for a broad overview of the field.

    \subsection{Parsing Semantic Representations}
      %  supertagging: https://arxiv.org/abs/2310.14124
    Beyond syntactic analyses, there are several concurrent formalisms that were introduced as general frameworks to capture the meaning of natural language.
    The following two are frequently used and seem to have established as standard to some extend:
    \begin{itemize}
        \item \citet{langkilde-knight-1998-generation} introduced \emph{abstract meaning representations} (short: \abrv{amr}) that illustrate concepts within an utterance as nodes and the relation between them as edges in a rooted, directed and acyclic graph. They denote the graphs as strings that include parentheses for nesting as well as variables that allow referencing nodes as children of multiple parent nodes. \citet{knight2021abstract} published the \abrv{amr} bank, which is used as a benchmark to evaluate the recognition of semantic understanding in natural language.
        \item \citet{reddy-etal-2016-transforming} propose logic formulas to represent the meaning of natural language. Their formulas comform to first-order predicate logic and are denoted in the form of lambda-calculus terms. \citet{reddy-etal-2017-universal} introduce a project that transforms the universal dependency treebanks in their semantic formalism, providing a wide-coverage dataset in multiple languages.
    \end{itemize}
    Besides these formalisms that act as general-purpose frameworks, there are some representations that are tailored to specific tasks.
    E.g.\@ the field of \emph{task-oriented semantic parsing} is concerned with recognizing commands in utterances for dialogue systems.
    Publications involved in this tasks typically recognize an \emph{intent} and a fixed set of \emph{slots} that are filled with tokens from the utterance to identify the intention of a request in a dialogue.
    \citet{gupta-etal-2018-semantic-parsing} propose hierarchical representation to overcome the limitation to one itent per utterance.

    The approaches at semantic parsing are just as diverse as the proposed formalisms to represent meaning.
    In the following, we  want to share a small glimpse into parsing approaches that relate to some extend to the previously discussed procedures for syntax analysis; c.f.\@ \citet{kamath2018survey} for a broad overview.
    Grammar anad transition based parsing concepts well known in the area of syntax analysis are also represented in parsing for semantic representations, albeit less prevalent.
    Representation formalisms that are based on graph structures, such as \abrv{amr}, may be approached using graph grammar or automata formalisms, for example \emph{directed acyclic graph automata} (short: \abrv{dag} automata) \citep{fancellu2019semantic} or \emph{hyperedge replacement grammars} \citep{drewes1997hyperedge}, e.g.\@ \citet{peng2015synchronous} propose a probabilistic synchronous grammar that recognizes a string with context-free grammar rules while generating an \abrv{amr}.
    \citet{peng2018amr,vilares-gomez-rodriguez-2018-transition} define transition systems that are able to produce \abrv{amr}.
    Sequence-to-sequence models are more common in the literature; they are used to generate a linearized semantic representation, e.g.\@ a logic formular \citep{dong-lapata-2016-language} or \abrv{amr} (\citep{zhang-etal-2019-amr}), from an input sentence using artificial neural networks.
    \citet{fancellu2019semantic} propose to enhance the decoder of a sequence-to-sequence model using a \abrv{dag} automaton.

    \section{Parsing with Supertagging}\label{sec:literature:supertagging}
    \citet{bangalore1999supertagging} propose the approach of \emph{supertagging} as a preliminary step in parsing with lexicalized tree-adjoining grammars.
    It consists of a preliminary step for parsing where a classifier predicts elementary trees for each lexical item in the input sentence using features extracted from the sentence.
    As this process resembles the prediction of \abrv{pos} tags to a certain extend, but the predicted elementary trees contain a great share of grammatical information including long distance dependencies, they call them \emph{supertags}.
    They used the \abrv{xtag} grammar \citep{xtag01}, i.e.\@ a \emph{hand-crafted tree-adjoining grammar} for the English language, in their experiments. 
    They suggest this process to reduce reduce the search space of the usual parser that then ``only'' needs to combine the supertags to form a derivation, and propose to use it for other lexical(ized) grammar formalisms such as \abrv{ccg} and \abrv{hpsg}.
    The remainder of the section gives an overview of the developements in supertagging after its inception.

    Shortly after, \citet{clark2002supertagging} reported experiments for supertagging with a combinatory categorial grammar.
    They predicted lexical categories found in \emph{\abrv{ccg}-bank} \citep{Hoc07}, i.e.\@ a hand-crafted grammar.   
    \citet{Clark04} recognized supertagging as an essential step in parsing with \abrv{ccg}, due to the formalism's complex categories that are assigned to each lexical item in the input.
    Instead of the hidden Markov model that \citet{bangalore1999supertagging} used to predict supertags, they define a maximum-entropy model using a set of features in a similar manner as e.g.\@ contemporary reranking mechanisms used in constituent parsing (see \cref{sec:reranking}).
    \citet{Auli12} described a general framework for parsers that involve supertagging.
    Specifically, they propose an iterative algorithm that incrementally expands the set of used supertags during parsing until the process succeeds.
    \citet{kaeshammer2012german,Kaeshammer2012GermanAE} presented a semi-automatic conversions of a German and English constituent treebanks into \abrv{ltag} derivations; \citet{Bla18} contribute a conversion for the \emph{French Treebank} \citep{abeille2003building} using similar methology.
    These conversions involve some specific treatments for certain language phenomena that depend on the used treebanks.
    With the general availability of deep learning models, the prediction of supertags could pass on hand-craftet features and utilize context-sensitive embeddings computed by recurrent neural networks, most prominently bidirectional \abrv{lstm}s.
    \citet{vaswani2016supertagging} proposed an architecture, that combines such embeddings with an autoregressive language model for supertags to enhance the prediction; they show experimental results using \abrv{ccg}-bank.
    \citet{Kas17,Bla18} used similar embeddings without this language model in their experiments with \abrv{ltag} supertags in English, French and German treebanks.
    All of the referenced publications in this section report solely supertag prediction and/or dependency parsing scores.

    \ifSubfilesClassLoaded{%
        \printindex
        \bibliography{../references}%
    }{}
    % This chapter shall provide an overview of different published approaches for parsing discontinuous constituents.
    % As roughly outlined in the introduction, many such approaches were derived from parsing continuous structures.
    % We will cover them shortly alongside the discontinuous parsers described in the following sections and subsections to point to the origins of each discontinuous parsing procedure.
    % The chapter is structured in sections and subsections that distinguish different parsing concepts.
    % At the top level, we contrast traditional grammar-based parsing with more recent approaches based around discriminative classifiers such as neural networks.
    % The latter is divided into subsections for specific parsing concepts that established in the parsing community (and a last subsection that contains everthing else).

    % \section{Parsing with Grammars}
    % The foundations for the concept of analyzing natural language phrases into constituent trees is due to \citet{Cho56}.
    % They introduced the theoretic framework alongside with grammar formalisms used to explain or create these analyzations.
    % Among these grammar formalisms were context-free grammars, that formulate rules as direct dominance relations of the form $A \to B_1 \ldots B_k$ where $B_1 \ldots B_k$ are $k$ constituent or \abrv{pos} symbols that are dominated by the constituent symbol $A$; within the rules the symbols $A, B_1, \ldots, B_k$ are called nonterminals.
    % The aim for parsing with such a grammar is a constituent structure where each direct parent-child relation matches one of the available grammar rules.
    % The formalism was extended in several directions, must notably by adding weights to the rules that indicate a preference value or cost \citep{Goodman}, most notably probabilistic context-free grammars (PCFG), or latent annotations that add the ability to specify the possible dominance relations beyond the occurring constiuent symbols \citep{Petrov06}.

    % Later publications of constituent treebanks, such as the Penn Treebank \citep{Marcus94}, offered hand-crafted and linguistically sound analyzations of natural language sentences.
    % They are used to automatically extract grammars and assess parsers by comparing the constituent tree obtained by a parser with those in the treebank for a common set of sentences.
    % There were several branches of research that aimed to improve upon the accuracy and/or the parse time with such automatically extracted grammars, such as the following:
    % \begin{itemize}
    %     \item
    %         Binarization was first introduced to show the equivalence of binary context-free grammars to the general case. \citep{}
    %         However, this process was generalized and extended to 
    %     \item
    %         Probabilistic grammar formalisms and stochastic parsing established as standards in the field.
    %         In the most primitive case, each rule's weight is determined by relative frequency estimation, i.e.\@ counting how often the rule occurs as a relation in the treebank. \citep{}
    %         \citep{Petrov06} introduced a PCFG-LA extraction pipeline base on the Expectation-Maximization framework \citep{}.
    %         They define implicit latent annotations by iteratively splitting the nonterminal symbols and tuning the weights according to the treebank.
    % \end{itemize}

    
    
    % It became evident that this formalism lacks certain capabilities to describe natural languages. \citep{Shieber85}
    % But it  it established as the base for several branches of developments aiming to overcome these limitations, such as generalized phrase structre grammars \citep{} and head-driven phrase structre grammars \citep{}, combinatory categorial grammars  \citep{}, tree-adjoining grammars  \citep{} as well as multiple context-free grammars \citep{}.
    % Other developements, such as context-free grammars with latent annotations (cfg-la) and 

    % \section{Parsing with Discriminative Models}
    % \subsection{Supertagging}
    % \subsection{Reranking}
    % \subsection{Chart Parsing}
    % \subsection{Transition Systems}
    % \subsection{Other Approaches}
\end{document}