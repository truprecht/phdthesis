\documentclass[../document.tex]{subfiles}


\begin{document}
    \chapter{Parsing via Supertagging}
    This chapter describes a unified framework of a supertagging-based parsing process based on the grammars as explained in the previous chapter.
    The parsing process is divided into 3 consecutive steps, plus one optional refinement procedure (reranking) as illustrated in \cref{fig:parsing:overview}:
    \begin{enumerate}
        \item\label{parsing:item:1}
            First, a set of blueprints is predicted for each token in the input sentence; the number of predicted blueprints per position is a hyperparameter.
            Each blueprint is transformed into a supertag by replacing its wildcard symbol with the position in the sentence it was predicted for.
            The confidence value for the prediction of each blueprint accompanies each supertag as weight.
        \item\label{parsing:item:2}
            The supertags are combined into weighted grammars.
            A statistical parsing process for the grammar formalism is used to find derivations for the sequence of consecutive positions in the input sentence.
            If reranking is used, then this step yields a collection of \(n\) best derivation trees, where \(n \in \DN_+\) is a hyperparameter and \emph{best} refers to an endorelation on the weights (e.g.\@ highest probability).
            Otherwise, it yields \emph{a} best derivation.
        \item\label{parsing:item:3}
            The derivations are converted into constituent structures by evaluating the \abrv{dcp} compositions or (in case of \abrv{lcfrs} supertags) assuming the nonterminals as inner nodes.
            These constituent structures are extended to constituent trees by adding predicted \abrv{pos} symbols and the tokens from the input sentence.
        \item\label{parsing:item:4}
            Lastly, a reranking approach is used to (re-)score the constituent trees and pick the best as result.
            We use \emph{data-oriented parsing}, a precise statistical grammar formalism, that was used as final stage of a coarse to fine parsing approach before. \cite{CraSchBod16}
    \end{enumerate}
    For step \ref{parsing:item:1}, we use a neural network to compute the prediction confidence for each blueprint at each position in the input sentence.
    The training of these models is described in \cref{chapter:training}.
    The transformation from grammar derivations to constituent structures in step \ref{parsing:item:3} is part of the definitions in \cref{sec:grammar:lcfrs,sec:grammar:hybrid}.
    The remaining steps \ref{parsing:item:2} and \ref{parsing:item:4} are described in more detail in the following two sections.

    \begin{figure}
        \subfile{figures/parsing-overview.tex}
        \caption{
            Illustration of the supertagging-based parsing process.
            Staggered gray boxes indicate sequences of objects, e.g.\@ the \emph{parsing} step yields a collection of derivation trees for the input sentence.
        }
    \end{figure}

    \section{Statistical Parsing with Predicted Supertags}
    Suppose that we have a collection of supertags with an associated weight for each tag and position in an input sentence.
    This section deals with the construction of weighted grammars from the supertags and the subsequent statistical grammar parsing process to find derivations.
    Here, \emph{grammars} is in plural because we use a strategy that incrementally constructs grammars in growing size, in a similar fashion as described by \citet[Section 5.1]{Clark04} and \cite[Section 2.2.2]{Auli12}.
    The strategy defines a sequence of decreasing thresholds such that a grammar for such a threshold \(\beta\) includes rules for all supertags that were predicted with confidence \(\ge \beta\).
    Starting with the first, if the grammar for the \(i\)th threshold can not find a derivation for the sequence of input positions, then we try the next and repeat until at least one derivation was found; only this grammar is used to compute a collection of \(n\) best derivations.
    For each increment \(i > 1\), the parsing results with the previous grammar are re-used, as the set of rules in increment \(i-1\) is a subset of that in increment \(i\).
    This approach is different to the one that we used in previous versions of the parser. \citep{RupMoe21, Rup22}
    There, we used a fixed amount of predicted blueprints per sentence position.

    \paragraph{Parsing with \abrv{Lcfrs} and \abrv{Hg} Supertags}
    The parsing procedure for \abrv{lcfrs} was well described by \citet[Section 7.1]{Kal10} and can be used for parsing with grammars in the two formalisms \abrv{lcfrs} and \abrv{hg}.
    In the case of \abrv{hg}, the sets of derivations for an input sequence is defined using the projection into the \abrv{lcfrs} components only, so the \abrv{dcp} components have not influence at all on the sets of derivations; they are solely used to define the constituent structure for an already determined derivation.
    In previous versions, we used the \abrv{lcfrs} parser implemented by \citet{CraSchBod16} and extended it for our purposes.\footnote{
        For using \emph{disco-dop} with our extracted supertags, we implemented some transformations to accommodate that it is only able to parse with binary and simple \abrv{lcfrs}.
    }
    However, we omitted this in favor of a unified implementation with supertags containing only \abrv{dcp}.

    \paragraph{Fanout-Restricted Parsing with \abrv{Dcp}}

    \section{Reranking}

    \printindex
\end{document}
