\documentclass[../document.tex]{subfiles}


\begin{document}
    \chapter{Parsing via Supertagging}
    This chapter describes a unified framework of a supertagging-based parsing process based on the grammars as explained in the previous chapter.
    The parsing process is divided into 3 consecutive steps, plus one optional refinement procedure (reranking) as illustrated in \cref{fig:parsing:overview}:
    \begin{enumerate}
        \item\label{parsing:item:1}
            First, a set of blueprints is predicted for each token in the input sentence; the number of predicted blueprints per position is a hyperparameter.
            Each blueprint is transformed into a supertag by replacing its wildcard symbol with the position in the sentence it was predicted for.
            The confidence value for the prediction of each blueprint accompanies each supertag as weight.
        \item\label{parsing:item:2}
            The supertags are combined into a weighted grammar.
            A statistical parsing\todo{clearly distinguish statistical grammar parsing from the whole process} process for the grammar formalism is used to find derivations for the sequence of consecutive positions in the input sentence.
            If reranking is used, then this step yields a collection of \(k\) best derivation trees, where \(k \in \DN_+\) is a hyperparameter and \emph{best} refers to an endorelation on the weights (e.g.\@ lowest cost or highest probability).
            Otherwise, it yields \emph{a best} derivation.
        \item\label{parsing:item:3}
            The derivations are converted into constituent structures by evaluating the \abrv{dcp} compositions or (in case of \abrv{lcfrs} supertags) assuming the nonterminals as inner nodes.
            These constituent structures are extended to constituent trees by adding predicted \abrv{pos} symbols and the tokens from the input sentence.
        \item\label{parsing:item:4}
            Lastly, a reranking approach is used to (re-)score the constituent trees and pick the best as result.
            We use \emph{data-oriented parsing}, a precise statistical grammar formalism, that was used as final stage of a coarse to fine parsing approach before. \citep{CraSchBod16}
    \end{enumerate}
    For step \ref{parsing:item:1}, we use a neural network to compute the prediction confidence for each blueprint at each position in the input sentence.
    The training of these models is described in \cref{chapter:training}.
    The transformation from grammar derivations to constituent structures in step \ref{parsing:item:3} is part of the definitions in \cref{sec:grammar:lcfrs,sec:grammar:hybrid}.
    The remaining steps \ref{parsing:item:2} and \ref{parsing:item:4} are described in more detail in the following two sections.

    \begin{figure}
        \subfile{figures/parsing-overview.tex}
        \caption{
            Illustration of the supertagging-based parsing process.
            Staggered gray boxes indicate sequences of objects, e.g.\@ the \emph{parsing} step yields a collection of derivation trees for the input sentence.
        }
    \end{figure}

    \section{Statistical Parsing with Predicted Supertags}\label{sec:parsing}
    In this section, we will describe two approaches to combine predicted supertags into derivations, one for \abrv{lcfrs} and \abrv{hg} supertags and one for \abrv{dcp} supertags.
    For the remainder of the section, consider we have a set \(B\) of supertag blueprints, an input sentence of length \(n\), an assignment of prediction confidence values \(\mu\colon B \times [n] \to \DR\) obtained from the prediction of blueprints (step \ref{parsing:item:1} of the parsing process outlined in the beginning of the chapter), and assume that we aim to find a collection of up to \(k \in \DN_+\) best derivations.
    The three objects are used to construct a series of grammars in growing size, in the same fashion as described by \citet[Section 5.1]{Clark04} and \citet[Section 2.2.2]{Auli12}.
    This strategy defines a sequence of decreasing thresholds \((\beta_i \in \DR \mid i \in I)\) where the set of indices \(I\) is a \(\DN_+\) prefix.
    These values are used to define growing intervals of confidence values that are admitted in the constructed grammars; each starting from the best confidence value for each position and up to \(\beta_i^j = \max_{b \in B} \mu(b, j) + \beta_i\) for each \(i \in I\) and \(j \in [n]\).
    For each index \(i \in I\) and position \(j \in [n]\), we define the set \(S_i^j = \{ b \in B \mid \mu(b, j) \ge \beta^j_i \}\) of supertag blueprints that were predicted with confidence that differs at most by \(\beta_i\) from the highest confident prediction at the same position \(j\).
    Then the iterative process to find derivations is as follows:\todo{die Liste lieber als Alg.\@ darstellen?}
    \begin{compactenum}
        \item We start with \(i = 1\).
        \item \label{item:parsing:2:2}
            The weighted grammar \(G_i\) is constructed from the set of supertags \(S_i = \{b[j] \mid j \in [n], b \in S_i^j \}\) and the weight assignment \(\weight_{i}\colon S_i \to \DR\) with \(\weight_i(b[j]) = \mu(b, j)\) for each \(j \in [n]\) and \(b \in S_i^j\).
        \item We search for admissible derivations in the constructed grammar as described in \cref{sec:parsing:lcfrs,sec:parsing:dcp}.
        \item
            If there is at least one admissible derivation in \(G_i\), the sequence of \(k\) best admissible derivations in \(G_i\) is reported as result of this step.
            If that is not the case and \(i+1 \in I\), then we let \(i := i+1\) and go back to step \ref{item:parsing:2:2}.
            Otherwise, we report failure as we did not find any derivation.
    \end{compactenum}
    Finally, only the first grammar in the series \((G_i \mid i \in I)\) that contains an admissible derivation is used to compute a collection of \(k\) best derivations.
    For each increment \(i > 1\), the parsing results with the previous grammar are re-used, as the set of rules in increment \(i-1\) is a subset of that in increment \(i\).
    This approach is different to the one that we used in previous versions of the parser, wehere we used a fixed amount of predicted blueprints per sentence position. \citep{RupMoe21, Rup22}
    We used the values by \citet{Clark04} as orientation and chose a series fixed values \((\beta_i = 0.1^i \mid i \in \DN_+)\) in our experiments.

    \subsection{Parsing with \abrv{Lcfrs} and \abrv{Hg} Supertags}\label{sec:parsing:lcfrs}
    In the case of \abrv{lcfrs} and \abrv{hg} supertags, the sets of admissible derivations for an input sequence is defined using the nonterminals and the \abrv{lcfrs} compositions in the grammar rules.
    The \abrv{dcp} compositions in \abrv{hg} supertags as well as the transportation marker in \abrv{lcfrs} supertags have no influence at all on the sets of derivations.
    They are solely used to define the constituent structure for an already determined derivation.
    With both formalisms, we exploit statistical parsing procedures for \abrv{lcfrs} as \citet{Geb20} did in the case of parsing with hybrid grammars.
    Parsing procedures for \abrv{lcfrs} are well docuemented, e.g.\@ as described by \citet[Section 7.1]{Kal10}.
    For the case of \(k\) best parsing, the approach can be extended using the techniques described by \citet{HuaChia05} to enumerate found derivations with increasing weight.
    In previous versions, we used the \abrv{lcfrs} parser implemented by \citet{CraSchBod16} and extended it for our purposes.\footnote{
        For using \emph{disco-dop} with our predicted supertags, we implemented some transformations to accommodate that it is only able to parse with binary and simple \abrv{lcfrs}.
        Each supertag is transformed into a collection of appropriate rules by splitting the terminal symbol into a separate rule where the supertag is its \abrv{lhs} nonterminal.
        In the case of supertags with binary rules, this transformation yields a ternary rule that is then split into two using the usual binarization techniques and by introducing a unique intermediate nonterminal.
        As these rules are connected using unique nonterminals, they must occur together in derivations.
        This transformation is easily reverted in a derivation by identifying the supertag at the bottom rule of such a cluster of rules and replacing the whole construct with the supertag.
    }
    However, we omitted this in favor of a unified implementation that also implements parsing with \abrv{dcp} supertags.

    \subsection{Fanout-Restricted Parsing with \abrv{Dcp} Supertags}\label{sec:parsing:dcp}
    Using grammars that solely consist of \abrv{dcp} rules was, to our knowledge, not considered before for in the field of discontinuous constituent parsing.
    By itself, the \abrv{dcp} compositions do not impose any restrictions to the lexical symbols in the defined sets of derivations, compared to \abrv{lcfrs} which precisely specify which parts of the spans may occur next to each other.
    We chose to extend the rules by a fanout restriction to limit the sets of derivations.
    For each \abrv{dcp} supertag \((r, f)\), the fanout restriction is a positive integer \(f \in \DN_+\) such that the set of lexical symbols in each derivation rooted with the supertag must have a fanout of \(f\).
    And additionally to the fanout restriction, we demand that 
    We adapt a parsing algorithm for \abrv{lcfrs} grammars for parsing with \abrv{dcp} supertags.
    As usual, the algorithm is split into two steps:
    \begin{enumerate}
        \item \label{step:parsing:chart}
            First, we compute a compact representation (called \emph{parse chart}) for the set of derivations for an input sentence in a given grammar and their weights.
            To summarize, the parse chart combines derivations rooted by a common rule and with the same set of lexical symbols occurring within the rules.
        \item \label{step:parsing:derivation}
            Secondly, the parse chart is used to determine the least parse in the set of derivations.
            This aproach recursively descents into the parse chart and, in each recursion, determines the rule at the root of each (sub-)derivation with the least weight.
            It can be extended using the techniches described by \citet{HuaChia05} to enumerate the derivations with increasing weight to implement a \(k\) best parsing algorithm.
    \end{enumerate}
    These two are described in more detail in the following paragraphs with complementary algorithms in pseudo-code.
    
    \begin{algorithm}
        \caption{\label{alg:parsing:chart}
            Weighted parsing algorithm for \abrv{dcp} supertags.
            This coarsely resembles an algorithm for parsing \abrv{lcfrs} as shown by \citet[the ``na\"ive algorithm'' in section~3]{Burden05}.
        }
        \subfile{figures/algorithms/parsing.tex}
    \end{algorithm}
    
    \paragraph{Step \ref{step:parsing:chart}}
    \Cref{alg:parsing:chart} illustrates a process that implements this step.
    It is derived from an \abrv{lcfrs} parsing algorithm as presented by \citet[the na\"ive algorithm in sec.~3]{Burden05}.
    In the following we will refer to this original presentation of the parsing algorithm by \abrv{bl}.
    Unlike \abrv{bl}, we opted to describe the process not in the form of a deduction system but in pseudocode.
    This process demands the aforementioned inputs: a set of supertags for each position in the sentence to be parsed as well as weights according to the prediction confidence.
    Additionally, we utilize a distinct symbol \(X_i\) for each position \(i \in [n]\) that does not occur as a nonterminal.
    The procedure constructs tuples of the form \((a, b, w)\) with the following components
    \begin{itemize}
        \item The first component \(a\) is a tuple of the form \((A, f, B_1 \ldots B_k, L)\) where
            \begin{compactitem}
                \item \(A\) is a nonterminal occurring at the \abrv{lhs}, and each symbol in \(B_1, \ldots, B_k\) is either a nonterminal occurring at the \abrv{rhs} of a rule or of the form \(X_i\),
                \item \(f \in \DN_+\) is a fanout restriction, and
                \item \(L \subset [n]\) is a set of set of sentence positions.
            \end{compactitem}
            If \(k>0\) the tuple \(a\) corresponds to an \emph{active item} in \abrv{bl} and denotes a partially revealed derivation starting with the \abrv{lhs} \(A\).
            This intermediate result already considers subderivations with rules carrying the terminal symbols in \(L\).
            The symbols in \(B_1, \ldots, B_k\) denote nonterminals for the subderivations that are still left to investigate and, if \(X_i\) occurs among them, it denotes the terminal in the root of the derivation.
            If \(k=0\) the investigation is complete and found a derivation, this corresponds to a \emph{passive item} in \abrv{bl}.
        \item The second component \(b\) is a tuple of the form \((s, L_1 \ldots L_k)\) where
            \begin{compactitem}
                \item \(s\) is a supertag (such that the whole object considers a derivation rooted by \(s\)), and
                \item \(L_1, \ldots, L_k \subseteq [n]\) are the sets of sentence positions in the subderivations that were already investigated for this parse item.
            \end{compactitem}
            The tuple \(b\) is a \emph{backtrace} used to reproduce the derivations that the parse item represents.
            This corresponds to the ``list of daughters'' that are stored with the parse items in \abrv{bl}.
        \item
            The third component \(w\) is the weight for the parse item.
            As the description in \abrv{bl} was unweighted, this part does not occur in it.
    \end{itemize}
    
    The algorithm uses four collections to store (intermediate) results:
    \begin{compactitem}
        \item A queue \(Q\) that contains tuples in the above form and stores them until they are investigated.
            The third component \(w\) of the elements in \(Q\) decides which tuple is (drawn and removed).
        \item A \emph{parse chart} \(C\) that keeps track of possible derivations for each nonterminal and yield.
            In the figure, it is a \(\mathcal{P}([n]) \times N\)-indexed family of subsets of \(S \times \mathcal{P}([n])^*\).
            For each \(L \subseteq [n]\) and \(A \in N\), the set \(C_{(A, L)}\) contains tuples of the form \((A \to c\,(B_1, \ldots, B_k), L_1 \ldots L_k)\) where \(A \to c\,(B_1, \ldots, B_k)\) is the root of a derivation for the yield \(L\) and the combinations \((B_1, L_1), \ldots, (B_k, L_k)\) recursively link to child derivations stored in \(C\).
            Whenever we draw a passive item from the queue, it assimilates into \(C\).
        \item A \(\mathcal{P}([n]) \times N\)-indexed family \(W\) of elements in \(\DR\) stores the weight of derivations.
            For each \(L \subseteq [n]\) and \(A \in N\), the value \(W_{(A, L)}\) is the best weight of a derivation of rules in \(S\) starting with \(A\) for the yield \(L\).
            As well as \(C\), this collection is updated each time we investigate a passive item.
        \item A collection of tuples containing active items \(I\) that is accessed by the next \abrv{rhs} nonterminal symbol reserved for the next investigation.
    \end{compactitem}
    
    Roughly, we can distinguish two parts of \cref{alg:parsing:dcp}:
    \begin{itemize}
        \item First, there is a setup in lines 2--9 where the four collections are initialized (lines 2--4) and the queue \(Q\) is equipped with tuples containing an active item for each supertag (lines 6--9).
            The active items are constructed from the supertags by adopting the \abrv{lhs} nonterminal and fanout restriction from the supertag.
            The sequence \(B_1' \ldots B_{k+1}'\) is obtained from the \abrv{rhs} nonterminals and one symbol \(X_i\) for the lexical symbol \(i\) occurring in the supertag.
            They are ordered such that they occur in order of their corresponding variables and the lexical symbol in \(c\).
            The last component of each active item starts with an empty set of sentence positions.
            This initialization of active items corresponds to the ``Predict'' inference rule in \abrv{bl}.
        \item
            The remaineder in lines 10--23 contains a loop that investigates each tuple added to the queue \(Q\).
            It distinguished three cases for tuple popped from \(Q\):
            \begin{compactitem}
                \item In the first case (lines 12--16), the tuple contains a complete item for derivations whose occurring lexical symbols adhere to the fanout restriction.
                    It is added into the collections \(C\) and \(W\).
                    After that, the loop in lines 15--16 consults the collection \(I\) to re-iterate all tuples with active items that need to be investigated using a passive item with matching \abrv{lhs} nonterminal \(A\).
                    The function \textsc{addSucessor} concludes the investigation and constructs a follow-up parse item.
                \item The second case (lines 17--19) adds the lexical symbol to the set of leaves and constructs a follow-up parse item.
                \item The final case (lines 20--23) considers an active item that represents an incomplete derivation and shall be supplemented with a subderivation for the right-most nonterminal \(B_k\).
                    The tuple is stored in \(I\) indexed by the nonterminal \(B_k\).
                    Finally, in lines 22--23, the function \textsc{addSuccessor} is used again to construct follow-up items with the information about the investigated passive items stored in \(C\) and the weights in \(W\).
            \end{compactitem}
            The first and third case correspond to the ``combine'' inference rule in \abrv{bl}.
            It triggers every time that a new active or passive item is seen and is therefore distributed to these two places.
    \end{itemize}
    
    The most obvious difference to \abrv{bl} still remaining to be mentioned is that we investigate active items in direction from right to left of the \abrv{rhs} nonterminals instead of left to right.
    This is because makes the restriction, that the sequence of children at each node in a \abrv{dcp} derivation must be ordered by the least lexical symbol, easier to check when we do not use a composition function for the lexical symbols.
    From right to left, we check the order using the least positions included in intermidiate results as in line 17 and line 25.
    If we would investigate the successors in the other direction, then we could not distinguish the least positions starting with the second successor as we only store the union.
    
    \begin{algorithm}
        \caption{\label{alg:parsing:deriv}
            Illustration for the enumeration of $k$ best derivations from a parse chart that was obtained as illustrated in \cref{alg:parsing:chart}.
            This is a direct adaption of the algorithm presented by \citet{HuangChiang05}.
            This illistration is split and continues on the following page.
        }
        \subfile{figures/algorithms/read-derivation.tex}
    \end{algorithm}
    
    \begin{algorithm}\ContinuedFloat
        \caption{
            (Coninued from the previous page)
        }
        \subfile{figures/algorithms/kbest-read.tex}
    \end{algorithm}
    
    \paragraph{Step \ref{step:parsing:derivation}}
    The listing of \cref{alg:alg:parsing:deriv} shows how the parse chart \(C\) and the weight assignment \(W\) obtained in the previous step are used to enumerate derivation trees with increasing weight.
    The algorithm is derived from \citet[Algorithm 2, 3 and 4]{HuangChiang05} with some adjustments to accomodate our grammar format (i.e.\@ multiple initial nonterminals, non-binary grammars, and the form of \abrv{dcp} supertags).
    In the following we will refer to this publication as \abrv{hc} when we compare the two algorithms.
    Aside from the two structures \(C\) and \(W\), our illustration of the algorithm expects the following inputs: the length \(n\) of the sentence to parse, the prediction confidence assignment \(\mu\colon S \to \DR\) for each supertag, and a set of initial nonterminals.
    In it, we observe tuples of the following two forms:
    \begin{itemize}
        \item
            \((A, L, k')\) where \(A\) is a nonterminal, \(L\) is a set of sentence positions, and \(k'\) is an integer less or equal \(k\); e.g. in the subscript of \(D\) and \(W\) or as parameters of the function \textsc{chkNode}.
            Intuitively these tuples extend the concept of passive parse items by the index \(k'\).
            Each occurence of such a tuple is a reference to the \(k'\)th derivation starting with the nonterminal \(A\) with the yield \(L\) among a sequence of at most \(k\) best derivations.
            Let us call these tuples \emph{indexed passive items}.
        \item
            \((r, \vec{L}, \vec{k})\) where \(r\) is a supertag, \(\vec{L}\) is a sequence of sets of sentence positions and \(\vec{k}\) is a sequence of integers less or equal \(k\) such that the length of \(\vec{L}\) and \(\vec{k}\) is the arity of the grammar rule in \(r\); e.g.\@ as parameters of the function \textsc{chkEdge} and in line 24.
            This notation further extends the previous tuple form by specifying the root and direct children of the referenced derivation:
                Its root is \(r\) and the children are recursively specified by the \abrv{rhs} nonterminals in the grammar rule in \(r\), lets denote them by \(\vec{B}\), and the two sequences \(\vec{L}\) and \(\vec{k}\).
                The \(i\)th child is the \(\vec{k}_i\)th derivation starting with nonterminal \(\vec{B}_i\) with the yield \(\vec{L}_i\).
                Or in terms of the previous tuple form it is referenced as \((\vec{B}_i, \vec{L}_i, \vec{k}_i)\).
            Let us call these tuples \emph{indexed derivation nodes}.
    \end{itemize}
    As the second form suggests, the algorithm determines a \(k'\)th derivation by its root node and then recurses by referencing its direct children.
    In the following we will describe this principle in more detail by focussing on the collections that are used to store intermediate results and each individual procedure and function in the illustration.
    There are four collections that are used throughout the algorithm:
    \begin{itemize}
        \item A family of priority queues \(H\) indexed by combinations of nonterminal symbols and sets of sentence positions is used to store candidate indexeded derivation nodes.
            Each queue's elements is considered with the sum of the weights assigned by the prediction confidence and the weight computed for its children as expressed in the function \textsc{weight} in lines 5--6.
        \item A family of sets of index sequences \(E\) that is used to avoid constructing the indexed derivation nodes twice.
        \item A family of indexed derivation nodes \(D\) indexed by indexed passive items stores the intermediate results of the process.
            Per indexed passive item it stores the root node and references to children as explained above to recursively express a derivation tree.
        \item An assignment \(W'\) of a weight to each indexed passive item stores the weight of the according derivation in the sequence of $k$ best derivations.
            It extends the weight assignment \(W\) computed by \textsc{fillchart}.
    \end{itemize}
    The functions and procedures are as follows:
    \begin{itemize}
        \item \textsc{weight} as mentioned before, computes the weight of a derivation represented by an indexed derivation node.
            It is the sum of the prediction confidence for the included supertag plus the weight of its child indexed passive item that is computed in the function \textsc{chkNode} and stored in line 31.
        \item \textsc{successors} constructs a set of index sequences for successor items that are considered an increment of its argument.
            It contains the sequences that are increased in exactly one position by one and the rest remains equal.
            This procedure corresponds to the strategy in \abrv{HL} (explained in their section 4.2) where the next best derivation with a fixed root must differ from current one by at most one child.
        \item \textsc{chkEdge} lifts the function \textsc{chkNode} to the successors of its argument indexed derivation node.
            If any indexed passive item among the successors does not exists it returns "False" to represent that the indexed derivation node does not exist as well, otherwise it returns "True".
        \item \textsc{chkNode} determines if a derivation for an indexed passive item exists and computes the necessary intermidiate results to determine the derivation.
            Starting with the lines 20--21, it initializes a queue \(H_{(A, L)}\) of indexed derivation nodes for each link in the parse chart.
            Each successor index sequence only consists of the index 1 during the initialization as we start with the best derivations.
            Next follows a loop in lines 22--32 where each iteration determines one indexed derivation node until the \(k\)th is found.
            The loop starts with a re-population of the queue \(H_{(A, L)}\) in lines 23--26:
                If there is an indexed derivation node in the intermediate results \(D_{(A, L)}\), then it adds the indexed derivation nodes with incremented successor index sequence as obtained by the function \textsc{successors}.
            If the heap stays empty after this step of re-population, then there are no further candidates and we abort in lines 27--28.
            In the remaining three lines, we obtain the indexed derivation node for the next best derivation (line 29) and store it and its weight in the collections $D$ (line 30) and $W'$ (line 31) respectively.
        \item \textsc{readDerivation} constructs the derivation for an indexed passive item.
            It determines the indexed derivation node from the collection \(D\), assumes its grammar rule as root and obtains the children by recursive calls withe the successor indexed passive items.
        \item \textsc{kBestDerivations} is the entry point of the algorithm as it wraps the other functions and procedures enumerate the sequence of at most \(k\) best derivations.
            It uses a similar strategy as in function \textsc{chkNode} to populate a queue of initial nonterminals with index to determine the indexed passive items in order of their weight.
            Note that before such an initial indexed passive item is passed to the function \textsc{readDerivation} in line 45, the function \textsc{chkNode} must be called with it as an argument in either condition of line 9 or 15.
            It is necessary as the function \textsc{chkNode} prepares the collection \(D\) used to determine the derivations.
    \end{itemize}
    \todo[inline]{Referenzen in Beschreibung an \abrv{hc}, Tupelformen entsprechen Knoten/Kanten im Hypergraph nach der Beschreibung von \abrv{hc}}
    
    \paragraph{Some Notes on the Implementation of the Algorithms}
    In our implementation, the algorithms above are augmented with some well-established techniques either generally in programming or in the field of parsing.
    The most notable are as follows:
    \begin{itemize}
        \item The sets of sentence positions that occurr throughout both \cref{alg:parsing:chart,alg:parsing:kbest} (e.g.\@ in the index of the collections \(C\), \(W\) and \(D\) as well as in the parse items) are expressed as sequences of \emph{spans} of the form \(((l_1, r_1), \ldots, (l_s, r_s))\) where \(s \in \DN_+\) and \(l_1,\ldots,l_s,r_1,\ldots,r_s \in [0,n]\) and \(n \DN_+\) is the sentence length.
            Such a sequence must contain non-overlapping and non-bordering spans in increasing order from left to right, therefore each pair of neighbouring indices are strictly increasing \(r_i < l_{i+1}\) for \(i \in [s-1]\).
            Sequences of spans are a technique in the field of discontinuous parsing \citep[e.g.][cf.\@ range vector in def.\@ 6.6]{Kal10}.
            These span sequences are more efficient in storing the sets of positions and computing the union, when \(s\) is much smaller than the set of positions in the spans (which is usually the case).
            Moreover, the minimum \(l_1\) and the fanout \(s\) are easily accessible.
        \item The queues \(Q\) in \cref{alg:parsing:chart} and \(H_{(A,L)}\) in \cref{alg:parsing:kbest} are implemented using the \emph{heap} data structure.
            It allows us to efficiently access the minimal element.
        \item Objects that occur repeatedly without any structural investigation are mapped to (and replaced by) integers for efficient storage and comparisons; this process is called \emph{integerizing}.
            A trivial example are nonterminal symbols, wich are only used in comparisons to find matching rules during the parsing process.
            But also tuples of the form \((A, L)\) for a nonterminal \(A\) and set of sentence position \(L\) are integerized during the implementation of both \cref{alg:parsing:chart,alg:parsing:kbest} because they are used so frequently as indices.
        \item The collections expressed as families in both \cref{alg:parsing:chart,alg:parsing:kbest} are implemented either as \emph{hash maps} or as \emph{lists} when the index is integerized.
            Even though they are denoted with an initial value for each index in both \cref{alg:parsing:chart,alg:parsing:kbest}, they are empty at start and assume these values implicitly if there is no entry for the index.
            Otherwise collections with indices containing a subset of sentence positions like \(C\) and \(W\) would be highly infeasible.
    \end{itemize}
    
    \section{Reranking}
    This section focusses on a refinement procedure that we use to pick one final constituent tree among a sequence of \(k\) parses.
    A \deflab{reranking} process is itself an empirical prediction that assesses a previous empirical prediction by assigning confidence values to each label in conjunction with its prior confidence value.
    In the case of reranking in the field of parsing, it takes the predicted sequece of \(k\) best parses, each complemented with the score of the parse, and computes a new score for each parse tree; we use the best among these new scores to pick the final parse tree.
    The concept of reranking has its tradition in recommender systems, a discipline in the field of machine learning that focusses on recommending products to individuals by assigning a match score to each pair.
    Reranking is used in this context to further assess matches that were obtained in a query by a prior prediction model. \citep{???}
    
    Formally, we define a \emph{reranking} process as follows:
        Let \(Y\) be a set of labels, \(\mathcal{C}\) a set of confidence values.
        A reranker is a function \(\psi \colon Y \times \mathcal{C} \to \mathcal{C}\) that assigns a confidence value to each pair of a label and \emph{prior} confidence value.
    Often, only only a tiny fraction \(\hat{Y} \subseteq Y\) of the set of labels, called \emph{search space}, is considered for the reranking process.
    In the case of reranking in parsing, the search space is the collection of constituent trees in a \(k\) best sequence and the reranking process is used to find the highest scoring among those.
    Let \((t_1, \ldots, t_k) \in Y^k\) be such a sequence of \(k\) labels and \(\phi \colon Y \to C\) an assignment of prior confidence values.
    We call the \(\psi\)-\deflab{pick} among \(t_1 \ldots t_k\) the tree that maximizes the reranker \(\psi\)'s confidence value, i.e. \(
        \hat{t} = \argmax_{t \in \hat{Y}} \psi(t, \phi(t))
    \) where \(\hat{Y} = \{t_i \mid i \in [i]\}\) is the search space.
    
    Reranking has quite a history in parsing.
    One track of research driven by \citet{collins2001convolution,shen2003svm,collins05,huang2008forest} i.a.\@ defined discriminative classifiers that predict assessment scores for constituent trees in the form described above.
    Their reranking models consist of two parts:
        a \emph{feature extraction} procedure that maps each constituent tree into a vector, and a function that maps each vector to a score.
    The latter part was often realized as a support vector machine or perceptron, as they are computanionally efficient in both learning and prediction, and they are interpretable.
    These reranking models are used to score the constituent trees obtained from a parser based on context-free grammars, which are known to be highly ambiguous on their own when it comes to parsing natural languages.
    \citet{collins2001convolution, shen2003svm} define features from the training set of constituent trees by counting the occurring tree fragments.\footnote{
        A tree fragment is a coherent part that occurs a constituent tree.
        Each subtree is a tree fragment, and beyond that a fragment may also end at an inner node.
        E.g.\@ \(\cn{s} (\cn{vp} (\cn{vp} \tn{was}), \cn{np})\) is a fragment in our running example "where the survey was carried out".
        As there is a vast amount of such fragments in a whole treebank, \citet{collins2001convolution} define a kernel function that counts the tree fragments occurring commonly in two constituent trees.
        The use of tree fragments strongly relates this strategy to the approach of \emph{data oriented parsing}. \citep{Bod02}
    }
    \citet{collins05,charniak2005coarse} define a hand-craftet set of feature patterns that are instantiated using the constituent trees in a training set.
    Both of these methods used their rerankers for a pick among an sequence of \(n\) best constituent trees.
    \citet{huang2008forest} introduced an approach that applies the reranking mechanism to a parse chart, practically eliminating the restriction to a limited number of constituent trees.
    However, this is only an approximative solution.
    The general training procedure for the reranker is common among the approaches:
        Parse charts or \(n\) best sequences of constituent trees are obtained from sentences in the training set with an off-the-shelf parser and context-free grammars that are obtained from the training set as well.
        To simulate disjoint data used for obtaining the grammars and parsing the sentences (but both coming from the training set), they define hold-out data by principle of cross validation.
        This obtained data is used to extract the features and train a discriminative classifier for deciding among the candidate parse trees.
        
    Another track that fits into the concept of reranking are coarse-to-fine approaches among grammar formalisms.
    These approaches define pipelines that concists of multiple parsing stages, and a coarser/finer relation on the used grammar formalism or instances, respectively.
    Comparing two stages, the earlier uses a coarser and the later a finer formalism/instance; the later stage uses the results of its predecessors to filter unviable items during its own parsing process; we consider each stage after the first a reranking process that refines the result of the previous one.
    \citet{Charniak06} use probabilistic context-free grammars for parsing and define the carser/finer relation over the nonterminal symbols.
    They start with defining a usual (binarized and Markovized) treebank grammar at the finest level, and construct a coarser grammar by replacing nonterminal symbols with equivalence classes.
    This is repeated 3 times in total (so there are 4 stages), the coarsest stage uses merely one nonterminal (next to an artificial root nonterminal).
    \citet{Cranenbourgh16} uses three different probabilistic grammar formalisms for parsing: context-free grammars, \abrv{lcfrs} and data-oriented parsing; which are considered in that order from coarse to fine.
    Both approaches describe a filtering mechanism by relating nonterminals (and thus parse items) between stages and testing in the finer stage if the related parse item existed or is better than some threshold weight, respectively.
    \citet{Denkinger18} used a similar technique but with automata formalisms instead of grammar formalisms.
    Togehter, we implemented a coarse-to-fine parser that uses two stages with, one with a context-free grammar and the other with an \abrv{lcfrs}. \citep{RupDen19} 
    
    In this work, we rerank constituent trees using the \defabrv{discontinuous data-oriented parsing}{disco-dop} formalism, which was characterized as an instance of \abrv{lcfrs} with latent annotations \citep[cf.\@ section~4]{Cranenbourgh11} and hence hybrid grammars. \citep[cf.\@ section~8.5.1]{Geb20}
    The formalism was introduced by \citet{Cranenbourgh11} as a generalization of data-oriented parsing \citep{Bod02} to the case of discontinuou constituent structures, as its name suggests.
    The grammars consist of constituent tree fragments with subsitution sites at leaves that carry nonterminal symbols.
    To accomodate the discontinuities, each tree fragments is complemented by an \abrv{lcfrs} composition that restricts the substitution in consideration of the terminal symbols occurring in the tree fragments.
    \Cref{ex:dop} shows an example of a disco-dop grammar.
    Following \citet{San11, Cranenbourgh16}, we use the \emph{double dop} extraction approach for discontinuous constituent trees.
    It extracts rules for each constituent tree fragment in the training set that occurs at least twice.
    We restrict the fragments such that they do not include tokens from the constituent trees but end at the \abrv{pos} symbols.
    We estimate probabilistic weights for each rule by conditioned on the \abrv{lhs} nonterminal/root constituent symbol.
    However, in contrast to the coarse-to-fine approach described by \citet{Cra11}, we do not define a filtering mechanism based on the parse chart from the previous part of the parsing pipeline, but view the disco-dop reranking as a completely separate process for weighting each single constituent tree in a \(k\) best parsing sequence.
    Therefore, we augment the usual parsing procedure for \abrv{lcfrs} (which can be used for disco-dop) such that it can only produce derivations for the input constituent tree by matching its nodes.
    Moreover, we slightly diverge from the weight structure that we use for parsing with the supertags:
        In this case we do not use the Viterbi-equivalent sum and maximum operations with logarithms of proabilistic values, but the pababilistic product and sum operations.
    These augmentations are straight-forward and we refrain from a further elaboration of the parsing process.
    
    \begin{example}\label{ex:dop}
        \Cref{fig:ex:dop} shows four rules of a disco-dop grammar that consist of fragments in the tree to the left.
        The rules are in the notation used by \citet{Cra15}.
        They denote them in a similar fashion to \abrv{lcfrs}:
            Each leaf that is a substitution site is complemented with a sequence of variables and each leaf that is a terminal with an index, such that each of these objects uniquely occurs within each rule.
            A composition below the root node indicates how terminals may be concatenated by the rules, using the variables and indices from the leaves.
            The compositions are tuples allowing discontinuities just in the same sense as in \abrv{lcfrs}.
        The illustrated rules are able to produce the constituent structure with two different derivations, one starting with the upper left rule and one with the bottom right one.
        The following hybrid grammar rules are equivalend to the above notation: \begin{align*}
            \cn{sbar} &\to (\x_1^1 \, \x_2^1 \, \x_1^2)\;(\cn{sbar} (\cn{s} (x_1, x_2)))\;(\cn{vp}_2, \cn{np}) \\
            \cn{np} &\to (\cn{pt} \, \cn{nn})\;(\cn{np} (\cn{pt}, \cn{nn})) \\
            \cn{vp}_2 &\to (\cn{wrb}, \cn{vbn} \, \cn{rp})\;(\cn{vp} (\cn{wh}(\cn{wrb}),\cn{vbn},\cn{np}(\cn{rp}))) \\
            \cn{sbar} &\to (\x_1^1 \, \cn{pt} \, \cn{nn} \, \x_1^2)\;(\cn{sbar} (\cn{s} (\cn{vp}_2(x_1),\cn{np}(\cn{pt}, \cn{nn}))))\;(\cn{vp}_2)
        \end{align*}
    \end{example}
    
    \begin{figure}
        \null\hfill
        \subfile{figures/example-constituents.tex}
        \hspace{1cm}
        \subfile{figures/example-dop.tex}
        \hfill\null
        \caption{\label{fig:ex:dop}
            The running example constituent tree and four rules of a disco-dop grammar with derivations for the constituent structure and its sequence of \abrv{pos} tags.}
    \end{figure}
    
    \ifSubfilesClassLoaded{%
        \printindex
        \bibliography{../refeferences.bib}%
    }{}
\end{document}
